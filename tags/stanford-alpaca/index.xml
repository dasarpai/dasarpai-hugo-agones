<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stanford Alpaca on Agones</title>
    <link>http://localhost:1313/tags/stanford-alpaca/</link>
    <description>Recent content in Stanford Alpaca on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 11:34:17 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/stanford-alpaca/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Stanford Alpaca</title>
      <link>http://localhost:1313/dsblog/Stanford-Alpaca/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Stanford-Alpaca/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6116-Stanford-Alpaca.jpg&#34; alt=&#34;Stanford-Alpaca&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;stanford-alpaca&#34;&gt;Stanford Alpaca&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca Github Report&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li&gt;&#xA;&lt;li&gt;This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The 52K &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json&#34;&gt;instruction-following data&lt;/a&gt; used for fine-tuning the model.&lt;/li&gt;&#xA;&lt;li&gt;The code for generating the data.&lt;/li&gt;&#xA;&lt;li&gt;The code for fine-tuning the model.&lt;/li&gt;&#xA;&lt;li&gt;The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li&gt;&#xA;&lt;li&gt;Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li&gt;&#xA;&lt;li&gt;Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li&gt;&#xA;&lt;li&gt;Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li&gt;&#xA;&lt;li&gt;Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li&gt;&#xA;&lt;li&gt;Model weights can be released if the creators of LLaMA gives permission.&lt;/li&gt;&#xA;&lt;li&gt;Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li&gt;&#xA;&lt;li&gt;Based on followin papers:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;Hugo2023&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Yizhong2022&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Data Release&#xA;&lt;ul&gt;&#xA;&lt;li&gt;alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;highlevel-activities-of-the-alpaca-project&#34;&gt;Highlevel Activities of the Alpaca Project&lt;/h2&gt;&#xA;&lt;p&gt;Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
