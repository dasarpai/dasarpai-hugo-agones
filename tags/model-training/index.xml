<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Model Training on Agones</title>
    <link>http://localhost:1313/tags/model-training/</link>
    <description>Recent content in Model Training on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 15:25:42 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/model-training/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Architecture and Training</title>
      <link>http://localhost:1313/dsblog/LLM-Architecture-and-Training/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/LLM-Architecture-and-Training/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg&#34; alt=&#34;LLM-Architecture-and-Training&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;understanding-llm-architectures-and-model-training&#34;&gt;&lt;strong&gt;Understanding LLM Architectures and Model Training&lt;/strong&gt;&lt;/h1&gt;&#xA;&lt;p&gt;Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. Weâ€™ll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-introduction-to-large-language-models-llms&#34;&gt;&lt;strong&gt;1. Introduction to Large Language Models (LLMs)&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;definition-and-importance-of-llms&#34;&gt;&lt;strong&gt;Definition and Importance of LLMs&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Large Language Models are advanced deep learning models trained on massive amounts of text data. LLMs have made it possible to perform a wide variety of natural language tasks, from answering complex questions to generating human-like responses in chat applications. These models use billions (sometimes trillions) of parameters to capture intricate relationships within language, enabling them to comprehend and generate coherent responses.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why to Finetune LLM?</title>
      <link>http://localhost:1313/dsblog/why-to-finetune-llm/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/why-to-finetune-llm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6115-why-to-finetune-llm.jpg&#34; alt=&#34;Why to Finetune LLM?&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;finetuning-fewshot-learning-why-and-how&#34;&gt;Finetuning, Fewshot Learning, Why and How?&lt;/h1&gt;&#xA;&lt;h2 id=&#34;why-to-finetune-a-llm&#34;&gt;Why to finetune a LLM?&lt;/h2&gt;&#xA;&lt;p&gt;Fine-tuning a large language model (LLM) can provide several benefits, depending on your specific needs and objectives. Here are some key reasons to consider fine-tuning an LLM:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Domain Specialization&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fine-tuning allows the model to become more proficient in specific domains, such as medical, legal, or technical fields, by training it on domain-specific data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Task Adaptation&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stanford Alpaca</title>
      <link>http://localhost:1313/dsblog/Stanford-Alpaca/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Stanford-Alpaca/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6116-Stanford-Alpaca.jpg&#34; alt=&#34;Stanford-Alpaca&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;stanford-alpaca&#34;&gt;Stanford Alpaca&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca Github Report&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Stanford Alpaca is An &amp;ldquo;Instruction-following&amp;rdquo; LLaMA Model&lt;/li&gt;&#xA;&lt;li&gt;This is the repo aims to build and share an instruction-following LLaMA model. The repo contains:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The 52K &lt;a href=&#34;https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json&#34;&gt;instruction-following data&lt;/a&gt; used for fine-tuning the model.&lt;/li&gt;&#xA;&lt;li&gt;The code for generating the data.&lt;/li&gt;&#xA;&lt;li&gt;The code for fine-tuning the model.&lt;/li&gt;&#xA;&lt;li&gt;The code for recovering Alpaca-7B weights from our released weight diff.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The current &amp;ldquo;Alpaca 7B model&amp;rdquo; is fine-tuned from a &amp;ldquo;7B LLaMA&amp;rdquo; model on 52K instruction-following data generated by the techniques in the Self-Instruct paper.&lt;/li&gt;&#xA;&lt;li&gt;Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite.&lt;/li&gt;&#xA;&lt;li&gt;Alpaca is still under development, and there are many limitations that have to be addressed.&lt;/li&gt;&#xA;&lt;li&gt;Alphaca is not yet fine-tuned to be safe and harmless.&lt;/li&gt;&#xA;&lt;li&gt;Initial release contains the data generation procedure, dataset, and training recipe.&lt;/li&gt;&#xA;&lt;li&gt;Model weights can be released if the creators of LLaMA gives permission.&lt;/li&gt;&#xA;&lt;li&gt;Live demo to help readers better understand the capabilities and limits of Alpaca is available.&lt;/li&gt;&#xA;&lt;li&gt;Based on followin papers:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLaMA: Open and Efficient Foundation Language Models. &lt;a href=&#34;https://arxiv.org/abs/2302.13971v1&#34;&gt;Hugo2023&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Self-Instruct: Aligning Language Model with Self Generated Instructions. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;Yizhong2022&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Data Release&#xA;&lt;ul&gt;&#xA;&lt;li&gt;alpaca_data.json contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: Instruction, input, output (text-davinci-003 geneated answer).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;highlevel-activities-of-the-alpaca-project&#34;&gt;Highlevel Activities of the Alpaca Project&lt;/h2&gt;&#xA;&lt;p&gt;Highlevel Actitivies done by Stanford Alpaca team and Project Output&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cost Functions and Optimizers in Machine Learning</title>
      <link>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg&#34; alt=&#34;Cost-Functions-and-Optimizers-in-Machine-Learning&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;cost-functions-and-optimizers-in-machine-learning&#34;&gt;Cost-Functions-and-Optimizers-in-Machine-Learning&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-machine-learning&#34;&gt;What is machine learning?&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning is a subfield of artificial intelligence that focuses on the &lt;strong&gt;development of algorithms and statistical models&lt;/strong&gt; that enable computers to improve their performance on a specific task through experience.&lt;/p&gt;&#xA;&lt;p&gt;In machine learning, the goal is to develop models that can &lt;strong&gt;automatically learn patterns and relationships in data, and use that knowledge to make predictions or take actions&lt;/strong&gt;. The models are trained on a large dataset, and the learning process involves &lt;strong&gt;optimizing the parameters of the model to minimize the prediction error&lt;/strong&gt;. For this purpose every algorithms uses some &lt;strong&gt;cost function or loss function&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Neural Network</title>
      <link>http://localhost:1313/dsblog/Introduction-to-Neural-Network/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Introduction-to-Neural-Network/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6034-Introduction-to-Neural-Network.jpg&#34; alt=&#34;Introduction to Neural Network&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;introduction-to-neural-network&#34;&gt;Introduction to Neural Network&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction-to-a-perceptron&#34;&gt;Introduction to a Perceptron&lt;/h2&gt;&#xA;&lt;p&gt;A perceptron is a type of artificial neural network that can be used for binary classification. It is a simple model that consists of a single layer of artificial neurons and is used to classify input data into one of two categories. The perceptron algorithm learns the weights of the artificial neurons by adjusting them based on the input data and the desired output. The perceptron is considered a basic building block for more complex neural networks.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
