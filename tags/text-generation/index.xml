<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text Generation on Agones</title>
    <link>http://localhost:1313/tags/text-generation/</link>
    <description>Recent content in Text Generation on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 11:34:17 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/text-generation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring All Dimensions of Application Development</title>
      <link>http://localhost:1313/dsblog/Exploring-All-Dimensions-of-Application-Development/</link>
      <pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Exploring-All-Dimensions-of-Application-Development/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6175-Exploring-All-Dimensions-of-Application-Development.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;exploring-all-dimensions-of-application-development&#34;&gt;Exploring All Dimensions of Application Development&lt;/h1&gt;&#xA;&lt;p&gt;These aspects highlight the diverse areas involved in application development beyond just frontend, backend, or mobile/desktop apps. Each plays a critical role in building, deploying, and maintaining robust, scalable, and user-friendly applications.&lt;/p&gt;&#xA;&lt;p&gt;Each of these aspects is crucial to modern software development, covering everything from handling the user interface on the frontend to processing data and requests on the backend, as well as building specialized mobile or desktop applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring LLM Application Development</title>
      <link>http://localhost:1313/dsblog/Exploring-LLM-App-Development/</link>
      <pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Exploring-LLM-App-Development/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6174-Exploring-LLM-App-Development.jpg&#34; alt=&#34;Exploring LLM Application Development&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;exploring-llm-application-development&#34;&gt;Exploring LLM Application Development&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-llm-application-development&#34;&gt;What is LLM Application Development?&lt;/h2&gt;&#xA;&lt;p&gt;Large Language Model (LLM) application development involves creating applications that leverage pretrained large language models, like GPT (like GPT3.5, GPT4.o), Sonnet, DALLE, SORA, BERT, T5, Gemma, RoBERTa, DINO, Turning-NLG, Phi, Llama, Stable Diffusion, Flang, Einstine, Megatron, StyleGAN, BART,  Granite, or others, to perform natural language processing tasks. Unlike classical applications, which operate on explicit programming logic, LLM-based applications rely on trained models to process human language, make predictions, and respond dynamically based on vast amounts of text data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Benchmarks Explained</title>
      <link>http://localhost:1313/dsblog/AI-Benchmarks-Explained/</link>
      <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/AI-Benchmarks-Explained/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6173-AI-Benchmarks-Explained.jpg&#34; alt=&#34;AI-Benchmarks-Explained&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;ai-benchmarks-explained-essential-components-and-leading-llm-evaluation-techniques&#34;&gt;AI Benchmarks Explained: Essential Components and Leading LLM Evaluation Techniques&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-a-benchmark-in-ai&#34;&gt;What is a Benchmark in AI?&lt;/h2&gt;&#xA;&lt;p&gt;A &lt;strong&gt;benchmark&lt;/strong&gt; in AI is like a standard measurement tool that helps researchers and developers assess how well their artificial intelligence models perform. Just like athletes are judged based on their performance against specific standards, AI models are evaluated against predefined tasks and metrics.&lt;/p&gt;&#xA;&lt;p&gt;Thus, benchmarks are essential tools in the AI development ecosystem. They help ensure that AI models are evaluated fairly and consistently, providing a basis for comparison, improvement, and innovation in the field. By using benchmarks, developers can better understand their models’ capabilities and limitations, ultimately leading to more effective and robust AI systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transfer Learning Key AI Techniques Explained</title>
      <link>http://localhost:1313/dsblog/Transfer-Learning-Key-AI-Techniques-Explained/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Transfer-Learning-Key-AI-Techniques-Explained/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6172-Transfer-Learning-Key-AI-Techniques-Explained.jpg&#34; alt=&#34;Transfer Learning Key AI Techniques Explained&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;transfer-learning-key-ai-techniques-explained&#34;&gt;Transfer Learning Key AI Techniques Explained&lt;/h1&gt;&#xA;&lt;p&gt;In this article we will understand some important concepts used within machine learning.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What is in-context Learning?&lt;/li&gt;&#xA;&lt;li&gt;What is Prompt-Engineering?&lt;/li&gt;&#xA;&lt;li&gt;What is the relationship between Prompt Engineering and In-Context Learning?&lt;/li&gt;&#xA;&lt;li&gt;What is Zero-shot learning?&lt;/li&gt;&#xA;&lt;li&gt;How Zero-shot learning is different from In-context Learning?&lt;/li&gt;&#xA;&lt;li&gt;What is Meta-Learning?&lt;/li&gt;&#xA;&lt;li&gt;What is Few-shot learning?&lt;/li&gt;&#xA;&lt;li&gt;Do we need foundational models for Meta-learning and Few-shot learning?&lt;/li&gt;&#xA;&lt;li&gt;What is transfer learning?&lt;/li&gt;&#xA;&lt;li&gt;How do we do transfer learning from existing model?&lt;/li&gt;&#xA;&lt;li&gt;What is finetuning?&lt;/li&gt;&#xA;&lt;li&gt;Which layers to update, what weight to update during finetuning?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;prompt-engineering-in-context-learning-and-zero-shot-learning&#34;&gt;Prompt Engineering, In Context Learning and Zero-shot Learning&lt;/h2&gt;&#xA;&lt;h3 id=&#34;what-is-in-context-learning&#34;&gt;What is in-context Learning?&lt;/h3&gt;&#xA;&lt;p&gt;In-Context Learning refers to a model&amp;rsquo;s ability to adapt its responses based on the context provided in the input prompt without updating its parameters or undergoing explicit training. The model uses the examples, instructions, or context given in the input to influence its behavior during inference.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Types of Large Language Models (LLM)</title>
      <link>http://localhost:1313/dsblog/Types-of-LLM/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Types-of-LLM/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6171-Types-of-LLM.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction:&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;The world of Generative AI (GenAI) is expanding at an astonishing rate, with new models emerging almost daily, each sporting unique names, capabilities, versions, and sizes. For AI professionals, keeping track of these models can feel like a full-time job. But for business users, IT professionals, and software developers trying to make the right choice, understanding the model’s name and what it represents can seem overwhelming. Wouldn’t it be helpful if we could decode the meaning behind these names to know if a model fits our needs and is worth the investment? In this article, we’ll break down how the names of GenAI models can reveal clues about their functionality and suitability for specific tasks, helping you make informed decisions with confidence.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Navigating the JavaScript Ecosystem</title>
      <link>http://localhost:1313/dsblog/Navigating-the-JavaScript-Ecosystem/</link>
      <pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Navigating-the-JavaScript-Ecosystem/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6170-Navigating-the-JavaScript-Ecosystem.jpg&#34; alt=&#34;Navigating the JavaScript Ecosystem&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;navigating-the-javascript-ecosystem-npm-yarn-unpkg-and-more&#34;&gt;Navigating the JavaScript Ecosystem: npm, Yarn, unpkg, and More&lt;/h1&gt;&#xA;&lt;p&gt;This article is trying to answer following questions.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Evoluation of Javascript and Relationship with Java.&lt;/li&gt;&#xA;&lt;li&gt;What are popular javascript libraries?&lt;/li&gt;&#xA;&lt;li&gt;What is Node and Node.js?&lt;/li&gt;&#xA;&lt;li&gt;Key Features of Node.js.&lt;/li&gt;&#xA;&lt;li&gt;How are Node and Node.js Related?&lt;/li&gt;&#xA;&lt;li&gt;What are the Central Repositories of Javascript Packages?&lt;/li&gt;&#xA;&lt;li&gt;What is the difference between npm and npx?&lt;/li&gt;&#xA;&lt;li&gt;What are important npx commands?&lt;/li&gt;&#xA;&lt;li&gt;What is the &amp;rsquo;export&amp;rsquo; keyword in javascript?&lt;/li&gt;&#xA;&lt;li&gt;How to Use the Exported Function?&lt;/li&gt;&#xA;&lt;li&gt;What is the meaning of workspace in Yarn pacakge manager?&lt;/li&gt;&#xA;&lt;li&gt;Key Features of Yarn Workspaces.&lt;/li&gt;&#xA;&lt;li&gt;How to Set Up Yarn Workspaces?&lt;/li&gt;&#xA;&lt;li&gt;Can I use multiple package managers in my Javascript project?&lt;/li&gt;&#xA;&lt;li&gt;What are other Important Languages and their primary purpose?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;evoluation-of-javascript-and-relationship-with-java&#34;&gt;Evoluation of Javascript and Relationship with Java.&lt;/h2&gt;&#xA;&lt;p&gt;There is no relationship between Java and JavaScript.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Applications of GenAI</title>
      <link>http://localhost:1313/dsblog/Applications-of-GenAI/</link>
      <pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Applications-of-GenAI/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6169-Applications-of-GenAI.jpg&#34; alt=&#34;Applications of GenAI&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;application-of-generative-ai-genai&#34;&gt;Application of Generative AI (GenAI)&lt;/h1&gt;&#xA;&lt;p&gt;Generative AI (GenAI) is transforming how we interact with technology by producing human-like text, images, audio, and even code. Leveraging advanced models, especially large language models (LLMs), GenAI offers a wide range of applications across industries and data types. Let&amp;rsquo;s explore some of the key use cases and how different sectors are benefiting from this technology.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-text-generation&#34;&gt;1. &lt;strong&gt;Text Generation&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Text generation using GenAI models is a powerful tool for automating content creation. Pretrained models can generate natural, coherent text for various business and creative purposes. This can be particularly valuable for:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation</title>
      <link>http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6088-rps-Pretrained-Language-Models-for-Text-Generation.jpg&#34; alt=&#34;Pretrained Language Models for Text Generation&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Paper Name :- Pretrained Language Models for Text Generation: A Survey&lt;/strong&gt;&lt;br&gt;&#xA;Typer of Paper:- Survey Paper  &lt;br&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2105.10311&#34;&gt;Paper URL&lt;/a&gt;&lt;br&gt;&#xA;Paper title of the citations mentioned can be found at &lt;a href=&#34;http://localhost:1313/dsblog/aip&#34;&gt;AI Papers with Heading&lt;/a&gt;. Use citation code to locate.&lt;/p&gt;&#xA;&lt;h1 id=&#34;paper-summary---pretrained-language-models-for-text-generation&#34;&gt;Paper Summary :- Pretrained Language Models for Text Generation&lt;/h1&gt;&#xA;&lt;h2 id=&#34;paper-outcome&#34;&gt;Paper Outcome&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;General task deﬁnition&lt;/li&gt;&#xA;&lt;li&gt;Describe the mainstream architectures of PLMs for text generation.&lt;/li&gt;&#xA;&lt;li&gt;How to adapt existing PLMs to model different input data and satisfy special properties in the generated text.&lt;/li&gt;&#xA;&lt;li&gt;Summarize several important ﬁne-tuning strategies for text generation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;ideas-from-the-paper&#34;&gt;Ideas from the Paper&lt;/h2&gt;&#xA;&lt;h3 id=&#34;main-ideas&#34;&gt;Main Ideas&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;This paper discusses &amp;ldquo;major advances achieved in the topic of PLMs for text generation&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;This survey aims to provide &amp;ldquo;text generation researchers a synthesis&amp;rdquo; and pointer to related research.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;general-ideas&#34;&gt;General Ideas&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Text generation has become one of the most important yet challenging tasks in natural language processing (NLP).&lt;/li&gt;&#xA;&lt;li&gt;Neural generation model are deep learning models&lt;/li&gt;&#xA;&lt;li&gt;Pretrained language models (PLMs) are neural generation model&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;task-types-and-typical-applications&#34;&gt;Task Types and Typical Applications&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX )&lt;/li&gt;&#xA;&lt;li&gt;If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional&#xA;generation task(generate text without any constraint) &lt;a href=&#34;http://localhost:1313/dsblog/aip#radford2019&#34;&gt;Radford2019&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or&#xA;attribute-based generation.  X plays the role of guiding the text generation. &lt;a href=&#34;http://localhost:1313/dsblog/aip#Keskar2019&#34;&gt;Keskar2019&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation &lt;a href=&#34;http://localhost:1313/dsblog/aip#Li2021c&#34;&gt;Li2021c&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;If X is multimedia input such as image, the task becomes image caption &lt;a href=&#34;http://localhost:1313/dsblog/aip#Xia2020&#34;&gt;Xia2020&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;If X is multimedia input such as speech, the task become speech recognition &lt;a href=&#34;http://localhost:1313/dsblog/aip#Fan2019&#34;&gt;Fan2019&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system.&lt;/li&gt;&#xA;&lt;li&gt;Machine translation aims to translate text from one language into another language automatically &lt;a href=&#34;http://localhost:1313/dsblog/aip#Conneau2019&#34;&gt;Conneau2019&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Generating condensed summary of a long document &lt;a href=&#34;http://localhost:1313/dsblog/aip#Zhang2019b&#34;&gt;Zhang2019b&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Dialogue system to converse with humans using natural language. &lt;a href=&#34;http://localhost:1313/dsblog/aip#Wolf2019&#34;&gt;Wolf2019&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;architectures-for-text-generation&#34;&gt;Architectures for Text Generation&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MASS &lt;a href=&#34;http://localhost:1313/dsblog/aip#song2019&#34;&gt;Song2019&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;T5 &lt;a href=&#34;http://localhost:1313/dsblog/aip#raffel2020&#34;&gt;Raffel2020&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;BART &lt;a href=&#34;http://localhost:1313/dsblog/aip#lewis2020&#34;&gt;Lewis2020&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GPT &lt;a href=&#34;http://localhost:1313/dsblog/aip#radfordet2019&#34;&gt;Radfordet2019&lt;/a&gt;; &lt;a href=&#34;http://localhost:1313/dsblog/aip#brown2020&#34;&gt;Brown2020&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;CTRL [Keskar2019]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;modeling-different-data-types-from-input&#34;&gt;Modeling Different Data Types from Input&lt;/h2&gt;&#xA;&lt;h3 id=&#34;unstructured-input&#34;&gt;Unstructured Input&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b]&lt;/li&gt;&#xA;&lt;li&gt;Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a]&lt;/li&gt;&#xA;&lt;li&gt;Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019]&lt;/li&gt;&#xA;&lt;li&gt;Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018].&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;structured-input&#34;&gt;Structured Input&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;PLMs are not designed for structured or tabular data but for sequential text/data.&lt;/li&gt;&#xA;&lt;li&gt;Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020]&lt;/li&gt;&#xA;&lt;li&gt;To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020]&lt;/li&gt;&#xA;&lt;li&gt;Introduced an additional graph encoder to encode the input KG. [Li2021b]&lt;/li&gt;&#xA;&lt;li&gt;Template based method to serialize input table into text sequence.  [Gong2020]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020]&lt;/li&gt;&#xA;&lt;li&gt;The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b].&lt;/li&gt;&#xA;&lt;li&gt;Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;multimedia-input&#34;&gt;Multimedia Input&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a]&lt;/li&gt;&#xA;&lt;li&gt;Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020]&lt;/li&gt;&#xA;&lt;li&gt;Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019]&lt;/li&gt;&#xA;&lt;li&gt;Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020&lt;/li&gt;&#xA;&lt;li&gt;Image, video, speech recognition is hungry for human-transcripted supervised data.&lt;/li&gt;&#xA;&lt;li&gt;Integrate PLMs for weakly-supervised learning. For example,&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;satisfying-special-properties-for-output-text&#34;&gt;Satisfying Special Properties for Output Text&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation.&lt;/li&gt;&#xA;&lt;li&gt;Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should&#xA;also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;When applying PLMs to the task of dialogue systems, TransferTransfo  and DialoGPT were able to generate more relevant responses than  RNNbased models. [Wolf2019] [Zhang2020]&lt;/li&gt;&#xA;&lt;li&gt;Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input&#xA;representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020]&lt;/li&gt;&#xA;&lt;li&gt;Approach for non-conditioned dialogue [Bao2020].&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Faithfulness. Means the content in generated text should not contradict the facts in input text.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;PLMs are potentially beneficial to generate faithful text by utilizing background knowledge.&lt;/li&gt;&#xA;&lt;li&gt;Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020]&lt;/li&gt;&#xA;&lt;li&gt;With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities.&lt;/li&gt;&#xA;&lt;li&gt;Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018]&lt;/li&gt;&#xA;&lt;li&gt;Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation.&lt;/li&gt;&#xA;&lt;li&gt;Code-Switching Pre-training (CSP) for machine translation. [Yang2020a]&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Extracted the word-pair alignment information from the source and target language,&lt;/li&gt;&#xA;&lt;li&gt;Aplied the extracted alignment information to enhance order-preserving.&lt;/li&gt;&#xA;&lt;li&gt;Translation across multiple languages, called multilingual machine translation [Conneau2019].&lt;/li&gt;&#xA;&lt;li&gt;mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020]&lt;/li&gt;&#xA;&lt;li&gt;Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;summary-from-introduction&#34;&gt;Summary from Introduction&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a].&lt;/li&gt;&#xA;&lt;li&gt;Machine translation generates text in a different language based on the source text [Yang2020a];&lt;/li&gt;&#xA;&lt;li&gt;Summarization generates an abridged version of the source text to include salient information [Guan2020].&lt;/li&gt;&#xA;&lt;li&gt;Text generation tasks based on&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Recurrent neural networks (RNN) [Li2019],&lt;/li&gt;&#xA;&lt;li&gt;Convolutional neural networks (CNN) [Gehring2017],&lt;/li&gt;&#xA;&lt;li&gt;Graph neural networks (GNN) [Li2020],&lt;/li&gt;&#xA;&lt;li&gt;Attention mechanism [Bahdanau2015].&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation.&lt;/li&gt;&#xA;&lt;li&gt;Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity.&lt;/li&gt;&#xA;&lt;li&gt;Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice.&lt;/li&gt;&#xA;&lt;li&gt;The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve&#xA;state-of-the-art results.&lt;/li&gt;&#xA;&lt;li&gt;PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language.&lt;/li&gt;&#xA;&lt;li&gt;PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020].&lt;/li&gt;&#xA;&lt;li&gt;A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;conclusion--future-recommendations&#34;&gt;Conclusion &amp;amp; Future Recommendations&lt;/h2&gt;&#xA;&lt;p&gt;Model Extension.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
