<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cost Savings on Agones</title>
    <link>http://localhost:1313/tags/cost-savings/</link>
    <description>Recent content in Cost Savings on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 15:25:42 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/cost-savings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BitNet b1.58-2B4T: Revolutionary Binary Neural Network for Efficient AI</title>
      <link>http://localhost:1313/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/</link>
      <pubDate>Mon, 21 Apr 2025 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/BitNet-b1-58-2B4T-for-efficient-ai-processing/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6263-BitNet-b1.58-2B4T.jpg&#34; alt=&#34;BitNet b1.58-2B4T&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.12285&#34;&gt;Archive Paper Link&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;bitnet-b158-2b4t-the-future-of-efficient-ai-processing&#34;&gt;BitNet b1.58-2B4T: The Future of Efficient AI Processing&lt;/h1&gt;&#xA;&lt;h2 id=&#34;a-history-of-1-bit-transformer-model&#34;&gt;A History of 1 bit Transformer Model&lt;/h2&gt;&#xA;&lt;p&gt;A paper &amp;ldquo;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&amp;rdquo; was published by Stanford University, ETH ZÃ¼rich, and EPFL. It was published on October 2023 (published on arXiv on October 17, 2023). &lt;a href=&#34;https://arxiv.org/pdf/2310.11453&#34;&gt;Standord Paper Link&lt;/a&gt;. The core Concept of 1.58 bits per parameter, was introduced here. This demonstrated that LLMs could be effectively trained and operated with extremely low-bit representation while maintaining competitive performance&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ollama Setup and Running Models</title>
      <link>http://localhost:1313/dsblog/Ollama-Setup-and-Running-Models/</link>
      <pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Ollama-Setup-and-Running-Models/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6262-Ollama-Setup-and-Running-Models.jpg&#34; alt=&#34;Ollama Setup and Running Models&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;ollama-running-large-language-models-locally&#34;&gt;Ollama: Running Large Language Models Locally&lt;/h1&gt;&#xA;&lt;p&gt;The landscape of Artificial Intelligence (AI) and Large Language Models (LLMs) has traditionally been dominated by cloud-based services. While powerful, these often come with costs, privacy concerns, and require constant internet connectivity. Ollama emerges as a compelling open-source solution, designed to simplify the process of downloading, managing, and running LLMs directly on your local machine. This approach offers significant advantages, including enhanced privacy, cost savings, offline capability, and greater control over the models you use.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
