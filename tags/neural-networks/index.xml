<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Networks on Agones</title>
    <link>http://localhost:1313/tags/neural-networks/</link>
    <description>Recent content in Neural Networks on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 15:25:42 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Architecture and Training</title>
      <link>http://localhost:1313/dsblog/LLM-Architecture-and-Training/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/LLM-Architecture-and-Training/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6129-LLM-Architecture-and-Training.jpg&#34; alt=&#34;LLM-Architecture-and-Training&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;understanding-llm-architectures-and-model-training&#34;&gt;&lt;strong&gt;Understanding LLM Architectures and Model Training&lt;/strong&gt;&lt;/h1&gt;&#xA;&lt;p&gt;Large Language Models (LLMs) are transforming the field of artificial intelligence by enabling machines to understand and generate human language with unprecedented accuracy. This article delves into the architecture, training methods, and practical applications of LLMs. We’ll explore the core components that make these models so powerful and explain how they are trained and fine-tuned for real-world use cases.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-introduction-to-large-language-models-llms&#34;&gt;&lt;strong&gt;1. Introduction to Large Language Models (LLMs)&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;definition-and-importance-of-llms&#34;&gt;&lt;strong&gt;Definition and Importance of LLMs&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;p&gt;Large Language Models are advanced deep learning models trained on massive amounts of text data. LLMs have made it possible to perform a wide variety of natural language tasks, from answering complex questions to generating human-like responses in chat applications. These models use billions (sometimes trillions) of parameters to capture intricate relationships within language, enabling them to comprehend and generate coherent responses.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why to Finetune LLM?</title>
      <link>http://localhost:1313/dsblog/why-to-finetune-llm/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/why-to-finetune-llm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6115-why-to-finetune-llm.jpg&#34; alt=&#34;Why to Finetune LLM?&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;finetuning-fewshot-learning-why-and-how&#34;&gt;Finetuning, Fewshot Learning, Why and How?&lt;/h1&gt;&#xA;&lt;h2 id=&#34;why-to-finetune-a-llm&#34;&gt;Why to finetune a LLM?&lt;/h2&gt;&#xA;&lt;p&gt;Fine-tuning a large language model (LLM) can provide several benefits, depending on your specific needs and objectives. Here are some key reasons to consider fine-tuning an LLM:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Domain Specialization&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fine-tuning allows the model to become more proficient in specific domains, such as medical, legal, or technical fields, by training it on domain-specific data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Task Adaptation&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding LLM GAN and Transformers</title>
      <link>http://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Understanding-LLM-GAN-and-Transformers/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6127-Understanding-LLM-GAN-and-Transformers.jpg&#34; alt=&#34;Understanding-LLM-GAN-Transformers&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;understanding-llm-gan-and-transformers&#34;&gt;Understanding LLM, GAN and Transformers&lt;/h1&gt;&#xA;&lt;h2 id=&#34;llm-layers&#34;&gt;LLM Layers&lt;/h2&gt;&#xA;&lt;p&gt;Large Language Models (LLMs) are typically based on Transformer architectures, which consist of several types of layers that work together to process and generate text. Here are the primary kinds of layers found in an LLM:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Embedding Layers&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Token Embedding Layer&lt;/strong&gt;: Converts input tokens (words, subwords, or characters) into dense n dimensional vectors.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Position Embedding Layer&lt;/strong&gt;: Adds positional information to the token embeddings, allowing the model to understand the order of tokens.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Transformer Encoder Layers&lt;/strong&gt;: This layer is found in models, which are designed for generating encoded represenration of the input.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers Demystified A Step-by-Step Guide</title>
      <link>http://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/</link>
      <pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/transformers-demystified-a-step-by-step-guide/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6113-transformers-demystified-a-step-by-step-guide.jpg&#34; alt=&#34;Transformers Demystified A Step-by-Step Guide&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;transformers-demystified-a-step-by-step-guide&#34;&gt;Transformers Demystified A Step-by-Step Guide&lt;/h1&gt;&#xA;&lt;p&gt;All modern Transformers are based on a paper &amp;ldquo;Attention is all you need&amp;rdquo;&lt;/p&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;This was the mother paper of all the transformer architectures we see today around NLP, Multimodal, Deep Learning. It was presented by Ashish Vaswani et al from Deep Learning / Google in 2017. We will discuss following and anything whatever question/observation/idea I have.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The need&#xA;Why this paper was needed? What problem it solved?&lt;/li&gt;&#xA;&lt;li&gt;What is transformer? What is encoder transformer? What is decoder transformer? What is encoder-decoder transformer?&lt;/li&gt;&#xA;&lt;li&gt;What is embedding? What is need for embedding? What are different types of embedding? What embeddingg is proposed in this work&lt;/li&gt;&#xA;&lt;li&gt;What benchmark dataset was used, what metrics were used and what was the performance of this model?&lt;/li&gt;&#xA;&lt;li&gt;Finally we will looks all the calculations with one illustration.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Encourage all to read this &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;original paper&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Basics of Word Embedding</title>
      <link>http://localhost:1313/dsblog/basics-of-word-embedding/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/basics-of-word-embedding/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg&#34; alt=&#34;Basics of Word Embedding&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;basics-of-word-embedding&#34;&gt;Basics of Word Embedding&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-context-target-and-window&#34;&gt;What is Context, target and window?&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The &amp;ldquo;context&amp;rdquo; word is the surrounding word.&lt;/li&gt;&#xA;&lt;li&gt;The &amp;ldquo;target&amp;rdquo; word is the middle word.&lt;/li&gt;&#xA;&lt;li&gt;The &amp;ldquo;window distance&amp;rdquo; is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Let&amp;rsquo;s take a sentence&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph of Thoughts</title>
      <link>http://localhost:1313/dsblog/graph-of-thoughts/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/graph-of-thoughts/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6103-Graph-of-Thoughts.jpg&#34; alt=&#34;Graph of Thoughts&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;graph-of-thoughts&#34;&gt;Graph of Thoughts&lt;/h1&gt;&#xA;&lt;p&gt;This is a valuable resource for learning Graph of Thoughts (GoT) concepts. The YouTube video is from code_your_own_AI. I&amp;rsquo;m utilizing the comments made by @wesleychang2005 on the video, which provide an excellent summary of GoT. If you&amp;rsquo;re interested in this topic and find the summary below intriguing, I recommend watching the entire 41-minute video.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tCPA89n6NGQ&amp;amp;t=1562&#34;&gt;https://www.youtube.com/watch?v=tCPA89n6NGQ&amp;t=1562&lt;/a&gt;&lt;br&gt;&#xA;&lt;strong&gt;Take Aways from the video&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;00:26 🤯 Graph of Thoughts (GoT) is a non-linear approach to reasoning for AI agents, using interconnected nodes and edges to represent the thought process.&lt;/li&gt;&#xA;&lt;li&gt;01:18 📊 The Tree of Thoughts method suffers from inefficiency, requiring hundreds of queries to solve a single problem.&lt;/li&gt;&#xA;&lt;li&gt;02:36 🎯 An AI agent is defined as an entity that can perceive its environment, make decisions, and initiate actions based on a control cycle and a reward function.&lt;/li&gt;&#xA;&lt;li&gt;05:39 🌐 The latest research focuses on AI agents augmented by Large Language Models (LLMs) for more intelligent and autonomous behavior.&lt;/li&gt;&#xA;&lt;li&gt;08:43 🤖 LLM-augmented AI agents can interact with and learn from their environment, making them more adaptive and capable.&lt;/li&gt;&#xA;&lt;li&gt;12:45 📝 Explanation fine-tuning of LLMs (Large Language Models) is guided by GPT-4&amp;rsquo;s own reasoning explanation, serving as a blueprint for development.&lt;/li&gt;&#xA;&lt;li&gt;13:34 🕸️ The &amp;ldquo;Graph of Thoughts&amp;rdquo; allows for a flexible approach to reasoning, where multiple chains of thoughts can be pursued and evaluated simultaneously.&lt;/li&gt;&#xA;&lt;li&gt;16:50 🎛️ The application of graph theory in AI involves the use of graph attention networks and various encoding techniques to manage both visual and textual data.&lt;/li&gt;&#xA;&lt;li&gt;22:46 📊 A scoring mechanism is used to assess the LLM&amp;rsquo;s replies for accuracy and relevance, aiding in quality control of the model&amp;rsquo;s output.&lt;/li&gt;&#xA;&lt;li&gt;24:16 🎮 A &amp;ldquo;Controller&amp;rdquo; manages the entire reasoning process, using a &amp;ldquo;Graph of Operations&amp;rdquo; (GoO) to dictate the execution plan for tasks, making the reasoning adaptable and structured.&lt;/li&gt;&#xA;&lt;li&gt;25:35 🌍 Graph-of-Thoughts (GoT) can be used for planet classification tasks. The speaker uses a simple example where an AI system decides whether a planet is habitable based on attributes like distance from the sun and atmospheric conditions.&lt;/li&gt;&#xA;&lt;li&gt;27:32 🛠️ In GoT, each node in the &amp;lsquo;Graph of Operations&amp;rsquo; (GoO) represents a specific task (e.g., check distance from the sun). The &amp;lsquo;Graph Reasoning State&amp;rsquo; (GRS) records and updates the system&amp;rsquo;s understanding as nodes are executed.&lt;/li&gt;&#xA;&lt;li&gt;29:30 📝 The speaker describes a more complex example involving multiple types of planets and a list of features for classification. He emphasizes the need for a specialized Language Learning Model (LLM) trained in astrophysics.&lt;/li&gt;&#xA;&lt;li&gt;32:56 🎯 Scoring and validation are essential for assessing the reliability of the AI&amp;rsquo;s responses. The system assigns a confidence score to its classification decision.&lt;/li&gt;&#xA;&lt;li&gt;35:48 🔄 The GoT system can incorporate human feedback, iterating through multiple loops to refine its reasoning process and improve classification outcomes.&lt;/li&gt;&#xA;&lt;li&gt;36:57 🛠️ The Graph-of-Operation (GoO) framework lays out how AI operations interact and depend on each other in a sequence, from initial query to final output.&lt;/li&gt;&#xA;&lt;li&gt;38:18 🙋‍♂️ Human domain expertise is essential for designing the reasoning flow within the GoO, as it&amp;rsquo;s not automatically generated by the AI system itself.&lt;/li&gt;&#xA;&lt;li&gt;39:18 🤔 GPT-4 suggests that future AI systems like GPT-5 could potentially engage in meta-learning or self-improvement, opening the possibility for AI to design its own GoO structure.&lt;/li&gt;&#xA;&lt;li&gt;39:43 📊 Adequate training data is crucial for advanced AI systems to learn diverse tasks in multiple domains and potentially design complex GoO structures.&lt;/li&gt;&#xA;&lt;li&gt;40:07 📈 Mathematical graph theory could help in constructing multiple graphs for specific problems, setting the stage for training more advanced AI systems.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>What is GAN Architecture?</title>
      <link>http://localhost:1313/dsblog/What-is-GAN-Architecture/</link>
      <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-is-GAN-Architecture/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6069-What-is-GAN-Architecture.jpg&#34; alt=&#34;What is GAN Architecture?&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-is-gan-architecture&#34;&gt;What is GAN Architecture?&lt;/h1&gt;&#xA;&lt;p&gt;Generative Adversarial Networks (GANs) are a powerful class of neural networks that are used for unsupervised learning. It was developed and introduced by Ian J. Goodfellow in 2014. It is a type of artificial intelligence (AI) model that consists of two neural networks: a generator and a discriminator. GANs are used for generative tasks, such as creating realistic images, videos, or even audio.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Capabilities of AI Transformers</title>
      <link>http://localhost:1313/dsblog/Capabilities-of-AI-Transformers/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Capabilities-of-AI-Transformers/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6067-Capabilities-of-AI-Transformers.jpg&#34; alt=&#34;Capabilities of AI Transformers&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;capabilities-of-ai-transformers&#34;&gt;Capabilities of AI Transformers&lt;/h1&gt;&#xA;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Types of Machine Learning</title>
      <link>http://localhost:1313/dsblog/Types-of-Machine-Learning/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Types-of-Machine-Learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6056-Types-of-Machine-Learning.jpg&#34; alt=&#34;Types of Machine Learning&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;types-of-machine-learning&#34;&gt;Types of Machine Learning&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning is a field of artificial intelligence that focuses on developing algorithms that can learn from data and make predictions or decisions. There are several types of machine learning techniques, each with its strengths and weaknesses. In this post, we will explore some of the most commonly used machine learning techniques, including supervised learning, unsupervised learning, reinforcement learning, and more. This post is not about deep diving into these topics but to give you a oneliner understanding and the difference between these different techniques.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cost Functions and Optimizers in Machine Learning</title>
      <link>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg&#34; alt=&#34;Cost-Functions-and-Optimizers-in-Machine-Learning&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;cost-functions-and-optimizers-in-machine-learning&#34;&gt;Cost-Functions-and-Optimizers-in-Machine-Learning&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-machine-learning&#34;&gt;What is machine learning?&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning is a subfield of artificial intelligence that focuses on the &lt;strong&gt;development of algorithms and statistical models&lt;/strong&gt; that enable computers to improve their performance on a specific task through experience.&lt;/p&gt;&#xA;&lt;p&gt;In machine learning, the goal is to develop models that can &lt;strong&gt;automatically learn patterns and relationships in data, and use that knowledge to make predictions or take actions&lt;/strong&gt;. The models are trained on a large dataset, and the learning process involves &lt;strong&gt;optimizing the parameters of the model to minimize the prediction error&lt;/strong&gt;. For this purpose every algorithms uses some &lt;strong&gt;cost function or loss function&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Neural Network</title>
      <link>http://localhost:1313/dsblog/Introduction-to-Neural-Network/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Introduction-to-Neural-Network/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6034-Introduction-to-Neural-Network.jpg&#34; alt=&#34;Introduction to Neural Network&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;introduction-to-neural-network&#34;&gt;Introduction to Neural Network&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction-to-a-perceptron&#34;&gt;Introduction to a Perceptron&lt;/h2&gt;&#xA;&lt;p&gt;A perceptron is a type of artificial neural network that can be used for binary classification. It is a simple model that consists of a single layer of artificial neurons and is used to classify input data into one of two categories. The perceptron algorithm learns the weights of the artificial neurons by adjusting them based on the input data and the desired output. The perceptron is considered a basic building block for more complex neural networks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is GAN?</title>
      <link>http://localhost:1313/dsblog/What-is-GAN/</link>
      <pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-is-GAN/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6043-gan.jpg&#34; alt=&#34;Partial Dependence Plots&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-is-gan&#34;&gt;What is GAN?&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-gan-generative-adversarial-network&#34;&gt;What is GAN (Generative Adversarial Network)?&lt;/h2&gt;&#xA;&lt;p&gt;Generative adversarial networks (GANs) are besing used to generate images, videos, text, audio and music. GAN is a class of machine-learning models introduced by Ian Goodfellow and his colleagues in 2014. The GANs became popular among researchers quickly because of their property to generate new data with the same statistics as the input training set. It can be applied to images, videos, textual data, tabular data and more, proving useful for semi-supervised, fully supervised, and reinforcement learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is Computer Vision</title>
      <link>http://localhost:1313/dsblog/what-is-computer-vision/</link>
      <pubDate>Wed, 28 Dec 2022 15:50:00 +0530</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/what-is-computer-vision/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6018-What-is-Computer-Vision.jpg&#34; alt=&#34;What is Computer Vision&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-is-computer-vision&#34;&gt;What is Computer vision?&lt;/h1&gt;&#xA;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;In the digital world, scientists are working hard to create machines and robots that can interact with humans the way humans interact with each other. You cannot interact with another human being around if you are not aware of the objects and background around you. There are many ways to know the things around us. We can know them through smell; without looking anything around we can tell, here is a rose flower or samosa or sugar factory around. Without looking we can tell whether a train is coming or going, a person is going or coming, this is a song sung by Lata Mangeshkar. Without looking I can tell this is smooth or rough, hard or soft, cold or hot. In all these cases we could identify the objects and things around us without using our eyes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What Are Transformers in AI</title>
      <link>http://localhost:1313/dsblog/What-Are-Transformers-in-AI/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/What-Are-Transformers-in-AI/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6031-What-are-Transformers-in-AI.jpg&#34; alt=&#34;What-are-Transformers-in-AI&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;what-are-transformers-in-ai&#34;&gt;What Are Transformers in AI&lt;/h1&gt;&#xA;&lt;h2 id=&#34;transformer-architecture&#34;&gt;Transformer Architecture&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/transformer/transformer-arch.jpg&#34; alt=&#34;Transformer&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;&#xA;&lt;p&gt;Whether GPT, ChatGPT, DALL-E, Whisper, Satablity AI or whatever significant you see in the AI worlds nowdays it is because of Transformer Architecture. Transformers are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Important AI Research Papers</title>
      <link>http://localhost:1313/dsblog/important-ai-research-papers/</link>
      <pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/important-ai-research-papers/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dsresources/dsr105-Important-AI-Research-Papers.jpg&#34; alt=&#34;Important AI Research Papers&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;important-ai-research-papers&#34;&gt;Important AI Research Papers&lt;/h1&gt;&#xA;&lt;p&gt;Content from this page is migrated to &lt;a href=&#34;https://dasarpai.com/dsblog/select-ai-papers&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
