<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Model Optimization on Agones</title>
    <link>http://localhost:1313/tags/model-optimization/</link>
    <description>Recent content in Model Optimization on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 11:34:17 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/model-optimization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring GGUF and Other Model Formats</title>
      <link>http://localhost:1313/dsblog/exploring-gguf-and-other-model-formats/</link>
      <pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/exploring-gguf-and-other-model-formats/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6180-exploring-gguf.jpg&#34; alt=&#34;Understanding GGUF and Other Model Formats in Machine Learning&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;understanding-gguf-and-other-model-formats-in-machine-learning&#34;&gt;&lt;strong&gt;Understanding GGUF and Other Model Formats in Machine Learning&lt;/strong&gt;&lt;/h1&gt;&#xA;&lt;p&gt;As machine learning models continue to grow in complexity, the need for efficient, flexible, and versatile model formats becomes more pronounced. While formats like ONNX, TensorFlow’s SavedModel, and PyTorch’s native format have been around for some time, newer formats like GGUF are gaining attention for their unique benefits. This article explores these formats, their use cases, and how they support various aspects of machine learning, including deployment, compatibility, and optimization.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Key Concepts</title>
      <link>http://localhost:1313/dsblog/Machine-Learning-Key-Concepts/</link>
      <pubDate>Thu, 03 Oct 2024 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Machine-Learning-Key-Concepts/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6152-Machine-Learning-Key-Concepts.jpg&#34; alt=&#34;Exploring Docker and VS Code Integration&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;machine-learning-key-concepts&#34;&gt;Machine Learning Key Concepts&lt;/h1&gt;&#xA;&lt;p&gt;In this article Essential Machine Learning Techniques/Concepts are Explained, some of them are are Cross-Validation, Hyperparameter Optimization, Machine learning types and much More.&lt;/p&gt;&#xA;&lt;h2 id=&#34;is-this-article-for-me&#34;&gt;Is this article for me?&lt;/h2&gt;&#xA;&lt;p&gt;If you are looking for the answer to any of the following questions, then the answer is &amp;lsquo;Yes.&amp;rsquo;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What is Cross-validation?&lt;/li&gt;&#xA;&lt;li&gt;What is Advantages of Cross-Validation?&lt;/li&gt;&#xA;&lt;li&gt;In cross-validation what is the use of the averaging the performance of 5 models?&lt;/li&gt;&#xA;&lt;li&gt;Why Averaging the Performance of Cross-Validation Models Matters:&lt;/li&gt;&#xA;&lt;li&gt;How Does Cross-Validation Help in Final Model Creation?&lt;/li&gt;&#xA;&lt;li&gt;Why Not Just Train on the Full Data from the Beginning?&lt;/li&gt;&#xA;&lt;li&gt;When should I use Cross-Validation?&lt;/li&gt;&#xA;&lt;li&gt;What is Feature Engineering?&lt;/li&gt;&#xA;&lt;li&gt;What is Regularization?&lt;/li&gt;&#xA;&lt;li&gt;What are different types of regularization techniques in ML?&lt;/li&gt;&#xA;&lt;li&gt;What is Bias-Variance Tradeoff?&lt;/li&gt;&#xA;&lt;li&gt;How to handle Bias-Variance problem?&lt;/li&gt;&#xA;&lt;li&gt;How to evaluate a model&amp;rsquo;s goodness/fitness/robustness?&lt;/li&gt;&#xA;&lt;li&gt;What is Ensemble Learning?&lt;/li&gt;&#xA;&lt;li&gt;What are different ensemble learning techniques?&lt;/li&gt;&#xA;&lt;li&gt;What is Dimensionality Reduction?&lt;/li&gt;&#xA;&lt;li&gt;What is kernel trick, can you explain with simple example?&lt;/li&gt;&#xA;&lt;li&gt;What are popular Dimensionality Reduction Techniques?&lt;/li&gt;&#xA;&lt;li&gt;What is Clustering?&lt;/li&gt;&#xA;&lt;li&gt;What are popular clustering algorithms?&lt;/li&gt;&#xA;&lt;li&gt;What is Deep Learning and Neural Networks?&lt;/li&gt;&#xA;&lt;li&gt;What is Self-Supervised Learning (SSL)?&lt;/li&gt;&#xA;&lt;li&gt;what is Meta-Learning (Learning to Learn)?&lt;/li&gt;&#xA;&lt;li&gt;What is Reinforcement Learning (RL)?&lt;/li&gt;&#xA;&lt;li&gt;What is Generative Model?&lt;/li&gt;&#xA;&lt;li&gt;What are different Generative Models?&lt;/li&gt;&#xA;&lt;li&gt;What is Federated Learning?&lt;/li&gt;&#xA;&lt;li&gt;What is Causal Inference?&lt;/li&gt;&#xA;&lt;li&gt;What is Neural Architecture Search (NAS)?&lt;/li&gt;&#xA;&lt;li&gt;What are Transformers and Attention Mechanisms?&lt;/li&gt;&#xA;&lt;li&gt;What is Explainable AI (XAI)?&lt;/li&gt;&#xA;&lt;li&gt;What are popular XAI methods?&lt;/li&gt;&#xA;&lt;li&gt;What is Uncertainty Quantification?&lt;/li&gt;&#xA;&lt;li&gt;What is Continual Learning (Lifelong Learning)?&lt;/li&gt;&#xA;&lt;li&gt;What is Adversarial Machine Learning?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;what-is-cross-validation&#34;&gt;What is Cross-validation?&lt;/h2&gt;&#xA;&lt;p&gt;In machine learning, &lt;strong&gt;cross-validation&lt;/strong&gt; is a technique used to evaluate the performance of a model by partitioning the dataset into subsets, training the model on some of these subsets, and then testing it on the remaining subsets. The goal is to assess how well the model generalizes to unseen data, thus preventing issues like overfitting or underfitting.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Compressing Large Language Model</title>
      <link>http://localhost:1313/dsblog/compressing-llm/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/compressing-llm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6099-Compressing-LLM.jpg&#34; alt=&#34;Compressing Large Language Model&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;compressing-large-language-model&#34;&gt;Compressing Large Language Model&lt;/h1&gt;&#xA;&lt;h2 id=&#34;is-this-article-for-me&#34;&gt;Is this article for me?&lt;/h2&gt;&#xA;&lt;p&gt;If you are looking answers to following question then &amp;ldquo;Yes&amp;rdquo;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What is LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;Why is LLM compression necessary?&lt;/li&gt;&#xA;&lt;li&gt;What are the different techniques for LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;How does quantization work in LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;What is pruning, and how does it help in compressing LLMs?&lt;/li&gt;&#xA;&lt;li&gt;Can you explain knowledge distillation in the context of LLMs?&lt;/li&gt;&#xA;&lt;li&gt;What is low-rank factorization and its role in LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;How effective are weight sharing techniques in compressing LLMs?&lt;/li&gt;&#xA;&lt;li&gt;What are the trade-offs involved in LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;How does fine-tuning work in the context of compressed LLMs?&lt;/li&gt;&#xA;&lt;li&gt;What are the benefits of fine-tuning in compressed LLMs?&lt;/li&gt;&#xA;&lt;li&gt;What role does hardware play in LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;What are the ethical considerations in LLM compression?&lt;/li&gt;&#xA;&lt;li&gt;What are the future directions in LLM compression?&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;1-what-is-llm-compression&#34;&gt;1. &lt;strong&gt;What is LLM Compression?&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;LLM (Large Language Model) compression refers to a set of techniques and methodologies aimed at reducing the size of large language models while maintaining their performance as much as possible. Large language models, such as GPT, BERT, and their variants, often contain hundreds of millions to billions of parameters, making them resource-intensive to deploy and run. The sheer size of these models poses challenges in terms of storage, computation, and real-time inference, especially when deploying on devices with limited hardware resources like mobile phones or edge devices.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model Tuning with VertexAI</title>
      <link>http://localhost:1313/dsblog/Model-Tuning-with-VertexAI/</link>
      <pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Model-Tuning-with-VertexAI/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6081-Model-Tuning-with-VertexAI.jpg&#34; alt=&#34;Model Tuning with VertexAI&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;tuning-large-language-model-with-vertexai&#34;&gt;Tuning Large Language Model with VertexAI&lt;/h1&gt;&#xA;&lt;h2 id=&#34;why-model-tuning&#34;&gt;Why Model Tuning?&lt;/h2&gt;&#xA;&lt;p&gt;Tuning is required when you want the model to learn something niche or specific that deviates from general language patterns.&lt;/p&gt;&#xA;&lt;h2 id=&#34;goal-of-tuning&#34;&gt;Goal of Tuning&lt;/h2&gt;&#xA;&lt;h3 id=&#34;classification&#34;&gt;Classification&lt;/h3&gt;&#xA;&lt;p&gt;prompt: &amp;ldquo;Classify the following text into one of the following classes: [business, entertainment].&amp;rdquo;&lt;/p&gt;&#xA;&lt;h3 id=&#34;summarization&#34;&gt;Summarization&lt;/h3&gt;&#xA;&lt;p&gt;prompt: &amp;ldquo;Summarize: Jessica: That sounds great! See you in Times Square! Alexander: See you at 10!&amp;rdquo;&lt;/p&gt;&#xA;&lt;p&gt;response: &amp;ldquo;#Person1 and #Person2 agree to meet at Times Square at 10:00 AM.&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cost Functions and Optimizers in Machine Learning</title>
      <link>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/Cost-Functions-and-Optimizers-in-Machine-Learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6045-Cost-Functions-and-Optimizers-in-Machine-Learning.jpg&#34; alt=&#34;Cost-Functions-and-Optimizers-in-Machine-Learning&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;cost-functions-and-optimizers-in-machine-learning&#34;&gt;Cost-Functions-and-Optimizers-in-Machine-Learning&lt;/h1&gt;&#xA;&lt;h2 id=&#34;what-is-machine-learning&#34;&gt;What is machine learning?&lt;/h2&gt;&#xA;&lt;p&gt;Machine learning is a subfield of artificial intelligence that focuses on the &lt;strong&gt;development of algorithms and statistical models&lt;/strong&gt; that enable computers to improve their performance on a specific task through experience.&lt;/p&gt;&#xA;&lt;p&gt;In machine learning, the goal is to develop models that can &lt;strong&gt;automatically learn patterns and relationships in data, and use that knowledge to make predictions or take actions&lt;/strong&gt;. The models are trained on a large dataset, and the learning process involves &lt;strong&gt;optimizing the parameters of the model to minimize the prediction error&lt;/strong&gt;. For this purpose every algorithms uses some &lt;strong&gt;cost function or loss function&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
