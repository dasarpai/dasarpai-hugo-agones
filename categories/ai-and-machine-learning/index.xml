<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai-and-Machine-Learning on Agones</title>
    <link>http://localhost:1313/categories/ai-and-machine-learning/</link>
    <description>Recent content in Ai-and-Machine-Learning on Agones</description>
    <generator>Hugo</generator>
    <language>en</language>
    <managingEditor>hari@dasarpai.com (Hari Thapliyaal)</managingEditor>
    <webMaster>hari@dasarpai.com (Hari Thapliyaal)</webMaster>
    <lastBuildDate>Thu, 08 May 2025 11:34:17 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/ai-and-machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Whisper: Robust Speech Recognition via Large-Scale Weak Supervision</title>
      <link>http://localhost:1313/dsblog/whisper-large-scale-weak-supervision-speech-recognition/</link>
      <pubDate>Fri, 25 Apr 2025 00:00:00 +0000</pubDate><author>hari@dasarpai.com (Hari Thapliyaal)</author>
      <guid>http://localhost:1313/dsblog/whisper-large-scale-weak-supervision-speech-recognition/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/assets/images/dspost/dsp6266-whisper-large-scale-weak-supervision-speech-recognition.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;whisper-robust-speech-recognition-via-large-scale-weak-supervision-whisper&#34;&gt;Whisper: Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Briefing Document:&lt;/strong&gt; Whisper: Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; Excerpts from &amp;ldquo;&lt;a href=&#34;https://arxiv.org/pdf/2212.04356%22&#34;&gt;https://arxiv.org/pdf/2212.04356&#34;&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Date:&lt;/strong&gt; December 2022 (Based on arXiv preprint date)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Organization:&lt;/strong&gt; OpenAI&lt;/p&gt;&#xA;&lt;h2 id=&#34;what-problem-is-addressed&#34;&gt;What Problem is Addressed?&lt;/h2&gt;&#xA;&lt;p&gt;This paper is about Speech to text transcription.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Existing automatic speech recognition (ASR) models lack robustness and generalization&lt;/strong&gt;. They perform well on data similar to their training data but struggle on different datasets or conditions (out-of-distribution data), even showing &amp;ldquo;superhuman&amp;rdquo; performance on specific benchmarks.&lt;/li&gt;&#xA;&lt;li&gt;Many state-of-the-art ASR systems &lt;strong&gt;require fine-tuning on specific datasets&lt;/strong&gt; to achieve high quality. Whisper aims to work well &lt;strong&gt;zero-shot&lt;/strong&gt;, without needing dataset-specific fine-tuning.&lt;/li&gt;&#xA;&lt;li&gt;There is a &lt;strong&gt;limited amount of easily available, high-quality supervised speech data&lt;/strong&gt;. Whisper tackles this by using a much larger scale of &lt;strong&gt;weakly supervised data (680,000 hours)&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Traditional speech processing often involves &lt;strong&gt;multiple separate components&lt;/strong&gt; for tasks like voice activity detection or translation. The paper seeks to create a &lt;strong&gt;single model&lt;/strong&gt; that can perform the entire pipeline, handling transcription, translation, language identification, and voice activity detection.&lt;/li&gt;&#xA;&lt;li&gt;Transcribing &lt;strong&gt;long audio recordings&lt;/strong&gt; is challenging for models trained on short segments. The paper develops strategies for &lt;strong&gt;buffered transcription of long audio&lt;/strong&gt; to make it practical for real-world use.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;executive-summary&#34;&gt;&lt;strong&gt;Executive Summary:&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;p&gt;This paper introduces Whisper, a speech processing system trained on a massive dataset of 680,000 hours of diverse, weakly supervised audio and their transcripts from the internet. The core finding is that &lt;strong&gt;simple scaling of weakly supervised pre-training has been significantly underappreciated&lt;/strong&gt; for speech recognition. Unlike many recent large-scale speech recognition models that rely heavily on unsupervised pre-training or self-training, Whisper achieves strong performance across various tasks (multilingual speech recognition, speech translation, language identification, voice activity detection) and generalizes remarkably well to standard benchmarks in a zero-shot setting, without the need for fine-tuning. When compared to humans, the models approach their accuracy and robustness, particularly in out-of-distribution scenarios where traditional supervised models struggle. The authors release models and inference code to promote further research in robust speech processing.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
