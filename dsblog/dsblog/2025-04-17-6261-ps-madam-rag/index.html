<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/site/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=site/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/site/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/site/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/site/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Retrieval-Augmented Generation with Conflicting Evidence | Agones</title><meta property="og:url" content="http://localhost:1313/site/dsblog/dsblog/2025-04-17-6261-ps-madam-rag/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Retrieval-Augmented Generation with Conflicting Evidence">
  <meta property="og:description" content="Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence arXiv Paper
The hypothesis of this paper is that real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents. The authors argue that prior work has largely addressed these challenges in isolation.
Key learnings from this paper include: Real-world RAG encounters a complex interplay of ambiguity, misinformation, and noise in retrieved documents. Existing RAG evaluation benchmarks and methods often focus on individual aspects of conflict, such as ambiguity or misinformation, but do not adequately address their simultaneous occurrence. Different types of conflict necessitate different handling strategies. Ambiguous queries might require presenting multiple valid answers, while misinformation and noise should be filtered out. The newly introduced RAMDocs dataset, designed to simulate these complex real-world scenarios, poses a significant challenge for current RAG baselines, including strong LLMs. Even the best-performing baseline on RAMDocs achieved a relatively low exact match score. The proposed MADAM-RAG framework, which employs a multi-agent debate mechanism, demonstrates effectiveness in jointly handling diverse sources of conflict, showing improvements over strong RAG baselines on AmbigDocs (handling ambiguity) and FaithEval (suppressing misinformation). Ablation studies on MADAM-RAG highlight the importance of both the aggregator module and the multi-round debate process in achieving its performance gains. The paper finds that imbalances in the number of supporting documents for different valid answers can lead to standard RAG systems favoring the more frequently supported answer. Increasing the level of misinformation in retrieved documents negatively impacts the performance of RAG systems, even strong LLMs. MADAM-RAG shows more resilience to this compared to baselines. The new methods suggested in this paper are: RAMDocs (Retrieval with Ambiguity and Misinformation in Documents): This is a novel dataset specifically constructed to evaluate RAG systems’ ability to handle conflicting information arising from ambiguity, misinformation, and noise simultaneously. It also features variability in the number of documents supporting different valid answers. MADAM-RAG (Multi-agent Debate for Ambiguity and Misinformation in RAG): This is a new multi-agent framework designed to address the challenges posed by RAMDocs. In MADAM-RAG: Each retrieved document is assigned to an independent LLM agent that generates an initial response based solely on its assigned document. These agents then engage in a multi-round debate, where they can revise their answers based on a summary of the previous round’s responses provided by a centralized aggregator module. The aggregator module synthesizes a final response from the agent discussions, aiming to present all valid answers for ambiguous queries while discarding misinformation and noise. The final output of this paper includes: The introduction of the RAMDocs dataset, which serves as a challenging benchmark for evaluating RAG systems under realistic conditions of conflicting information. The dataset statistics, highlighting the average number of valid answers and the distribution of supporting, misinformation, and noisy documents, are provided. The proposal and empirical evaluation of the MADAM-RAG framework. The results demonstrate that MADAM-RAG outperforms several strong RAG baselines (No RAG, Concatenated-prompt, and Astute RAG) on FaithEval (misinformation), AmbigDocs (ambiguity), and the new RAMDocs dataset. Detailed ablation studies that highlight the contribution of the aggregator and the multi-round debate mechanism to MADAM-RAG’s performance. Analysis of the impact of varying the number of supporting documents for correct answers and the impact of increasing levels of misinformation on the performance of different RAG systems, including MADAM-RAG. The paper concludes by acknowledging that while MADAM-RAG shows promise, RAMDocs remains a challenging task, indicating room for future improvements in handling complex conflicting information in RAG systems.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2025-04-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-04-17T00:00:00+00:00">
    <meta property="article:tag" content="MADAM-RAG">
    <meta property="article:tag" content="Retrieval-Augmented Generation">
    <meta property="article:tag" content="Conflicting Evidence">
    <meta property="article:tag" content="Large Language Models">
    <meta property="article:tag" content="AI and NLP">
    <meta property="article:tag" content="Paper Summary">

  <meta itemprop="name" content="Retrieval-Augmented Generation with Conflicting Evidence">
  <meta itemprop="description" content="Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence arXiv Paper
The hypothesis of this paper is that real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents. The authors argue that prior work has largely addressed these challenges in isolation.
Key learnings from this paper include: Real-world RAG encounters a complex interplay of ambiguity, misinformation, and noise in retrieved documents. Existing RAG evaluation benchmarks and methods often focus on individual aspects of conflict, such as ambiguity or misinformation, but do not adequately address their simultaneous occurrence. Different types of conflict necessitate different handling strategies. Ambiguous queries might require presenting multiple valid answers, while misinformation and noise should be filtered out. The newly introduced RAMDocs dataset, designed to simulate these complex real-world scenarios, poses a significant challenge for current RAG baselines, including strong LLMs. Even the best-performing baseline on RAMDocs achieved a relatively low exact match score. The proposed MADAM-RAG framework, which employs a multi-agent debate mechanism, demonstrates effectiveness in jointly handling diverse sources of conflict, showing improvements over strong RAG baselines on AmbigDocs (handling ambiguity) and FaithEval (suppressing misinformation). Ablation studies on MADAM-RAG highlight the importance of both the aggregator module and the multi-round debate process in achieving its performance gains. The paper finds that imbalances in the number of supporting documents for different valid answers can lead to standard RAG systems favoring the more frequently supported answer. Increasing the level of misinformation in retrieved documents negatively impacts the performance of RAG systems, even strong LLMs. MADAM-RAG shows more resilience to this compared to baselines. The new methods suggested in this paper are: RAMDocs (Retrieval with Ambiguity and Misinformation in Documents): This is a novel dataset specifically constructed to evaluate RAG systems’ ability to handle conflicting information arising from ambiguity, misinformation, and noise simultaneously. It also features variability in the number of documents supporting different valid answers. MADAM-RAG (Multi-agent Debate for Ambiguity and Misinformation in RAG): This is a new multi-agent framework designed to address the challenges posed by RAMDocs. In MADAM-RAG: Each retrieved document is assigned to an independent LLM agent that generates an initial response based solely on its assigned document. These agents then engage in a multi-round debate, where they can revise their answers based on a summary of the previous round’s responses provided by a centralized aggregator module. The aggregator module synthesizes a final response from the agent discussions, aiming to present all valid answers for ambiguous queries while discarding misinformation and noise. The final output of this paper includes: The introduction of the RAMDocs dataset, which serves as a challenging benchmark for evaluating RAG systems under realistic conditions of conflicting information. The dataset statistics, highlighting the average number of valid answers and the distribution of supporting, misinformation, and noisy documents, are provided. The proposal and empirical evaluation of the MADAM-RAG framework. The results demonstrate that MADAM-RAG outperforms several strong RAG baselines (No RAG, Concatenated-prompt, and Astute RAG) on FaithEval (misinformation), AmbigDocs (ambiguity), and the new RAMDocs dataset. Detailed ablation studies that highlight the contribution of the aggregator and the multi-round debate mechanism to MADAM-RAG’s performance. Analysis of the impact of varying the number of supporting documents for correct answers and the impact of increasing levels of misinformation on the performance of different RAG systems, including MADAM-RAG. The paper concludes by acknowledging that while MADAM-RAG shows promise, RAMDocs remains a challenging task, indicating room for future improvements in handling complex conflicting information in RAG systems.">
  <meta itemprop="datePublished" content="2025-04-17T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-04-17T00:00:00+00:00">
  <meta itemprop="wordCount" content="587">
  <meta itemprop="keywords" content="paper summary,MADAM-RAG,retrieval-augmented generation,conflicting evidence,large language models,AI and NLP">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Retrieval-Augmented Generation with Conflicting Evidence">
  <meta name="twitter:description" content="Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence arXiv Paper
The hypothesis of this paper is that real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents. The authors argue that prior work has largely addressed these challenges in isolation.
Key learnings from this paper include: Real-world RAG encounters a complex interplay of ambiguity, misinformation, and noise in retrieved documents. Existing RAG evaluation benchmarks and methods often focus on individual aspects of conflict, such as ambiguity or misinformation, but do not adequately address their simultaneous occurrence. Different types of conflict necessitate different handling strategies. Ambiguous queries might require presenting multiple valid answers, while misinformation and noise should be filtered out. The newly introduced RAMDocs dataset, designed to simulate these complex real-world scenarios, poses a significant challenge for current RAG baselines, including strong LLMs. Even the best-performing baseline on RAMDocs achieved a relatively low exact match score. The proposed MADAM-RAG framework, which employs a multi-agent debate mechanism, demonstrates effectiveness in jointly handling diverse sources of conflict, showing improvements over strong RAG baselines on AmbigDocs (handling ambiguity) and FaithEval (suppressing misinformation). Ablation studies on MADAM-RAG highlight the importance of both the aggregator module and the multi-round debate process in achieving its performance gains. The paper finds that imbalances in the number of supporting documents for different valid answers can lead to standard RAG systems favoring the more frequently supported answer. Increasing the level of misinformation in retrieved documents negatively impacts the performance of RAG systems, even strong LLMs. MADAM-RAG shows more resilience to this compared to baselines. The new methods suggested in this paper are: RAMDocs (Retrieval with Ambiguity and Misinformation in Documents): This is a novel dataset specifically constructed to evaluate RAG systems’ ability to handle conflicting information arising from ambiguity, misinformation, and noise simultaneously. It also features variability in the number of documents supporting different valid answers. MADAM-RAG (Multi-agent Debate for Ambiguity and Misinformation in RAG): This is a new multi-agent framework designed to address the challenges posed by RAMDocs. In MADAM-RAG: Each retrieved document is assigned to an independent LLM agent that generates an initial response based solely on its assigned document. These agents then engage in a multi-round debate, where they can revise their answers based on a summary of the previous round’s responses provided by a centralized aggregator module. The aggregator module synthesizes a final response from the agent discussions, aiming to present all valid answers for ambiguous queries while discarding misinformation and noise. The final output of this paper includes: The introduction of the RAMDocs dataset, which serves as a challenging benchmark for evaluating RAG systems under realistic conditions of conflicting information. The dataset statistics, highlighting the average number of valid answers and the distribution of supporting, misinformation, and noisy documents, are provided. The proposal and empirical evaluation of the MADAM-RAG framework. The results demonstrate that MADAM-RAG outperforms several strong RAG baselines (No RAG, Concatenated-prompt, and Astute RAG) on FaithEval (misinformation), AmbigDocs (ambiguity), and the new RAMDocs dataset. Detailed ablation studies that highlight the contribution of the aggregator and the multi-round debate mechanism to MADAM-RAG’s performance. Analysis of the impact of varying the number of supporting documents for correct answers and the impact of increasing levels of misinformation on the performance of different RAG systems, including MADAM-RAG. The paper concludes by acknowledging that while MADAM-RAG shows promise, RAMDocs remains a challenging task, indicating room for future improvements in handling complex conflicting information in RAG systems.">



<link rel="stylesheet" href="/site/css/prism.css"/>

<link href="/site/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/site/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/site/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete="off">
</div></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <main role="main" class="td-main">
        <p><img src="/assets/images/dspost/dsp6261-Retrieval-Augmented-Generation-with-Conflicting-Evidence.jpg" alt=""></p>
<h1 id="paper-summary-retrieval-augmented-generation-with-conflicting-evidence">Paper Summary: Retrieval-Augmented Generation with Conflicting Evidence</h1>
<p><a href="https://arxiv.org/pdf/2504.13079">arXiv Paper</a></p>
<p>The hypothesis of this paper is that <strong>real-world retrieval-augmented generation (RAG) systems must simultaneously handle various sources of conflicting information, including ambiguity in user queries and contradictory information arising from misinformation and noise in retrieved documents</strong>. The authors argue that prior work has largely addressed these challenges in isolation.</p>
<iframe width="560" height="315" 
        src="https://www.youtube.com/embed/hbJaC2HI89s" 
        title="Retrieval-Augmented Generation with Conflicting Evidence" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        allowfullscreen>
</iframe>
<h2 id="key-learnings-from-this-paper-include">Key learnings from this paper include:</h2>
<ul>
<li><strong>Real-world RAG encounters a complex interplay of ambiguity, misinformation, and noise in retrieved documents</strong>.</li>
<li><strong>Existing RAG evaluation benchmarks and methods often focus on individual aspects of conflict</strong>, such as ambiguity or misinformation, but do not adequately address their simultaneous occurrence.</li>
<li><strong>Different types of conflict necessitate different handling strategies</strong>. Ambiguous queries might require presenting multiple valid answers, while misinformation and noise should be filtered out.</li>
<li>The newly introduced <strong>RAMDocs dataset, designed to simulate these complex real-world scenarios, poses a significant challenge for current RAG baselines</strong>, including strong LLMs. Even the best-performing baseline on RAMDocs achieved a relatively low exact match score.</li>
<li>The proposed <strong>MADAM-RAG framework, which employs a multi-agent debate mechanism, demonstrates effectiveness in jointly handling diverse sources of conflict</strong>, showing improvements over strong RAG baselines on AmbigDocs (handling ambiguity) and FaithEval (suppressing misinformation).</li>
<li>Ablation studies on MADAM-RAG highlight the <strong>importance of both the aggregator module and the multi-round debate process</strong> in achieving its performance gains.</li>
<li>The paper finds that <strong>imbalances in the number of supporting documents for different valid answers can lead to standard RAG systems favoring the more frequently supported answer</strong>.</li>
<li>Increasing the <strong>level of misinformation in retrieved documents negatively impacts the performance of RAG systems</strong>, even strong LLMs. MADAM-RAG shows more resilience to this compared to baselines.</li>
</ul>
<h2 id="the-new-methods-suggested-in-this-paper-are">The new methods suggested in this paper are:</h2>
<ul>
<li><strong>RAMDocs (Retrieval with Ambiguity and Misinformation in Documents)</strong>: This is a novel dataset specifically constructed to evaluate RAG systems&rsquo; ability to handle conflicting information arising from ambiguity, misinformation, and noise simultaneously. It also features variability in the number of documents supporting different valid answers.</li>
<li><strong>MADAM-RAG (Multi-agent Debate for Ambiguity and Misinformation in RAG)</strong>: This is a new multi-agent framework designed to address the challenges posed by RAMDocs. In MADAM-RAG:
<ul>
<li>Each retrieved document is assigned to an <strong>independent LLM agent</strong> that generates an initial response based solely on its assigned document.</li>
<li>These agents then engage in a <strong>multi-round debate</strong>, where they can revise their answers based on a summary of the previous round&rsquo;s responses provided by a centralized <strong>aggregator module</strong>.</li>
<li>The <strong>aggregator module</strong> synthesizes a final response from the agent discussions, aiming to present all valid answers for ambiguous queries while discarding misinformation and noise.</li>
</ul>
</li>
</ul>
<h2 id="the-final-output-of-this-paper-includes">The final output of this paper includes:</h2>
<ul>
<li>The <strong>introduction of the RAMDocs dataset</strong>, which serves as a challenging benchmark for evaluating RAG systems under realistic conditions of conflicting information. The dataset statistics, highlighting the average number of valid answers and the distribution of supporting, misinformation, and noisy documents, are provided.</li>
<li>The <strong>proposal and empirical evaluation of the MADAM-RAG framework</strong>. The results demonstrate that MADAM-RAG outperforms several strong RAG baselines (No RAG, Concatenated-prompt, and Astute RAG) on FaithEval (misinformation), AmbigDocs (ambiguity), and the new RAMDocs dataset.</li>
<li><strong>Detailed ablation studies</strong> that highlight the contribution of the aggregator and the multi-round debate mechanism to MADAM-RAG&rsquo;s performance.</li>
<li><strong>Analysis of the impact of varying the number of supporting documents</strong> for correct answers and the impact of <strong>increasing levels of misinformation</strong> on the performance of different RAG systems, including MADAM-RAG.</li>
<li>The paper concludes by acknowledging that while MADAM-RAG shows promise, <strong>RAMDocs remains a challenging task, indicating room for future improvements in handling complex conflicting information in RAG systems</strong>.</li>
</ul>

      </main>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/site/js/main.js"></script>
<script src='/site/js/prism.js'></script>
<script src='/site/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/site/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>