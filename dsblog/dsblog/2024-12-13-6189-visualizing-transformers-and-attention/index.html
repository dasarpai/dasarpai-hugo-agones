<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/site/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=site/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/site/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/site/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/site/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Visualizing Transformers and Attention | Agones</title><meta property="og:url" content="http://localhost:1313/site/dsblog/dsblog/2024-12-13-6189-visualizing-transformers-and-attention/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Visualizing Transformers and Attention">
  <meta property="og:description" content="Visualizing Transformers and Attention This is the summary note from Grant Sandersonâ€™s talk at TNG Big Tech 2024. My earlir article on transformers can be found here
Transformers and Their Flexibility ğŸ“œ Origin: Introduced in 2017 in the â€œAttention is All You Needâ€ paper, originally for machine translation. ğŸŒ Applications Beyond Translation: Used in transcription (e.g., Whisper), text-to-speech, and even image classification. ğŸ¤– Chatbot Models: Focused on models trained to predict the next token in a sequence, generating text iteratively one token at a time. Next Token Prediction and Creativity ğŸ”® Prediction Process: Predicts probabilities for possible next tokens, selects one, and repeats the process. ğŸŒ¡ï¸ Temperature Control: Adjusting randomness in token selection affects creativity vs. predictability in outputs. Tokens and Tokenization ğŸ§© What are Tokens? Subdivisions of input data (words, subwords, punctuation, or image patches). ğŸ”¡ Why Not Characters? Using characters increases context size and computational complexity; tokens balance meaning and computational efficiency. ğŸ“– Byte Pair Encoding (BPE): A common method for tokenization. Embedding Tokens into Vectors ğŸ“ Embedding: Tokens are mapped to high-dimensional vectors representing their meaning. ğŸ—ºï¸ Contextual Meaning: Vectors evolve through the network to capture context, disambiguate meaning, and encode relationships. The Attention Mechanism ğŸ” Purpose: Enables tokens to â€œattendâ€ to others, updating their vectors based on relevance. ğŸ”‘ Key Components: Query Matrix: Encodes what a token is â€œlooking for.â€ Key Matrix: Encodes how a token responds to queries. Value Matrix: Encodes information passed between tokens. ğŸ§® Calculations: Dot Product: Measures alignment between keys and queries. Softmax: Converts dot products into normalized weights for updates. â›“ï¸ Masked Attention: Ensures causality by blocking future tokens from influencing past ones. Multi-Headed Attention ğŸ’¡ Parallel Heads: Multiple attention heads allow different types of relationships (e.g., grammar, semantic context) to be processed simultaneously. ğŸš€ Efficiency on GPUs: Designed to maximize parallelization for faster computation. Multi-Layer Perceptrons (MLPs) ğŸ¤” Role in Transformers: Add capacity for general knowledge and non-contextual reasoning. Store facts learned during training, e.g., associations like â€œMichael Jordan plays basketball.â€ ğŸ”¢ Parameters: MLPs hold the majority of the modelâ€™s parameters. Training Transformers ğŸ“š Learning Framework: Models are trained on vast datasets using next-token prediction, requiring no manual labels. Cost Function: Measures prediction accuracy using negative log probabilities, guiding parameter updates. ğŸ”ï¸ Optimization: Gradient descent navigates a high-dimensional cost surface to minimize error. ğŸŒ Pretraining: Allows large-scale unsupervised learning before fine-tuning with human feedback. Embedding Space and High Dimensions ğŸ”„ Semantic Clusters: Similar words cluster together; directions in the space encode relationships (e.g., gender: King - Male &#43; Female = Queen). ğŸŒŒ High Dimensionality: Embedding spaces have thousands of dimensions, enabling distinct representations of complex concepts. ğŸ“ˆ Scaling Efficiency: High-dimensional spaces allow exponentially more â€œalmost orthogonalâ€ directions for encoding meanings. Practical Applications âœï¸ Language Models: Effective for chatbots, summarization, and more due to their generality and parallel processing. ğŸ–¼ï¸ Multimodal Models: Transformers can integrate text, images, and sound by treating all as tokens in a unified framework. Challenges and Limitations ğŸ“ Context Size Limitations: Attention grows quadratically with context size, requiring optimization for large contexts. â™»ï¸ Inference Redundancy: Token-by-token generation can involve redundant computations; caching mitigates this at inference time. Engineering and Design ğŸ› ï¸ Hardware Optimization: Transformers are designed to exploit GPUsâ€™ parallelism for efficient matrix multiplication. ğŸ”— Residual Connections: Baked into the architecture to enhance stability and ease of training. The Power of Scale ğŸ“ˆ Scaling Laws: Larger models and more data improve performance, often qualitatively. ğŸ”„ Self-Supervised Pretraining: Enables training on vast unlabeled datasets before fine-tuning. BPE (Byte Pair Encoding) BPE is a widely used tokenization method in natural language processing (NLP) and machine learning. It is designed to balance between breaking text into characters and full words by representing text as a sequence of subword units. This approach helps models handle rare and unseen words effectively while keeping the vocabulary size manageable.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2024-12-13T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-13T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Transformers">
    <meta property="article:tag" content="Attention">
    <meta property="article:tag" content="Grant Sanderson">

  <meta itemprop="name" content="Visualizing Transformers and Attention">
  <meta itemprop="description" content="Visualizing Transformers and Attention This is the summary note from Grant Sandersonâ€™s talk at TNG Big Tech 2024. My earlir article on transformers can be found here
Transformers and Their Flexibility ğŸ“œ Origin: Introduced in 2017 in the â€œAttention is All You Needâ€ paper, originally for machine translation. ğŸŒ Applications Beyond Translation: Used in transcription (e.g., Whisper), text-to-speech, and even image classification. ğŸ¤– Chatbot Models: Focused on models trained to predict the next token in a sequence, generating text iteratively one token at a time. Next Token Prediction and Creativity ğŸ”® Prediction Process: Predicts probabilities for possible next tokens, selects one, and repeats the process. ğŸŒ¡ï¸ Temperature Control: Adjusting randomness in token selection affects creativity vs. predictability in outputs. Tokens and Tokenization ğŸ§© What are Tokens? Subdivisions of input data (words, subwords, punctuation, or image patches). ğŸ”¡ Why Not Characters? Using characters increases context size and computational complexity; tokens balance meaning and computational efficiency. ğŸ“– Byte Pair Encoding (BPE): A common method for tokenization. Embedding Tokens into Vectors ğŸ“ Embedding: Tokens are mapped to high-dimensional vectors representing their meaning. ğŸ—ºï¸ Contextual Meaning: Vectors evolve through the network to capture context, disambiguate meaning, and encode relationships. The Attention Mechanism ğŸ” Purpose: Enables tokens to â€œattendâ€ to others, updating their vectors based on relevance. ğŸ”‘ Key Components: Query Matrix: Encodes what a token is â€œlooking for.â€ Key Matrix: Encodes how a token responds to queries. Value Matrix: Encodes information passed between tokens. ğŸ§® Calculations: Dot Product: Measures alignment between keys and queries. Softmax: Converts dot products into normalized weights for updates. â›“ï¸ Masked Attention: Ensures causality by blocking future tokens from influencing past ones. Multi-Headed Attention ğŸ’¡ Parallel Heads: Multiple attention heads allow different types of relationships (e.g., grammar, semantic context) to be processed simultaneously. ğŸš€ Efficiency on GPUs: Designed to maximize parallelization for faster computation. Multi-Layer Perceptrons (MLPs) ğŸ¤” Role in Transformers: Add capacity for general knowledge and non-contextual reasoning. Store facts learned during training, e.g., associations like â€œMichael Jordan plays basketball.â€ ğŸ”¢ Parameters: MLPs hold the majority of the modelâ€™s parameters. Training Transformers ğŸ“š Learning Framework: Models are trained on vast datasets using next-token prediction, requiring no manual labels. Cost Function: Measures prediction accuracy using negative log probabilities, guiding parameter updates. ğŸ”ï¸ Optimization: Gradient descent navigates a high-dimensional cost surface to minimize error. ğŸŒ Pretraining: Allows large-scale unsupervised learning before fine-tuning with human feedback. Embedding Space and High Dimensions ğŸ”„ Semantic Clusters: Similar words cluster together; directions in the space encode relationships (e.g., gender: King - Male &#43; Female = Queen). ğŸŒŒ High Dimensionality: Embedding spaces have thousands of dimensions, enabling distinct representations of complex concepts. ğŸ“ˆ Scaling Efficiency: High-dimensional spaces allow exponentially more â€œalmost orthogonalâ€ directions for encoding meanings. Practical Applications âœï¸ Language Models: Effective for chatbots, summarization, and more due to their generality and parallel processing. ğŸ–¼ï¸ Multimodal Models: Transformers can integrate text, images, and sound by treating all as tokens in a unified framework. Challenges and Limitations ğŸ“ Context Size Limitations: Attention grows quadratically with context size, requiring optimization for large contexts. â™»ï¸ Inference Redundancy: Token-by-token generation can involve redundant computations; caching mitigates this at inference time. Engineering and Design ğŸ› ï¸ Hardware Optimization: Transformers are designed to exploit GPUsâ€™ parallelism for efficient matrix multiplication. ğŸ”— Residual Connections: Baked into the architecture to enhance stability and ease of training. The Power of Scale ğŸ“ˆ Scaling Laws: Larger models and more data improve performance, often qualitatively. ğŸ”„ Self-Supervised Pretraining: Enables training on vast unlabeled datasets before fine-tuning. BPE (Byte Pair Encoding) BPE is a widely used tokenization method in natural language processing (NLP) and machine learning. It is designed to balance between breaking text into characters and full words by representing text as a sequence of subword units. This approach helps models handle rare and unseen words effectively while keeping the vocabulary size manageable.">
  <meta itemprop="datePublished" content="2024-12-13T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-12-13T00:00:00+00:00">
  <meta itemprop="wordCount" content="723">
  <meta itemprop="keywords" content="visualizing transformers and attention,transformers attention mechanism,grant sanderson talk at tng big tech 2024,ai and nlp concepts,transformers demystified,attention is all you need paper,language models and transformers">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Visualizing Transformers and Attention">
  <meta name="twitter:description" content="Visualizing Transformers and Attention This is the summary note from Grant Sandersonâ€™s talk at TNG Big Tech 2024. My earlir article on transformers can be found here
Transformers and Their Flexibility ğŸ“œ Origin: Introduced in 2017 in the â€œAttention is All You Needâ€ paper, originally for machine translation. ğŸŒ Applications Beyond Translation: Used in transcription (e.g., Whisper), text-to-speech, and even image classification. ğŸ¤– Chatbot Models: Focused on models trained to predict the next token in a sequence, generating text iteratively one token at a time. Next Token Prediction and Creativity ğŸ”® Prediction Process: Predicts probabilities for possible next tokens, selects one, and repeats the process. ğŸŒ¡ï¸ Temperature Control: Adjusting randomness in token selection affects creativity vs. predictability in outputs. Tokens and Tokenization ğŸ§© What are Tokens? Subdivisions of input data (words, subwords, punctuation, or image patches). ğŸ”¡ Why Not Characters? Using characters increases context size and computational complexity; tokens balance meaning and computational efficiency. ğŸ“– Byte Pair Encoding (BPE): A common method for tokenization. Embedding Tokens into Vectors ğŸ“ Embedding: Tokens are mapped to high-dimensional vectors representing their meaning. ğŸ—ºï¸ Contextual Meaning: Vectors evolve through the network to capture context, disambiguate meaning, and encode relationships. The Attention Mechanism ğŸ” Purpose: Enables tokens to â€œattendâ€ to others, updating their vectors based on relevance. ğŸ”‘ Key Components: Query Matrix: Encodes what a token is â€œlooking for.â€ Key Matrix: Encodes how a token responds to queries. Value Matrix: Encodes information passed between tokens. ğŸ§® Calculations: Dot Product: Measures alignment between keys and queries. Softmax: Converts dot products into normalized weights for updates. â›“ï¸ Masked Attention: Ensures causality by blocking future tokens from influencing past ones. Multi-Headed Attention ğŸ’¡ Parallel Heads: Multiple attention heads allow different types of relationships (e.g., grammar, semantic context) to be processed simultaneously. ğŸš€ Efficiency on GPUs: Designed to maximize parallelization for faster computation. Multi-Layer Perceptrons (MLPs) ğŸ¤” Role in Transformers: Add capacity for general knowledge and non-contextual reasoning. Store facts learned during training, e.g., associations like â€œMichael Jordan plays basketball.â€ ğŸ”¢ Parameters: MLPs hold the majority of the modelâ€™s parameters. Training Transformers ğŸ“š Learning Framework: Models are trained on vast datasets using next-token prediction, requiring no manual labels. Cost Function: Measures prediction accuracy using negative log probabilities, guiding parameter updates. ğŸ”ï¸ Optimization: Gradient descent navigates a high-dimensional cost surface to minimize error. ğŸŒ Pretraining: Allows large-scale unsupervised learning before fine-tuning with human feedback. Embedding Space and High Dimensions ğŸ”„ Semantic Clusters: Similar words cluster together; directions in the space encode relationships (e.g., gender: King - Male &#43; Female = Queen). ğŸŒŒ High Dimensionality: Embedding spaces have thousands of dimensions, enabling distinct representations of complex concepts. ğŸ“ˆ Scaling Efficiency: High-dimensional spaces allow exponentially more â€œalmost orthogonalâ€ directions for encoding meanings. Practical Applications âœï¸ Language Models: Effective for chatbots, summarization, and more due to their generality and parallel processing. ğŸ–¼ï¸ Multimodal Models: Transformers can integrate text, images, and sound by treating all as tokens in a unified framework. Challenges and Limitations ğŸ“ Context Size Limitations: Attention grows quadratically with context size, requiring optimization for large contexts. â™»ï¸ Inference Redundancy: Token-by-token generation can involve redundant computations; caching mitigates this at inference time. Engineering and Design ğŸ› ï¸ Hardware Optimization: Transformers are designed to exploit GPUsâ€™ parallelism for efficient matrix multiplication. ğŸ”— Residual Connections: Baked into the architecture to enhance stability and ease of training. The Power of Scale ğŸ“ˆ Scaling Laws: Larger models and more data improve performance, often qualitatively. ğŸ”„ Self-Supervised Pretraining: Enables training on vast unlabeled datasets before fine-tuning. BPE (Byte Pair Encoding) BPE is a widely used tokenization method in natural language processing (NLP) and machine learning. It is designed to balance between breaking text into characters and full words by representing text as a sequence of subword units. This approach helps models handle rare and unseen words effectively while keeping the vocabulary size manageable.">



<link rel="stylesheet" href="/site/css/prism.css"/>

<link href="/site/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/site/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/site/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this siteâ€¦" aria-label="Search this siteâ€¦" autocomplete="off">
</div></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <main role="main" class="td-main">
        <p><img src="/assets/images/dspost/dsp6189-Visualizing-transformers-and-attention.jpg" alt="Visualizing transformers and attention"></p>
<h1 id="visualizing-transformers-and-attention">Visualizing Transformers and Attention</h1>
<p>This is the summary note from Grant Sanderson&rsquo;s talk at TNG Big Tech 2024. My earlir article on transformers can be found <a href="/dsblog/transformers-demystified-a-step-by-step-guide">here</a></p>
<h2 id="transformers-and-their-flexibility"><strong>Transformers and Their Flexibility</strong></h2>
<ul>
<li>ğŸ“œ <strong>Origin:</strong> Introduced in 2017 in the &ldquo;Attention is All You Need&rdquo; paper, originally for machine translation.</li>
<li>ğŸŒ <strong>Applications Beyond Translation:</strong> Used in transcription (e.g., Whisper), text-to-speech, and even image classification.</li>
<li>ğŸ¤– <strong>Chatbot Models:</strong> Focused on models trained to predict the next token in a sequence, generating text iteratively one token at a time.</li>
</ul>
<hr>
<h2 id="next-token-prediction-and-creativity"><strong>Next Token Prediction and Creativity</strong></h2>
<ul>
<li>ğŸ”® <strong>Prediction Process:</strong> Predicts probabilities for possible next tokens, selects one, and repeats the process.</li>
<li>ğŸŒ¡ï¸ <strong>Temperature Control:</strong> Adjusting randomness in token selection affects creativity vs. predictability in outputs.</li>
</ul>
<hr>
<h2 id="tokens-and-tokenization"><strong>Tokens and Tokenization</strong></h2>
<ul>
<li>ğŸ§© <strong>What are Tokens?</strong> Subdivisions of input data (words, subwords, punctuation, or image patches).</li>
<li>ğŸ”¡ <strong>Why Not Characters?</strong> Using characters increases context size and computational complexity; tokens balance meaning and computational efficiency.</li>
<li>ğŸ“– <strong>Byte Pair Encoding (BPE):</strong> A common method for tokenization.</li>
</ul>
<hr>
<h2 id="embedding-tokens-into-vectors"><strong>Embedding Tokens into Vectors</strong></h2>
<ul>
<li>ğŸ“ <strong>Embedding:</strong> Tokens are mapped to high-dimensional vectors representing their meaning.</li>
<li>ğŸ—ºï¸ <strong>Contextual Meaning:</strong> Vectors evolve through the network to capture context, disambiguate meaning, and encode relationships.</li>
</ul>
<hr>
<h2 id="the-attention-mechanism"><strong>The Attention Mechanism</strong></h2>
<ul>
<li>ğŸ” <strong>Purpose:</strong> Enables tokens to &ldquo;attend&rdquo; to others, updating their vectors based on relevance.</li>
<li>ğŸ”‘ <strong>Key Components:</strong>
<ul>
<li>Query Matrix: Encodes what a token is &ldquo;looking for.&rdquo;</li>
<li>Key Matrix: Encodes how a token responds to queries.</li>
<li>Value Matrix: Encodes information passed between tokens.</li>
</ul>
</li>
<li>ğŸ§® <strong>Calculations:</strong>
<ul>
<li>Dot Product: Measures alignment between keys and queries.</li>
<li>Softmax: Converts dot products into normalized weights for updates.</li>
</ul>
</li>
<li>â›“ï¸ <strong>Masked Attention:</strong> Ensures causality by blocking future tokens from influencing past ones.</li>
</ul>
<hr>
<h2 id="multi-headed-attention"><strong>Multi-Headed Attention</strong></h2>
<ul>
<li>ğŸ’¡ <strong>Parallel Heads:</strong> Multiple attention heads allow different types of relationships (e.g., grammar, semantic context) to be processed simultaneously.</li>
<li>ğŸš€ <strong>Efficiency on GPUs:</strong> Designed to maximize parallelization for faster computation.</li>
</ul>
<hr>
<h2 id="multi-layer-perceptrons-mlps"><strong>Multi-Layer Perceptrons (MLPs)</strong></h2>
<ul>
<li>ğŸ¤” <strong>Role in Transformers:</strong>
<ul>
<li>Add capacity for general knowledge and non-contextual reasoning.</li>
<li>Store facts learned during training, e.g., associations like &ldquo;Michael Jordan plays basketball.&rdquo;</li>
</ul>
</li>
<li>ğŸ”¢ <strong>Parameters:</strong> MLPs hold the majority of the modelâ€™s parameters.</li>
</ul>
<hr>
<h2 id="training-transformers"><strong>Training Transformers</strong></h2>
<ul>
<li>ğŸ“š <strong>Learning Framework:</strong>
<ul>
<li>Models are trained on vast datasets using next-token prediction, requiring no manual labels.</li>
<li><strong>Cost Function:</strong> Measures prediction accuracy using negative log probabilities, guiding parameter updates.</li>
</ul>
</li>
<li>ğŸ”ï¸ <strong>Optimization:</strong> Gradient descent navigates a high-dimensional cost surface to minimize error.</li>
<li>ğŸŒ <strong>Pretraining:</strong> Allows large-scale unsupervised learning before fine-tuning with human feedback.</li>
</ul>
<hr>
<h2 id="embedding-space-and-high-dimensions"><strong>Embedding Space and High Dimensions</strong></h2>
<ul>
<li>ğŸ”„ <strong>Semantic Clusters:</strong> Similar words cluster together; directions in the space encode relationships (e.g., gender: King - Male + Female = Queen).</li>
<li>ğŸŒŒ <strong>High Dimensionality:</strong> Embedding spaces have thousands of dimensions, enabling distinct representations of complex concepts.</li>
<li>ğŸ“ˆ <strong>Scaling Efficiency:</strong> High-dimensional spaces allow exponentially more &ldquo;almost orthogonal&rdquo; directions for encoding meanings.</li>
</ul>
<hr>
<h2 id="practical-applications"><strong>Practical Applications</strong></h2>
<ul>
<li>âœï¸ <strong>Language Models:</strong> Effective for chatbots, summarization, and more due to their generality and parallel processing.</li>
<li>ğŸ–¼ï¸ <strong>Multimodal Models:</strong> Transformers can integrate text, images, and sound by treating all as tokens in a unified framework.</li>
</ul>
<hr>
<h2 id="challenges-and-limitations"><strong>Challenges and Limitations</strong></h2>
<ul>
<li>ğŸ“ <strong>Context Size Limitations:</strong> Attention grows quadratically with context size, requiring optimization for large contexts.</li>
<li>â™»ï¸ <strong>Inference Redundancy:</strong> Token-by-token generation can involve redundant computations; caching mitigates this at inference time.</li>
</ul>
<hr>
<h2 id="engineering-and-design"><strong>Engineering and Design</strong></h2>
<ul>
<li>ğŸ› ï¸ <strong>Hardware Optimization:</strong> Transformers are designed to exploit GPUs&rsquo; parallelism for efficient matrix multiplication.</li>
<li>ğŸ”— <strong>Residual Connections:</strong> Baked into the architecture to enhance stability and ease of training.</li>
</ul>
<hr>
<h2 id="the-power-of-scale"><strong>The Power of Scale</strong></h2>
<ul>
<li>ğŸ“ˆ <strong>Scaling Laws:</strong> Larger models and more data improve performance, often qualitatively.</li>
<li>ğŸ”„ <strong>Self-Supervised Pretraining:</strong> Enables training on vast unlabeled datasets before fine-tuning.</li>
</ul>
<h2 id="bpe-byte-pair-encoding"><strong>BPE (Byte Pair Encoding)</strong></h2>
<p>BPE is a widely used tokenization method in natural language processing (NLP) and machine learning. It is designed to balance between breaking text into characters and full words by representing text as a sequence of subword units. This approach helps models handle rare and unseen words effectively while keeping the vocabulary size manageable.</p>
<hr>
<h3 id="how-bpe-works"><strong>How BPE Works:</strong></h3>
<ol>
<li>
<p><strong>Start with Characters:</strong></p>
<ul>
<li>Initially, every character in the text is treated as a separate token.</li>
</ul>
</li>
<li>
<p><strong>Merge Frequent Pairs:</strong></p>
<ul>
<li>BPE repeatedly identifies the most frequent pair of adjacent tokens in the training corpus and merges them into a single token. This process is iteratively applied.</li>
<li>For example:
<ul>
<li>Input: <code>low</code>, <code>lower</code>, <code>lowest</code></li>
<li>Output Vocabulary: {low_, e, r, s, t}</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Build Vocabulary:</strong></p>
<ul>
<li>The merging process stops after a predefined number of merges, resulting in a vocabulary of subwords, characters, and some common full words.</li>
</ul>
</li>
</ol>
<p><a href="https://www.youtube.com/watch?v=KJtZARuO3JY&amp;t=992s">Visualizing transformers and attention</a></p>

      </main>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/site/js/main.js"></script>
<script src='/site/js/prism.js'></script>
<script src='/site/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/site/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>