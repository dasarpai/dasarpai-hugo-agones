<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/site/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=site/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/site/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/site/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/site/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>AI Benchmarks Explained | Agones</title><meta property="og:url" content="http://localhost:1313/site/dsblog/dsblog/2024-10-26-6173-ai-benchmarks-explained/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="AI Benchmarks Explained">
  <meta property="og:description" content="AI Benchmarks Explained: Essential Components and Leading LLM Evaluation Techniques What is a Benchmark in AI? A benchmark in AI is like a standard measurement tool that helps researchers and developers assess how well their artificial intelligence models perform. Just like athletes are judged based on their performance against specific standards, AI models are evaluated against predefined tasks and metrics.
Thus, benchmarks are essential tools in the AI development ecosystem. They help ensure that AI models are evaluated fairly and consistently, providing a basis for comparison, improvement, and innovation in the field. By using benchmarks, developers can better understand their models’ capabilities and limitations, ultimately leading to more effective and robust AI systems.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2024-10-26T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-10-26T00:00:00+00:00">
    <meta property="article:tag" content="Generative AI">
    <meta property="article:tag" content="Text Generation">
    <meta property="article:tag" content="AI Model Evaluation Benchmarks">
    <meta property="article:tag" content="Evaluation Benchmarks">
    <meta property="article:tag" content="LLM Benchmarks">
    <meta property="article:tag" content="Natural Language Processing">

  <meta itemprop="name" content="AI Benchmarks Explained">
  <meta itemprop="description" content="AI Benchmarks Explained: Essential Components and Leading LLM Evaluation Techniques What is a Benchmark in AI? A benchmark in AI is like a standard measurement tool that helps researchers and developers assess how well their artificial intelligence models perform. Just like athletes are judged based on their performance against specific standards, AI models are evaluated against predefined tasks and metrics.
Thus, benchmarks are essential tools in the AI development ecosystem. They help ensure that AI models are evaluated fairly and consistently, providing a basis for comparison, improvement, and innovation in the field. By using benchmarks, developers can better understand their models’ capabilities and limitations, ultimately leading to more effective and robust AI systems.">
  <meta itemprop="datePublished" content="2024-10-26T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-10-26T00:00:00+00:00">
  <meta itemprop="wordCount" content="1145">
  <meta itemprop="keywords" content="AI benchmarks,LLM evaluation,natural language processing,artificial intelligence,machine learning,language model,text generation">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="AI Benchmarks Explained">
  <meta name="twitter:description" content="AI Benchmarks Explained: Essential Components and Leading LLM Evaluation Techniques What is a Benchmark in AI? A benchmark in AI is like a standard measurement tool that helps researchers and developers assess how well their artificial intelligence models perform. Just like athletes are judged based on their performance against specific standards, AI models are evaluated against predefined tasks and metrics.
Thus, benchmarks are essential tools in the AI development ecosystem. They help ensure that AI models are evaluated fairly and consistently, providing a basis for comparison, improvement, and innovation in the field. By using benchmarks, developers can better understand their models’ capabilities and limitations, ultimately leading to more effective and robust AI systems.">



<link rel="stylesheet" href="/site/css/prism.css"/>

<link href="/site/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/site/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/site/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete="off">
</div></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <main role="main" class="td-main">
        <p><img src="/assets/images/dspost/dsp6173-AI-Benchmarks-Explained.jpg" alt="AI-Benchmarks-Explained"></p>
<h1 id="ai-benchmarks-explained-essential-components-and-leading-llm-evaluation-techniques">AI Benchmarks Explained: Essential Components and Leading LLM Evaluation Techniques</h1>
<h2 id="what-is-a-benchmark-in-ai">What is a Benchmark in AI?</h2>
<p>A <strong>benchmark</strong> in AI is like a standard measurement tool that helps researchers and developers assess how well their artificial intelligence models perform. Just like athletes are judged based on their performance against specific standards, AI models are evaluated against predefined tasks and metrics.</p>
<p>Thus, benchmarks are essential tools in the AI development ecosystem. They help ensure that AI models are evaluated fairly and consistently, providing a basis for comparison, improvement, and innovation in the field. By using benchmarks, developers can better understand their models’ capabilities and limitations, ultimately leading to more effective and robust AI systems.</p>
<h3 id="key-components-of-benchmarks">Key Components of Benchmarks</h3>
<ol>
<li>
<p><strong>Tasks</strong>: These are specific challenges or problems that the AI model needs to solve. For example, a task could be translating a sentence from one language to another, answering questions based on a text, or recognizing objects in an image.</p>
</li>
<li>
<p><strong>Metrics</strong>: These are the criteria used to evaluate the performance of the model on the tasks. Metrics might include:</p>
<ul>
<li><strong>Accuracy</strong>: How many answers were correct?</li>
<li><strong>Precision and Recall</strong>: How well does the model identify relevant results?</li>
<li><strong>F1 Score</strong>: A balance between precision and recall.</li>
<li><strong>BLEU Score</strong>: Often used for evaluating language translation tasks.</li>
</ul>
</li>
<li>
<p><strong>Datasets</strong>: Benchmarks usually come with specific datasets that provide the input for the tasks. These datasets contain examples that the model can learn from and be tested on. For instance, a benchmark for language models might include a set of sentences to translate or questions to answer.</p>
</li>
<li>
<p><strong>Evaluation Protocol</strong>: This outlines how the tasks should be executed and how results should be measured. It ensures that everyone evaluates their models in the same way, making comparisons fair.</p>
</li>
<li>
<p><strong>Baseline Models</strong>: Benchmarks often include comparisons to existing models. These baselines represent standard performance levels that new models can be compared against to see if they offer improvements.</p>
</li>
</ol>
<h3 id="why-are-benchmarks-important">Why Are Benchmarks Important?</h3>
<ul>
<li><strong>Standardization</strong>: They provide a consistent framework for evaluating AI models, making it easier to compare results across different studies and developments.</li>
<li><strong>Guidance for Improvement</strong>: Benchmarks help identify strengths and weaknesses in models, guiding researchers on where to focus their efforts to improve performance.</li>
<li><strong>Driving Innovation</strong>: By setting performance targets, benchmarks encourage the development of new and better algorithms and technologies.</li>
</ul>
<h3 id="example-in-practice">Example in Practice</h3>
<p>Consider an AI model designed to generate human-like text. A benchmark for this model might include tasks like:</p>
<ul>
<li>Completing a sentence given a prompt.</li>
<li>Answering questions based on a provided text.</li>
<li>Summarizing an article.</li>
</ul>
<p>How well these tasks are performed by a given model is evaluated using the Benchmarks. The evaluation would involve specific datasets (like sets of articles) and metrics (such as coherence and relevance of the generated text).</p>
<h2 id="what-are-the-most-popular-llm-benchmarks">What are the most popular LLM Benchmarks?</h2>
<h3 id="general">General</h3>
<ol>
<li>
<p><strong>MMLU (5-shot)</strong>:
Measures performance across a wide range of academic subjects using few-shot learning. It&rsquo;s designed to test a model&rsquo;s ability to generalize knowledge from minimal examples.</p>
</li>
<li>
<p><strong>AGIEval English (3-5 shot)</strong>:
Evaluates English proficiency and general reasoning skills through real-world scenarios and tasks. This benchmark aims to assess the model&rsquo;s adaptability to practical use cases.</p>
</li>
<li>
<p><strong>CommonSenseQA (7-shot)</strong>:
Tests common sense reasoning by requiring the model to answer questions that need general world knowledge. The benchmark focuses on evaluating how well the model understands everyday scenarios.</p>
</li>
<li>
<p><strong>Winogrande (5-shot)</strong>:
Assesses commonsense reasoning by resolving ambiguities in sentences that require an understanding of context. It challenges models to disambiguate sentences that humans find straightforward.</p>
</li>
<li>
<p><strong>BIG-Bench Hard (3-shot, CoT)</strong>:
Evaluates models on particularly challenging and diverse tasks, including those that require chain-of-thought prompting. This benchmark is part of the broader BIG-Bench suite, focusing on difficult tasks.</p>
</li>
<li>
<p><strong>ARC-Challenge (25-shot)</strong>:
The Advanced Reasoning Challenge focuses on hard science questions, testing deep understanding and logical reasoning. It&rsquo;s designed to push the limits of a model&rsquo;s scientific knowledge and reasoning skills.</p>
</li>
</ol>
<h3 id="knowledge-reasoning">Knowledge Reasoning</h3>
<ol start="7">
<li><strong>TriviaQA-Wiki (5-shot)</strong>:
Tests the model&rsquo;s ability to answer trivia questions using information from Wikipedia. This benchmark evaluates the breadth and accuracy of the model&rsquo;s factual knowledge.</li>
</ol>
<h3 id="reading-comprehension">Reading Comprehension</h3>
<ol start="8">
<li>
<p><strong>SQuAD (1-shot)</strong>:
The Stanford Question Answering Dataset assesses the model&rsquo;s comprehension based on reading passages. It requires the model to extract precise answers from given texts.</p>
</li>
<li>
<p><strong>QuAC (1-shot, F1)</strong>:
Question Answering in Context evaluates the model&rsquo;s ability to understand and respond to questions in a dialog-based format. It measures how well the model can handle interactive reading comprehension.</p>
</li>
<li>
<p><strong>BoolQ (0-shot)</strong>:
Tests the model&rsquo;s ability to answer yes/no questions based on a given passage. It evaluates the model&rsquo;s binary decision-making capability without prior examples.</p>
</li>
<li>
<p><strong>DROP (3-shot, F1)</strong>:
Discrete Reasoning Over Paragraphs assesses numerical and discrete reasoning abilities over text passages. This benchmark requires the model to perform complex reasoning tasks involving multiple pieces of information.</p>
</li>
</ol>
<p>These benchmarks collectively cover a broad spectrum of skills, from general knowledge and reasoning to specific reading comprehension and discrete reasoning abilities.</p>
<h2 id="what-is-the-meaning-n-shot-learning">What is the meaning n-shot learning?</h2>
<p>You will notice these benchmark are named like &ldquo;MMLU (5-shot)&rdquo;, &ldquo;BoolQ (0-shot)&rdquo;, &ldquo;TriviaQA-Wiki (5-shot)&rdquo; etc. For any new person it becomes very confusing to understand what is the meaning of this and why these benchmarks are named like this. Are they datasets? Are they tasks? Are they metrics? Are they taking about samples? Are they used during training or during evaluation? We will understand this with  &ldquo;MMLU (5-shot)&rdquo;</p>
<p>The benchmark &ldquo;MMLU (5-shot)&rdquo; refers to a specific evaluation framework for assessing the performance of machine learning models, particularly large language models (LLMs), on a variety of tasks with a few examples. Here’s a breakdown of what each part means:</p>
<p><strong>MMLU (Massive Multitask Language Understanding)</strong></p>
<p>In definition, MMLU is a benchmark designed to evaluate the performance of language models across multiple tasks in a unified manner. It includes a wide range of tasks, such as reading comprehension, factual knowledge, and various forms of reasoning.</p>
<p>MMLU encompasses different domains and types of questions, allowing researchers to gauge how well a language model can generalize its understanding across different contexts and task requirements.</p>
<p><strong>5-Shot Learning</strong></p>
<p>The &ldquo;5-shot&rdquo; part of MMLU indicates that during evaluation, the model is given 5 examples (or &ldquo;shots&rdquo;) for each task. This means that for each specific task within the benchmark, the model receives 5 labeled examples to learn from before making predictions.</p>
<p>In few-shot learning contexts, the goal is to assess how well a model can generalize from a limited number of examples. The model&rsquo;s performance is evaluated based on how effectively it can utilize those 5 examples to make predictions on new, unseen inputs within the same task.</p>
<p>The MMLU (5-shot) benchmark tests a model&rsquo;s ability to generalize from a small number of examples. This is particularly relevant in practical applications where labeled data may be scarce or expensive to obtain. By providing a standardized way to evaluate models on various tasks with few examples, MMLU allows for comparative analysis of different models and architectures, helping researchers understand which models perform better in terms of few-shot learning capabilities.</p>

      </main>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/site/js/main.js"></script>
<script src='/site/js/prism.js'></script>
<script src='/site/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/site/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>