<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation | Agones</title><meta property="og:url" content="http://localhost:1313/dsblog/rps-Pretrained-Language-Models-for-Text-Generation/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation">
  <meta property="og:description" content="Paper Name :- Pretrained Language Models for Text Generation: A Survey
Typer of Paper:- Survey Paper Paper URL
Paper title of the citations mentioned can be found at AI Papers with Heading. Use citation code to locate.
Paper Summary :- Pretrained Language Models for Text Generation Paper Outcome General task deﬁnition Describe the mainstream architectures of PLMs for text generation. How to adapt existing PLMs to model different input data and satisfy special properties in the generated text. Summarize several important ﬁne-tuning strategies for text generation. Ideas from the Paper Main Ideas This paper discusses “major advances achieved in the topic of PLMs for text generation” This survey aims to provide “text generation researchers a synthesis” and pointer to related research. General Ideas Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). Neural generation model are deep learning models Pretrained language models (PLMs) are neural generation model Task Types and Typical Applications In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX ) If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional generation task(generate text without any constraint) Radford2019 If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or attribute-based generation. X plays the role of guiding the text generation. Keskar2019. If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation Li2021c. If X is multimedia input such as image, the task becomes image caption Xia2020 If X is multimedia input such as speech, the task become speech recognition Fan2019. If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system. Machine translation aims to translate text from one language into another language automatically Conneau2019 Generating condensed summary of a long document Zhang2019b Dialogue system to converse with humans using natural language. Wolf2019 Architectures for Text Generation Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism. MASS Song2019 T5 Raffel2020 BART Lewis2020 Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens. GPT Radfordet2019; Brown2020 CTRL [Keskar2019] Modeling Different Data Types from Input Unstructured Input Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b] Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a] Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019] Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018]. Structured Input PLMs are not designed for structured or tabular data but for sequential text/data. Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020] To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020] Introduced an additional graph encoder to encode the input KG. [Li2021b] Template based method to serialize input table into text sequence. [Gong2020] For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data. Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020] The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b]. Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020] Multimedia Input Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a] Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020] Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019] Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020 Image, video, speech recognition is hungry for human-transcripted supervised data. Integrate PLMs for weakly-supervised learning. For example, Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019] Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task. Satisfying Special Properties for Output Text Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation. Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] Approach for non-conditioned dialogue [Bao2020]. Faithfulness. Means the content in generated text should not contradict the facts in input text. PLMs are potentially beneficial to generate faithful text by utilizing background knowledge. Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020] With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities. Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018] Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b] Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent. When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation. Code-Switching Pre-training (CSP) for machine translation. [Yang2020a] Extracted the word-pair alignment information from the source and target language, Aplied the extracted alignment information to enhance order-preserving. Translation across multiple languages, called multilingual machine translation [Conneau2019]. mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020] Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018 Summary from Introduction Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a]. Machine translation generates text in a different language based on the source text [Yang2020a]; Summarization generates an abridged version of the source text to include salient information [Guan2020]. Text generation tasks based on Recurrent neural networks (RNN) [Li2019], Convolutional neural networks (CNN) [Gehring2017], Graph neural networks (GNN) [Li2020], Attention mechanism [Bahdanau2015]. One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation. Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity. Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice. The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve state-of-the-art results. PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language. PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020]. A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020] Conclusion &amp; Future Recommendations Model Extension.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2023-08-18T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T11:34:17+05:30">
    <meta property="article:tag" content="AI Paper">
    <meta property="article:tag" content="Research Paper">
    <meta property="article:tag" content="Text Generation">
    <meta property="article:tag" content="Pretrained Models">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Machine Learning">

  <meta itemprop="name" content="Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation">
  <meta itemprop="description" content="Paper Name :- Pretrained Language Models for Text Generation: A Survey
Typer of Paper:- Survey Paper Paper URL
Paper title of the citations mentioned can be found at AI Papers with Heading. Use citation code to locate.
Paper Summary :- Pretrained Language Models for Text Generation Paper Outcome General task deﬁnition Describe the mainstream architectures of PLMs for text generation. How to adapt existing PLMs to model different input data and satisfy special properties in the generated text. Summarize several important ﬁne-tuning strategies for text generation. Ideas from the Paper Main Ideas This paper discusses “major advances achieved in the topic of PLMs for text generation” This survey aims to provide “text generation researchers a synthesis” and pointer to related research. General Ideas Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). Neural generation model are deep learning models Pretrained language models (PLMs) are neural generation model Task Types and Typical Applications In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX ) If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional generation task(generate text without any constraint) Radford2019 If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or attribute-based generation. X plays the role of guiding the text generation. Keskar2019. If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation Li2021c. If X is multimedia input such as image, the task becomes image caption Xia2020 If X is multimedia input such as speech, the task become speech recognition Fan2019. If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system. Machine translation aims to translate text from one language into another language automatically Conneau2019 Generating condensed summary of a long document Zhang2019b Dialogue system to converse with humans using natural language. Wolf2019 Architectures for Text Generation Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism. MASS Song2019 T5 Raffel2020 BART Lewis2020 Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens. GPT Radfordet2019; Brown2020 CTRL [Keskar2019] Modeling Different Data Types from Input Unstructured Input Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b] Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a] Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019] Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018]. Structured Input PLMs are not designed for structured or tabular data but for sequential text/data. Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020] To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020] Introduced an additional graph encoder to encode the input KG. [Li2021b] Template based method to serialize input table into text sequence. [Gong2020] For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data. Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020] The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b]. Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020] Multimedia Input Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a] Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020] Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019] Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020 Image, video, speech recognition is hungry for human-transcripted supervised data. Integrate PLMs for weakly-supervised learning. For example, Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019] Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task. Satisfying Special Properties for Output Text Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation. Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] Approach for non-conditioned dialogue [Bao2020]. Faithfulness. Means the content in generated text should not contradict the facts in input text. PLMs are potentially beneficial to generate faithful text by utilizing background knowledge. Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020] With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities. Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018] Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b] Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent. When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation. Code-Switching Pre-training (CSP) for machine translation. [Yang2020a] Extracted the word-pair alignment information from the source and target language, Aplied the extracted alignment information to enhance order-preserving. Translation across multiple languages, called multilingual machine translation [Conneau2019]. mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020] Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018 Summary from Introduction Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a]. Machine translation generates text in a different language based on the source text [Yang2020a]; Summarization generates an abridged version of the source text to include salient information [Guan2020]. Text generation tasks based on Recurrent neural networks (RNN) [Li2019], Convolutional neural networks (CNN) [Gehring2017], Graph neural networks (GNN) [Li2020], Attention mechanism [Bahdanau2015]. One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation. Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity. Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice. The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve state-of-the-art results. PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language. PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020]. A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020] Conclusion &amp; Future Recommendations Model Extension.">
  <meta itemprop="datePublished" content="2023-08-18T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-08T11:34:17+05:30">
  <meta itemprop="wordCount" content="1755">
  <meta itemprop="keywords" content="Pretrained,Language,Models,,Text,Generation,,NLP,Research,,Machine,Translation,,Text,Summarization,,Language,Model,Architecture,,AI,Text,Generation,,Research,Survey">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Paper-Summary- A Survey Paper# Pretrained Language Models for Text Generation">
  <meta name="twitter:description" content="Paper Name :- Pretrained Language Models for Text Generation: A Survey
Typer of Paper:- Survey Paper Paper URL
Paper title of the citations mentioned can be found at AI Papers with Heading. Use citation code to locate.
Paper Summary :- Pretrained Language Models for Text Generation Paper Outcome General task deﬁnition Describe the mainstream architectures of PLMs for text generation. How to adapt existing PLMs to model different input data and satisfy special properties in the generated text. Summarize several important ﬁne-tuning strategies for text generation. Ideas from the Paper Main Ideas This paper discusses “major advances achieved in the topic of PLMs for text generation” This survey aims to provide “text generation researchers a synthesis” and pointer to related research. General Ideas Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). Neural generation model are deep learning models Pretrained language models (PLMs) are neural generation model Task Types and Typical Applications In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX ) If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional generation task(generate text without any constraint) Radford2019 If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or attribute-based generation. X plays the role of guiding the text generation. Keskar2019. If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation Li2021c. If X is multimedia input such as image, the task becomes image caption Xia2020 If X is multimedia input such as speech, the task become speech recognition Fan2019. If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system. Machine translation aims to translate text from one language into another language automatically Conneau2019 Generating condensed summary of a long document Zhang2019b Dialogue system to converse with humans using natural language. Wolf2019 Architectures for Text Generation Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism. MASS Song2019 T5 Raffel2020 BART Lewis2020 Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens. GPT Radfordet2019; Brown2020 CTRL [Keskar2019] Modeling Different Data Types from Input Unstructured Input Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b] Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a] Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019] Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018]. Structured Input PLMs are not designed for structured or tabular data but for sequential text/data. Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020] To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020] Introduced an additional graph encoder to encode the input KG. [Li2021b] Template based method to serialize input table into text sequence. [Gong2020] For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data. Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020] The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b]. Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020] Multimedia Input Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a] Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020] Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019] Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020 Image, video, speech recognition is hungry for human-transcripted supervised data. Integrate PLMs for weakly-supervised learning. For example, Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019] Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task. Satisfying Special Properties for Output Text Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation. Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input. When applying PLMs to the task of dialogue systems, TransferTransfo and DialoGPT were able to generate more relevant responses than RNNbased models. [Wolf2019] [Zhang2020] Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020] Approach for non-conditioned dialogue [Bao2020]. Faithfulness. Means the content in generated text should not contradict the facts in input text. PLMs are potentially beneficial to generate faithful text by utilizing background knowledge. Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020] With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities. Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018] Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b] Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent. When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation. Code-Switching Pre-training (CSP) for machine translation. [Yang2020a] Extracted the word-pair alignment information from the source and target language, Aplied the extracted alignment information to enhance order-preserving. Translation across multiple languages, called multilingual machine translation [Conneau2019]. mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020] Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018 Summary from Introduction Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a]. Machine translation generates text in a different language based on the source text [Yang2020a]; Summarization generates an abridged version of the source text to include salient information [Guan2020]. Text generation tasks based on Recurrent neural networks (RNN) [Li2019], Convolutional neural networks (CNN) [Gehring2017], Graph neural networks (GNN) [Li2020], Attention mechanism [Bahdanau2015]. One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation. Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity. Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice. The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve state-of-the-art results. PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language. PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020]. A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020] Conclusion &amp; Future Recommendations Model Extension.">



<link rel="stylesheet" href="/css/prism.css"/>

<link href="/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>


<link rel="stylesheet" href="/css/custom.css">

<script src="/js/lunr.js"></script>


    <style>
       
      .td-main img {
        max-width: 100%;
        height: auto;
      }
      .td-main {
        padding-top: 60px;  
      }
       
      .td-sidebar-right {
          padding-left: 20px;  
      }
    </style>
  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/samskrutyatra/"><span>Samskrut Yatra Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search position-relative">
  <div class="td-search__icon"></div>
  <input
    id="agones-search"
    type="search"
    class="td-search__input form-control td-search-input"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
  >
  <ul id="agones-search-results" class="list-group position-absolute w-100" style="z-index:1000; top:100%; left:0;"></ul>
</div>

<script>
let lunrIndex, pagesIndex;

async function initLunr() {
  const response = await fetch('/index.json');
  pagesIndex = await response.json();
  lunrIndex = lunr(function () {
    this.ref('url');
    this.field('title', { boost: 10 });
    this.field('content');
    pagesIndex.forEach(function (doc) {
      this.add(doc);
    }, this);
  });
}

function search(query) {
  if (!lunrIndex || !query) return [];
  return lunrIndex.search(query).map(result =>
    pagesIndex.find(page => page.url === result.ref)
  );
}

document.addEventListener('DOMContentLoaded', function () {
  initLunr();
  const input = document.getElementById('agones-search');
  const resultsList = document.getElementById('agones-search-results');
  input.addEventListener('input', function (e) {
    const query = e.target.value.trim();
    if (!query) {
      resultsList.innerHTML = '';
      resultsList.style.display = 'none';
      return;
    }
    const results = search(query);
    if (results.length === 0) {
      resultsList.innerHTML = '<li class="list-group-item">No results found.</li>';
      resultsList.style.display = 'block';
      return;
    }
    resultsList.innerHTML = results.map(page =>
      `<li class="list-group-item"><a href="${page.url}">${page.title}</a></li>`
    ).join('');
    resultsList.style.display = 'block';
  });
  
  input.addEventListener('blur', function() {
    setTimeout(() => { resultsList.style.display = 'none'; }, 200);
  });
  
  input.addEventListener('focus', function() {
    if (input.value.trim()) resultsList.style.display = 'block';
  });
});
</script></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <div class="row">
        <div class="col-md-3">
          
        </div>
        <main role="main" class="col-md-6 td-main">
          <p><img src="/assets/images/dspost/dsp6088-rps-Pretrained-Language-Models-for-Text-Generation.jpg" alt="Pretrained Language Models for Text Generation"></p>
<p><strong>Paper Name :- Pretrained Language Models for Text Generation: A Survey</strong><br>
Typer of Paper:- Survey Paper  <br>
<a href="https://arxiv.org/abs/2105.10311">Paper URL</a><br>
Paper title of the citations mentioned can be found at <a href="/dsblog/aip">AI Papers with Heading</a>. Use citation code to locate.</p>
<h1 id="paper-summary---pretrained-language-models-for-text-generation">Paper Summary :- Pretrained Language Models for Text Generation</h1>
<h2 id="paper-outcome">Paper Outcome</h2>
<ul>
<li>General task deﬁnition</li>
<li>Describe the mainstream architectures of PLMs for text generation.</li>
<li>How to adapt existing PLMs to model different input data and satisfy special properties in the generated text.</li>
<li>Summarize several important ﬁne-tuning strategies for text generation.</li>
</ul>
<h2 id="ideas-from-the-paper">Ideas from the Paper</h2>
<h3 id="main-ideas">Main Ideas</h3>
<ul>
<li>This paper discusses &ldquo;major advances achieved in the topic of PLMs for text generation&rdquo;</li>
<li>This survey aims to provide &ldquo;text generation researchers a synthesis&rdquo; and pointer to related research.</li>
</ul>
<h3 id="general-ideas">General Ideas</h3>
<ul>
<li>Text generation has become one of the most important yet challenging tasks in natural language processing (NLP).</li>
<li>Neural generation model are deep learning models</li>
<li>Pretrained language models (PLMs) are neural generation model</li>
</ul>
<h3 id="task-types-and-typical-applications">Task Types and Typical Applications</h3>
<ul>
<li>In most cases, text generation is conditioned on input data, such as attributes, text and structured data, which is denoted as X. Formally, the text generation task can be described as: P(YjX ) = P(y1; : : : ; yj ; : : : ; ynjX )</li>
<li>If X is not provided or a random noise vector z, this task will degenerate into language modeling or unconditional
generation task(generate text without any constraint) <a href="/dsblog/aip#radford2019">Radford2019</a></li>
<li>If X is a set of discrete attributes (e.g., topic words, sentiment labels), the task becomes topic-to-text generation or
attribute-based generation.  X plays the role of guiding the text generation. <a href="/dsblog/aip#Keskar2019">Keskar2019</a>.</li>
<li>If X is structured data like knowledge graph or table, this task will be considered as KG-to-text or table-to-text generation (generate descriptive text about structured data), called data-to-text generation <a href="/dsblog/aip#Li2021c">Li2021c</a>.</li>
<li>If X is multimedia input such as image, the task becomes image caption <a href="/dsblog/aip#Xia2020">Xia2020</a></li>
<li>If X is multimedia input such as speech, the task become speech recognition <a href="/dsblog/aip#Fan2019">Fan2019</a>.</li>
<li>If X text sequence (most common form), there are several applications such as machine translation, summarization and dialogue system.</li>
<li>Machine translation aims to translate text from one language into another language automatically <a href="/dsblog/aip#Conneau2019">Conneau2019</a></li>
<li>Generating condensed summary of a long document <a href="/dsblog/aip#Zhang2019b">Zhang2019b</a></li>
<li>Dialogue system to converse with humans using natural language. <a href="/dsblog/aip#Wolf2019">Wolf2019</a></li>
</ul>
<h2 id="architectures-for-text-generation">Architectures for Text Generation</h2>
<ul>
<li>Encoder-decoder Transformer. It is two stacks of Transformer blocks. The encoder is fed with an input sequence, while the decoder aims to generate the output sequence based on encoder-decoder self-attention mechanism.
<ul>
<li>MASS <a href="/dsblog/aip#song2019">Song2019</a></li>
<li>T5 <a href="/dsblog/aip#raffel2020">Raffel2020</a></li>
<li>BART <a href="/dsblog/aip#lewis2020">Lewis2020</a></li>
</ul>
</li>
<li>Decoder-only Transformer. Employ a single Transformer decoder blocks. They apply unidirectional self-attention masking that each token can only attend to previous tokens.
<ul>
<li>GPT <a href="/dsblog/aip#radfordet2019">Radfordet2019</a>; <a href="/dsblog/aip#brown2020">Brown2020</a></li>
<li>CTRL [Keskar2019]</li>
</ul>
</li>
</ul>
<h2 id="modeling-different-data-types-from-input">Modeling Different Data Types from Input</h2>
<h3 id="unstructured-input">Unstructured Input</h3>
<ul>
<li>Hierarchical BERT to learn interactions between sentences with self-attention for document encoding. [Zhang2019b] and [Xu2020b]</li>
<li>Capturing intersentential relations, DiscoBERT stacked graph convolutional network (GCN) on top of BERT to model structural discourse graphs. [Xu2020a]</li>
<li>Cross-lingual language models (XLMs) for multilingual language understanding. [Conneau2019]</li>
<li>Text generation models can obtain effective input word embeddings even in a low-resource language [Wada2018].</li>
</ul>
<h3 id="structured-input">Structured Input</h3>
<ul>
<li>PLMs are not designed for structured or tabular data but for sequential text/data.</li>
<li>Incorporating PLMs for data-to text generation, especially in few-shot settings. [Chen2020b] and [Gong2020]</li>
<li>To adapt to the sequential nature of PLMs linearized input knowledge graph (KG) and abstract meaning representation (AMR) graph into a sequence of triples. [Ribeiro2020] and [Mager2020]</li>
<li>Introduced an additional graph encoder to encode the input KG. [Li2021b]</li>
<li>Template based method to serialize input table into text sequence.  [Gong2020]
<ul>
<li>For example, the attribute-value pair “name: jack reynolds” will be serialized as a sentence “name is jack reynolds”. However, direct linearization will lose the structural information of original data, which may lead to generating unfaithful text about data.</li>
</ul>
</li>
<li>Auxiliary reconstruction task for recovering the structural information of input data, which can enhance the capacity of modeling structural information. [Gong2020]</li>
<li>The pointer generator mechanism is adopted to copy words from input knowledge data. [See2017] [Chen2020b].</li>
<li>Content matching loss for measuring the distance between the information in input data and the output text. [Gong2020]</li>
</ul>
<h3 id="multimedia-input">Multimedia Input</h3>
<ul>
<li>Conducted pretraining for the video caption task. VideoBERT [Sun2019b] and CBT [Sun2019a]</li>
<li>Used a shared multi-layer Transformer network for both encoding and decoding. Unified VLP [Zhou2020]</li>
<li>Pretrained the model on two masked language modeling (MLM) tasks, like cloze tasks designed for sequence-to-sequence LM. UniLM [Dong2019]</li>
<li>Cross-modal pretrained model (XGPT) by taking images as inputs and using the image caption task as the basic generative task in the pretraining stage. Xia2020</li>
<li>Image, video, speech recognition is hungry for human-transcripted supervised data.</li>
<li>Integrate PLMs for weakly-supervised learning. For example,
<ul>
<li>Unsupervised approach to pretraining encoder-decoder model with unpaired speech and transcripts. [Fan2019]</li>
</ul>
</li>
<li>Two pretraining stages are used to extract acoustic and linguistic information with speech and transcripts, which is useful for downstream speech recognition task.</li>
</ul>
<h2 id="satisfying-special-properties-for-output-text">Satisfying Special Properties for Output Text</h2>
<ul>
<li>Generated text should satisfy several key properties like. relevance, faithfulness, and order-preservation.</li>
<li>Relevance. Relevance refers that the topics in output text is highly related to the input text. The generated responses should
also be relevant to the condition. RNN-based models still tend to generate irrelevant output text and lack consistency with input.
<ul>
<li>When applying PLMs to the task of dialogue systems, TransferTransfo  and DialoGPT were able to generate more relevant responses than  RNNbased models. [Wolf2019] [Zhang2020]</li>
<li>Utilize elaborated condition blocks to incorporate external conditions. They used BERT for both encoder and decoder by utilizing different input
representations and self-attention masks to distinguish the source and target sides of dialogue. On the target (generation) side, a new attention routing mechanism is adopted to generate context-related words. [Zeng2020]</li>
<li>Approach for non-conditioned dialogue [Bao2020].</li>
</ul>
</li>
<li>Faithfulness. Means the content in generated text should not contradict the facts in input text.
<ul>
<li>PLMs are potentially beneficial to generate faithful text by utilizing background knowledge.</li>
<li>Initialize the encoder and decoder with three outstanding PLMs, i.e., BERT, GPT and RoBERTa. [Rothe2020]</li>
<li>With pretraining, the models are more aware of the domain characteristics and less prone to language model vulnerabilities.</li>
<li>Decompose the decoder into a contextual network that retrieves relevant parts of the source document and a PLM that incorporates prior knowledge about language generation. [Kryscinski2018]</li>
<li>Generate faithful text in different target domains, fine-tuned PLMs on target domains through theme modeling loss. [Yang2020b]</li>
</ul>
</li>
<li>Order-preservation. Order-preservation denotes that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.
<ul>
<li>When translating from source language to target language, keeping the order of phrases consistent in source language and target language will ensure the accuracy of the translation.</li>
<li>Code-Switching Pre-training (CSP) for machine translation. [Yang2020a]
<ul>
<li>Extracted the word-pair alignment information from the source and target language,</li>
<li>Aplied the extracted alignment information to enhance order-preserving.</li>
<li>Translation across multiple languages, called multilingual machine translation [Conneau2019].</li>
<li>mRASP (technique of randomly aligned substitution), an approach to pretraining a universal multilingual machine translation model. [Lin2020]</li>
<li>Aligning word representations of each language, making it possible to preserve the word order consistent cross multiple languages. Wada2018</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="summary-from-introduction">Summary from Introduction</h2>
<ul>
<li>Researchers have developed numerous techniques for a wide range of applications of text generation [Li2021a].</li>
<li>Machine translation generates text in a different language based on the source text [Yang2020a];</li>
<li>Summarization generates an abridged version of the source text to include salient information [Guan2020].</li>
<li>Text generation tasks based on
<ul>
<li>Recurrent neural networks (RNN) [Li2019],</li>
<li>Convolutional neural networks (CNN) [Gehring2017],</li>
<li>Graph neural networks (GNN) [Li2020],</li>
<li>Attention mechanism [Bahdanau2015].</li>
</ul>
</li>
<li>One of the advantages of these neural models is that they enable end-to-end learning of semantic mappings from input to output in text generation.</li>
<li>Neural models are able to learn low-dimensional, dense vectors to implicitly represent linguistic features of text, which is also useful to alleviate data sparsity.</li>
<li>Deep neural networks usually have a large number of parameters to learn, which are likely to overﬁt on these small datasets and do not generalize well in practice.</li>
<li>The idea behind PLMs is to ﬁrst pretrain the models in large-scale corpus and then ﬁnetune these models in various downstream tasks to achieve
state-of-the-art results.</li>
<li>PLMs can encode a large amount of linguistic knowledge from corpus and induce universal representations of language.</li>
<li>PLMs are generally beneﬁcial for downstream tasks and can avoid training a new model from scratch [Brown2020].</li>
<li>A synthesis to the research on some text generation subtasks. Zaib et al. [2020], and Guan et al. [2020]</li>
</ul>
<h2 id="conclusion--future-recommendations">Conclusion &amp; Future Recommendations</h2>
<p>Model Extension.</p>
<ul>
<li>Discrepancies between pretraining and downstream generation tasks. For example, the “[MASK]” token in pretraining stage will not be used in
ﬁne-tuning stage, which further aggravates the pretraining ﬁnetuning discrepancy.</li>
<li>Design an appropriate pretraining paradigm for text generation.</li>
<li>Incorporating external knowledge into PLMs during pretraining has been shown to be effective <a href="/dsblog/aip#zhang2019c">Zhang2019c</a></li>
</ul>
<p>Controllable Generation.</p>
<ul>
<li>Controlling some attributes of the generated text has many useful applications such as generating positive response to patients with depression in dialogue systems.</li>
<li>PLMs are usually pretrained in universal corpus, which is difﬁcult to control the multi-grained attributes of the generated text (e.g., sentiment, topic, and coherence).</li>
<li>These control codes are preset and coarse-grained. Keskar et al. [2019]</li>
<li>Future work can explore multi-grained control and develop PLMs that are sufﬁciently steerable.</li>
</ul>
<p>Model Compression.</p>
<ul>
<li>PLMs with large-scale parameters models are challenging to be deployed in resource constrained environments.</li>
<li>Study how to achieve competitive performance with a small number of parameters.</li>
<li>Several methods have been proposed to compress PLMs, such as parameter sharing [Lan2020] - ALBERT</li>
<li>Knowledge distillation [Sanh2019] - DistilBERT</li>
<li>Compress PLMs for text generation.</li>
</ul>
<p>Fine-tuning Exploration:</p>
<ul>
<li>The direct intention of pretraining is to distill the linguistic knowledge learned in PLMs to downstream generation tasks.</li>
<li>Various ways to transfer knowledge from PLMs to downstream models.</li>
<li>Exploited knowledge distillation by adopting BERT as teacher model and a vanilla RNN generation model as student model. Chen et al. [2020a]</li>
</ul>
<p>Language-agnostic PLMs:</p>
<ul>
<li>PLMs for text generation are mainly based on English. These PLMs will encounter challenges when dealing with non-English generation tasks.</li>
<li>Language-agnostic PLMs are worthy to be investigated, which need to capture universal linguistic and semantic features across different languages.</li>
<li>An interesting direction is how to reuse existing English-based PLMs for text generation in non-English languages.</li>
</ul>
<p>Ethical Concern:</p>
<ul>
<li>Currently, PLMs are pretrained on largescale corpus crawled from the web without ﬁne-grained ﬁltering, potentially causing ethical issues such as generating private content about users. Therefore, researchers should try their best to prevent misusing PLMs.</li>
<li>Identifying threats and potential impacts and assessing likelihood. Ross [2012]</li>
<li>The text generated by PLMs might be prejudiced, which is in line with the bias in training data along the dimensions of gender, race, and religion [Brown2020].</li>
</ul>
<p><strong>Author</strong><br>
Dr Hari Thapliyaal<br>
dasarpai.com <br>
linkedin.com/in/harithapliyal</p>
<div class="category-section">
    <h4 class="category-section__title">Categories:</h4>
    <div class="category-badges"><a href="/categories/dsblog" class="category-badge">dsblog</a></div>
  </div><div class="td-tags">
    <h4 class="td-tags__title">Tags:</h4>
    <div class="category-badges"><a href="/tags/ai-paper" class="category-badge">AI Paper</a><a href="/tags/nlp" class="category-badge">NLP</a><a href="/tags/research-paper" class="category-badge">Research Paper</a><a href="/tags/text-generation" class="category-badge">Text Generation</a><a href="/tags/pretrained-models" class="category-badge">Pretrained Models</a><a href="/tags/nlp" class="category-badge">NLP</a><a href="/tags/machine-learning" class="category-badge">Machine Learning</a><a href="/tags/language-models" class="category-badge">Language Models</a><a href="/tags/ai-research" class="category-badge">AI Research</a></div>
  </div><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Hari Thapliyaal's avatar" class="author-image" >
      </div><div class="td-author-box__info">
      <h4 class="td-author-box__name">Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>

<div class="td-social-share">
  <h4 class="td-social-share__title">Share this article:</h4>
  <ul class="td-social-share__list"><div class="social-share">
        <a href="https://twitter.com/intent/tweet?text=Paper-Summary-%20A%20Survey%20Paper%23%20Pretrained%20Language%20Models%20for%20Text%20Generation&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2frps-Pretrained-Language-Models-for-Text-Generation%2f" target="_blank" rel="noopener" aria-label="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fdsblog%2frps-Pretrained-Language-Models-for-Text-Generation%2f" target="_blank" rel="noopener" aria-label="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2frps-Pretrained-Language-Models-for-Text-Generation%2f&title=Paper-Summary-%20A%20Survey%20Paper%23%20Pretrained%20Language%20Models%20for%20Text%20Generation" target="_blank" rel="noopener" aria-label="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
        <a href="https://www.reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2frps-Pretrained-Language-Models-for-Text-Generation%2f&title=Paper-Summary-%20A%20Survey%20Paper%23%20Pretrained%20Language%20Models%20for%20Text%20Generation" target="_blank" rel="noopener" aria-label="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
        <a href="mailto:?subject=Paper-Summary-%20A%20Survey%20Paper%23%20Pretrained%20Language%20Models%20for%20Text%20Generation&body=http%3a%2f%2flocalhost%3a1313%2fdsblog%2frps-Pretrained-Language-Models-for-Text-Generation%2f" aria-label="Share via Email">
          <i class="fas fa-envelope"></i>
        </a>
      </div></ul>
</div>


<div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>

<ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href="/dsblog/what-is-llm/" aria-label="Previous page">
            
            <div class="td-pager__meta">
              <i class="fa-solid fa-angle-left"></i>
              <span class="td-pager__meta-label"><b>Previous:</b></span>
              <span class="td-pager__meta-title">What is LLM</span>
            </div>
          </a><a class="td-pager__link td-pager__link--next" href="/dsblog/Machine-Learning-Metrics/" aria-label="Next page">
            <div class="td-pager__meta">
              <span class="td-pager__meta-label"><b>Next:</b></span>
              <span class="td-pager__meta-title">Machine Learning Metrics</span>
              <i class="fa-solid fa-angle-right"></i>
            </div>
          </a></ul>

        </main>
        <div class="col-md-3">
          
          
            <aside class="td-sidebar-right td-sidebar--flush">
              <div class="td-sidebar__inner">
                <div class="custom-toc">
                  <h5 class="custom-toc__heading">On This Page</h5>
                  <nav id="TableOfContents">
  <ul>
    <li><a href="#paper-outcome">Paper Outcome</a></li>
    <li><a href="#ideas-from-the-paper">Ideas from the Paper</a>
      <ul>
        <li><a href="#main-ideas">Main Ideas</a></li>
        <li><a href="#general-ideas">General Ideas</a></li>
        <li><a href="#task-types-and-typical-applications">Task Types and Typical Applications</a></li>
      </ul>
    </li>
    <li><a href="#architectures-for-text-generation">Architectures for Text Generation</a></li>
    <li><a href="#modeling-different-data-types-from-input">Modeling Different Data Types from Input</a>
      <ul>
        <li><a href="#unstructured-input">Unstructured Input</a></li>
        <li><a href="#structured-input">Structured Input</a></li>
        <li><a href="#multimedia-input">Multimedia Input</a></li>
      </ul>
    </li>
    <li><a href="#satisfying-special-properties-for-output-text">Satisfying Special Properties for Output Text</a></li>
    <li><a href="#summary-from-introduction">Summary from Introduction</a></li>
    <li><a href="#conclusion--future-recommendations">Conclusion &amp; Future Recommendations</a></li>
  </ul>
</nav>
                </div>
              </div>
            </aside>
          
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      
      <div class="col-2">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="http://localhost:1313/assets/images/site-logo.png" alt="dasarpAI" width="100" style="border-radius: 12px;">
        </a>
      </div>
      <div class="col-8"><div class="row"><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Key Links</h4>
                    <ul><li><a href="/aboutme">About Me</a></li><li><a href="/dscourses">My Data Science Courses/Services</a></li><li><a href="/summary-of-al-ml-projects">MyWork by Business Domain</a></li><li><a href="/summary-of-my-technology-stacks">MyWork by Tech Stack</a></li><li><a href="/summary-of-management-projects">MyWork in Project Management</a></li><li><a href="/clients">Clients</a></li><li><a href="/testimonials">Testimonial</a></li><li><a href="/terms-of-service">Terms &amp; Condition</a></li><li><a href="/privacy">Privacy Policy</a></li><li><a href="/comment-policy">Comment Policy</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>My Blogs</h4>
                    <ul><li><a href="/dsblog">Data Science Blog</a></li><li><a href="/booksumary">Books/Interviews Blog</a></li><li><a href="/news">AI and Business News</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li><li><a href="/wiaposts">Wisdom in Awareness Blog</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/mychanting">My Chantings</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>All Resources</h4>
                    <ul><li><a href="/datascience-tags#ds-resources">DS Resources</a></li><li><a href="https://aibenchmark-explorer.dasarpai.com">AI Benchmark Explorer</a></li><li><a href="/dsblog/ds-ai-ml-books">Data Science-Books</a></li><li><a href="/dsblog/data-science-cheatsheets">Data Science/AI Cheatsheets</a></li><li><a href="/dsblog/best-youtube-channels-for-ds">Video Channels to Learn DS/AI</a></li><li><a href="/dsblog/ds-ai-ml-interview-resources">DS/AI Interview Questions</a></li><li><a href="https://github.com/dasarpai/DAI-Datasets">GitHub DAI-Datasets</a></li><li><a href="/pmi-templates">PMBOK6 Templates</a></li><li><a href="/prince2-templates">PRINCE2 Templates</a></li><li><a href="/microsoft-pm-templates">Microsoft PM Templates</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Project Management</h4>
                    <ul><li><a href="/pmlogy-home">PMLOGY Home</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmglossary">PM Glossary</a></li><li><a href="/pmlogy-tags">PM Topics</a></li><li><a href="/pmbok6-tags">PMBOK6 Topics</a></li><li><a href="/pmbok6-summary">PMBOK6</a></li><li><a href="/pmbok6">PMBOK6 Explorer</a></li><li><a href="/pmbok6hi-tags">PMBOK6 Hindi Topics</a></li><li><a href="/pmbok6hi-summary">PMBoK6 Hindi</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li></ul>
                  </div>
                </div></div>
      


      <div class="row"><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Wisdom in Awareness</h4>
                  <ul><li><a href="/wia-home">WIA Home</a></li><li><a href="/wiaposts">WIA Blog</a></li><li><a href="/wia-tags">WIA Topics</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li><li><a href="/gk-tags">GK Topic</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Samskrutyatra</h4>
                  <ul><li><a href="/samskrutyatra-home">SamskrutYatra Home</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/samskrutyatra-tags">Samskrut Topics</a></li><li><a href="/mychanting">My Vedic Chantings</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>My Gallery</h4>
                  <ul><li><a href="/gallary/slider-online-sessions1">Online AI Classes 1</a></li><li><a href="/gallary/slider-online-sessions2">Online AI Classes 2</a></li><li><a href="/gallary/slider-online-sessions3">Online AI Classes 3</a></li><li><a href="/gallary/slider-online-sessions4">Online AI Classes 4</a></li><li><a href="/gallary/slider-pm-selected-photos">Management Classes</a></li><li><a href="/gallary/slider-pm-workshops">PM &amp; DS Workshop</a></li></ul>
                </div>
              </div></div>
    </div>

    <div class="col-2">

    </div>

      
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.js"></script>
<script src='/js/prism.js'></script>
<script src='/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>
