<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/site/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=site/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/site/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/site/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/site/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Basics of Word Embedding | Agones</title><meta property="og:url" content="http://localhost:1313/site/dsblog/2023-11-11-6101-basics-of-word-embedding/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Basics of Word Embedding">
  <meta property="og:description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2023-11-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2023-11-11T00:00:00+00:00">
    <meta property="article:tag" content="Word Embedding">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Vector Representation">
    <meta property="article:tag" content="Text Processing">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Neural Networks">

  <meta itemprop="name" content="Basics of Word Embedding">
  <meta itemprop="description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence">
  <meta itemprop="datePublished" content="2023-11-11T00:00:00+00:00">
  <meta itemprop="dateModified" content="2023-11-11T00:00:00+00:00">
  <meta itemprop="wordCount" content="2212">
  <meta itemprop="keywords" content="Word Embeddings,Text Vectorization,Natural Language Processing,Word2Vec,GloVe,BERT Embeddings,Neural Language Models,Text Analysis">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Basics of Word Embedding">
  <meta name="twitter:description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence">



<link rel="stylesheet" href="/site/css/prism.css"/>

<link href="/site/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/site/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/site/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/site/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete="off">
</div></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <main role="main" class="td-main">
        <p><img src="/assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg" alt="Basics of Word Embedding"></p>
<h1 id="basics-of-word-embedding">Basics of Word Embedding</h1>
<h2 id="what-is-context-target-and-window">What is Context, target and window?</h2>
<ul>
<li>The &ldquo;context&rdquo; word is the surrounding word.</li>
<li>The &ldquo;target&rdquo; word is the middle word.</li>
<li>The &ldquo;window distance&rdquo; is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.</li>
</ul>
<p>Let&rsquo;s take a sentence</p>
<blockquote>
<p>The quick brown fox jump over a lazy dog.</p></blockquote>
<p>R- Right, L - Left</p>
<table>
  <thead>
      <tr>
          <th>target</th>
          <th>context 1 window</th>
          <th>context 2 window</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>the</td>
          <td>quick (R)</td>
          <td>quick(R), brown(R)</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>the(L), brown(R)</td>
          <td>the(L), brown(R), fox(R)</td>
      </tr>
      <tr>
          <td>brown</td>
          <td>quick(L), fox(R)</td>
          <td>the(L), quick(L), fox(R), jump(R)</td>
      </tr>
      <tr>
          <td>fox</td>
          <td>brown(L), jump(R)</td>
          <td>quick(L), brown(L), jump(R), over(R)</td>
      </tr>
  </tbody>
</table>
<p>When creating dataset you don&rsquo;t write multiple words in one row, but you create multiple rows, as below.</p>
<table>
  <thead>
      <tr>
          <th>target</th>
          <th>context 2 window</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>the</td>
          <td>quick</td>
      </tr>
      <tr>
          <td>the</td>
          <td>brown</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>the</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>brown</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>fox</td>
      </tr>
  </tbody>
</table>
<h2 id="what-is-skipgram">What is Skipgram?</h2>
<p>Skipgram: <strong>With the help of target word</strong> we want to predict the context/surrounding word. From above example predicting &ldquo;quick&rdquo;, &ldquo;brown&rdquo;, &ldquo;the&rdquo;, &ldquo;brown&rdquo; etc with target word &ldquo;the&rdquo;, &ldquo;quick&rdquo;</p>
<h2 id="what-is-cbow-continuous-bag-of-words">What is CBOW (Continuous Bag of Words)</h2>
<p>CBOW : <strong>With the help of context</strong> we want to predict target. From above example, predicting &ldquo;the&rdquo;, &ldquo;quick&rdquo; when context words are &ldquo;quick&rdquo; or &ldquo;brown&rdquo;, &ldquo;the&rdquo;, &ldquo;fox&rdquo;.</p>
<h2 id="how-cbow-works">How CBOW works?</h2>
<p>For both, CBOW and Skipgram networks works in the same way as mentioned below. Only difference is when we are using CBOW we want to predict target word from context word. If you are using Skipgram then we want to predict context word from a target word.</p>
<h3 id="finalize-the-corpus-step-1">Finalize the corpus (Step 1)</h3>
<p>In reality corpus is extremely huge size, it is like entire wikipedia text or entire stakeoverflow text or entire quora text. For the illustration of skipgram we are taking a small example.</p>
<p><strong>Corpus</strong> : The quick brown fox jump over the dog</p>
<h3 id="create-skipgram-step-234-">Create Skipgram (Step 2+3+4 )</h3>
<p>As discussed earlier created 1 or 2 or 3 window skipgram from the corpus.</p>
<table>
  <thead>
      <tr>
          <th>Word</th>
          <th>(Step 3) onehot encoding for each word in the corpus</th>
          <th>(Step 4) random initial embedding, 4 dimensional</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>the</td>
          <td>[1,0,0,0,0,0,0,0]</td>
          <td>[0.11,0.12,0.14,0.15]</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>[0,1,0,0,0,0,0,0]</td>
          <td>[0.21,0.23,0.24,0.26]</td>
      </tr>
      <tr>
          <td>brown</td>
          <td>[0,0,1,0,0,0,0,0]</td>
          <td>[0.31,0.34,0.36,0.38]</td>
      </tr>
      <tr>
          <td>fox</td>
          <td>[0,0,0,1,0,0,0,0]</td>
          <td>[0.51,0.12,0.14,0.15]</td>
      </tr>
      <tr>
          <td>jump</td>
          <td>[0,0,0,0,1,0,0,0]</td>
          <td>[0.21,0.63,0.24,0.26]</td>
      </tr>
      <tr>
          <td>over</td>
          <td>[0,0,0,0,0,1,0,0]</td>
          <td>[0.31,0.34,0.86,0.38]</td>
      </tr>
      <tr>
          <td>the</td>
          <td>[0,0,0,0,0,0,1,0]</td>
          <td>[0.71,0.12,0.14,0.15]</td>
      </tr>
      <tr>
          <td>dog</td>
          <td>[0,0,0,0,0,0,0,1]</td>
          <td>[0.21,0.93,0.24,0.26]</td>
      </tr>
  </tbody>
</table>
<h3 id="create-neural-network-step-5">Create Neural Network (Step 5)</h3>
<p>Create a neural network for learning embedding.</p>
<ul>
<li>One input layer which can accept token/words. Convert token (context and target words) into onehot encoding</li>
<li>One embedding layer, for example sake we are taking 4 dimensional embedding of words. These embedding are randomingly intiated number initally (there are other ways also).</li>
<li>One dense layer of 5 neuron (example)</li>
<li>Softmax function</li>
<li>Output layer (to predict the probability of the predicted word. If vocabulary size of the corpus is 10,000 words, then softmax will predict 10,000 probabilities)</li>
<li>Loss function - Cross entropy loss function. L = $$-  \sum_{i=1}^{N} y_{i} \cdot \log(p_{i})$$, N is vocab size.</li>
<li>4 numbers from embedding will go to each of the 5 neuron, Each neuron will have 4 weights to embedding layer. 5*4 = 20 weights are learned + 5 biases learned</li>
<li>Learning Rate LR = .0002</li>
</ul>
<h3 id="training---forward-propagation-step-678910">Training - Forward propagation (Step 6+7+8+9+10)</h3>
<ul>
<li>Randomly initialize all the weights and biases of the network.</li>
<li>Pass target and context word to the network.</li>
</ul>
<table>
  <thead>
      <tr>
          <th>Step 6</th>
          <th>-</th>
          <th>-</th>
          <th>Step 7</th>
          <th>Step 8</th>
          <th>-</th>
          <th>Step 9</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Input layer</td>
          <td>Embedding layer</td>
          <td>Hidden layer (5 neuron, random init w&amp;b), dense layer</td>
          <td>matmul between weights and inputs (embedding)</td>
          <td>softmax (8 vocab size)</td>
          <td>actual vector for &ldquo;quick&rdquo;</td>
          <td>cross entropy loss</td>
      </tr>
      <tr>
          <td>The (context), quick (target)</td>
          <td>context (The)  = [.11,.12,.14,.15]</td>
          <td>n1=[.11,.12,.13,.14]</td>
          <td>0.0657</td>
          <td>0.1867</td>
          <td>0</td>
          <td>0.0897</td>
      </tr>
      <tr>
          <td>target (quick) = [.21,.23,.24,.26]</td>
          <td>n2=[.13,.14,.15,.16]</td>
          <td>0.0761</td>
          <td>0.1886</td>
          <td>1</td>
          <td>0.7244</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>n3=[.21..22,.23,.24]</td>
          <td>0.1177</td>
          <td>0.1966</td>
          <td>0</td>
          <td>0.0951</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>n4=[.32,.33,.34,.35]</td>
          <td>0.1749</td>
          <td>0.2082</td>
          <td>0</td>
          <td>0.1014</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>n5=[.42,.43,.45,.46]</td>
          <td>0.2298</td>
          <td>0.2199</td>
          <td>0</td>
          <td>0.1079</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td><strong>Step 10</strong></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td>Total Loss</td>
          <td>1.1185</td>
          <td></td>
      </tr>
  </tbody>
</table>
<h3 id="training---backward-propagation-step-1112">Training - backward propagation (Step 11+12)</h3>
<p>Updating weights of network neurons</p>
<table>
  <thead>
      <tr>
          <th>Step 11 (Gradient Calculation for 20 weights)</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>dL/dw1</td>
          <td>10.17</td>
          <td>9.32</td>
          <td>8.60</td>
          <td>7.99</td>
      </tr>
      <tr>
          <td>dL/dw2</td>
          <td>8.60</td>
          <td>7.99</td>
          <td>7.46</td>
          <td>6.99</td>
      </tr>
      <tr>
          <td>dL/dw3</td>
          <td>5.33</td>
          <td>5.08</td>
          <td>4.66</td>
          <td>4.66</td>
      </tr>
      <tr>
          <td>dL/dw4</td>
          <td>3.50</td>
          <td>3.39</td>
          <td>3.20</td>
          <td>3.20</td>
      </tr>
      <tr>
          <td>dL/dw5</td>
          <td>2.66</td>
          <td>2.60</td>
          <td>2.43</td>
          <td>2.43</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Step 12 Updated Weights</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>new w1</td>
          <td>0.11</td>
          <td>0.12</td>
          <td>0.13</td>
          <td>0.14</td>
      </tr>
      <tr>
          <td>new w2</td>
          <td>0.13</td>
          <td>0.14</td>
          <td>0.15</td>
          <td>0.16</td>
      </tr>
      <tr>
          <td>new w3</td>
          <td>0.21</td>
          <td>0.22</td>
          <td>0.23</td>
          <td>0.24</td>
      </tr>
      <tr>
          <td>new d4</td>
          <td>0.32</td>
          <td>0.33</td>
          <td>0.34</td>
          <td>0.35</td>
      </tr>
      <tr>
          <td>new w5</td>
          <td>0.42</td>
          <td>0.43</td>
          <td>0.45</td>
          <td>0.46</td>
      </tr>
  </tbody>
</table>
<p>new weight = old weight - Learning Rate * DL/dW</p>
<h3 id="update-embedding-step-1314-">Update Embedding (Step 13+14 )</h3>
<table>
  <thead>
      <tr>
          <th>Old Embedding (Vector)</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>context (The)  = [.11,.12,.14,.15]</td>
          <td>0.110</td>
          <td>0.120</td>
          <td>0.140</td>
          <td>0.150</td>
      </tr>
      <tr>
          <td>target (quick) = [.21,.23,.24,.26]</td>
          <td>0.210</td>
          <td>0.230</td>
          <td>0.240</td>
          <td>0.260</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Step 13 (Gradient of Old Embedding)</th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>dL/context</td>
          <td>10.17</td>
          <td>9.32</td>
          <td>7.99</td>
      </tr>
      <tr>
          <td>dL/target</td>
          <td>5.33</td>
          <td>4.86</td>
          <td>4.66</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Step 14 (Updated Embedding)</th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>context (The)</td>
          <td>0.108</td>
          <td>0.118</td>
          <td>0.138</td>
      </tr>
      <tr>
          <td>target (quick)</td>
          <td>0.209</td>
          <td>0.229</td>
          <td>0.239</td>
      </tr>
  </tbody>
</table>
<h3 id="complete-the-training">Complete the Training</h3>
<ul>
<li>Perform training forward and backword propagation in batch, multiple words at a time.</li>
<li>Everytime update w&amp;b and also update embedding.</li>
<li>Trained embedding can be used in future without these training steps.</li>
<li>Let entire dataset of paired words go through this network. One it goes through it is called one epoch.</li>
<li>Let embedding get updated over multiple epoch say 50 or 100. More epoch, will cause better embedding. It will cost more money.</li>
<li>More dimentional vector will have better represenation but will cost more computation and more money.</li>
</ul>
<h2 id="other-methods-of-embedding">Other Methods of Embedding</h2>
<h3 id="tf-idf">TF-IDF</h3>
<p>TF-IDF - Term Frequency - Inverse Document Frequency, is an old, traditional, frequency based text embedding technique. It is not based on neural network architecture therefore does not need expensive hardware to create these embedding and use TF-IDF embedding. Like skipgram or CBOW it is not vector based but frequency based, therefore understandign symantic of the text is not possible with TF-IDF. There is no use of pretrained embedding, everytime we have a corpus we need to create embedding for that and it is used only for that. We cannot use TF-IDF embedding, which was created using news text for something else, say history or enterainment. Thus, embedding transfer is meaninless but task transfer can be done. It means TF-IDF embedding which is used for classficittion purpose can be used for other task like topic modelling, sentiment analysis etc. Obviously there is a limit, we cannot use it for other task like translation or summarization.</p>
<h4 id="how-tf-idf-works">How TF-IDF works?</h4>
<ul>
<li>Term frequency (TF): The number of times a word appears in a document.</li>
<li>Inverse document frequency (IDF): The logarithm of the number of documents in the collection divided by the number of documents that contain the word.</li>
<li>The TF-IDF score for a word in a document is calculated as follows:</li>
<li>TF-IDF = TF * IDF (The higher the TF-IDF score, the more important the word is to the document.)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Document_1: <span style="color:#e6db74">&#34;The quick brown fox jumps over the lazy dog.&#34;</span>
</span></span><span style="display:flex;"><span>Document_2: <span style="color:#e6db74">&#34;The dog is lazy, but the fox is quick.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;quick&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF(quick, Document_1) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;quick&#34;</span>
</span></span><span style="display:flex;"><span>IDF(quick) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;quick&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(quick, Document_1) <span style="color:#f92672">=</span> TF(quick, Document_1) <span style="color:#f92672">*</span> IDF(quick) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;quick&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF(quick, Document_2) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;quick&#34;</span>
</span></span><span style="display:flex;"><span>IDF(quick) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;quick&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(quick, Document_2) <span style="color:#f92672">=</span> TF(quick, Document_2) <span style="color:#f92672">*</span> IDF(quick) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;lazy&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF(lazy, Document_1) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;lazy&#34;</span>
</span></span><span style="display:flex;"><span>IDF(lazy) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;lazy&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(lazy, Document_1) <span style="color:#f92672">=</span> TF(lazy, Document_1) <span style="color:#f92672">*</span> IDF(lazy) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;lazy&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF(lazy, Document_2) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;lazy&#34;</span>
</span></span><span style="display:flex;"><span>IDF(lazy) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;lazy&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(lazy, Document_2) <span style="color:#f92672">=</span> TF(lazy, Document_2) <span style="color:#f92672">*</span> IDF(lazy) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><h3 id="glove-global-vectors">GloVe (Global Vectors)</h3>
<p>GloVe is a method that learns word embeddings from global word-word co-occurrence statistics. It is similar to Skipgram and CBOW, but it is better at capturing long-range semantic relationships between words. GloVe embedding is good for text classification, and machine translation (MT).</p>
<h4 id="how-glove-embedding-works">How GloVe embedding works?</h4>
<ul>
<li>Tokenize the corpus: Split the corpus into individual words and punctuation marks.</li>
<li>Count word co-occurrences: For each word in the vocabulary, count how many times it co-occurs with other words in a given window size.</li>
<li>Build a word-word co-occurrence matrix: The word-word co-occurrence matrix is a square matrix, where each row and column represents a word in the vocabulary. The value at each cell in the matrix represents the number of times the two corresponding words co-occur in the corpus.</li>
<li>Factorize the word-word co-occurrence matrix: Factorize the word-word co-occurrence matrix into two lower-dimensional matrices, one for <strong>word embeddings</strong> (relationship between words) and one for <strong>context embeddings</strong> (relationship between words in the context). We can factorize the word-word co-occurrence matrix using a variety of matrix factorization techniques, such as singular value decomposition (SVD) or nonnegative matrix factorization (NMF).</li>
<li>Normalize the word embeddings: Normalize the word embeddings so that they have a unit length. We can normalize the word embeddings by dividing each embedding by its L2 norm. This will ensure that all of the embeddings have a unit length.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> KeyedVectors
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load the corpus</span>
</span></span><span style="display:flex;"><span>corpus <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#34;corpus.txt&#34;</span>, <span style="color:#e6db74">&#34;r&#34;</span>)<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Tokenize the corpus</span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> corpus<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Count word co-occurrences</span>
</span></span><span style="display:flex;"><span>word_co_occurrences <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(tokens), len(tokens)))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> tokens[i] <span style="color:#f92672">!=</span> tokens[j]:
</span></span><span style="display:flex;"><span>            word_co_occurrences[i, j] <span style="color:#f92672">=</span> tokens<span style="color:#f92672">.</span>count(tokens[i] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> tokens[j])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Factorize the word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>glove_model <span style="color:#f92672">=</span> KeyedVectors(word_vectors<span style="color:#f92672">=</span>word_co_occurrences, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save the word embeddings</span>
</span></span><span style="display:flex;"><span>glove_model<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;glove_embeddings.txt&#34;</span>)
</span></span></code></pre></div><p>How SVD (Singular Value Decomposition) works?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>word_co_occurrences <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform SVD</span>
</span></span><span style="display:flex;"><span>U, S, Vh <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(word_co_occurrences)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Truncate the singular values</span>
</span></span><span style="display:flex;"><span>S_truncated <span style="color:#f92672">=</span> S[:<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reconstruct the word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>word_co_occurrences_reconstructed <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(U[:, :<span style="color:#ae81ff">2</span>], np<span style="color:#f92672">.</span>dot(S_truncated, Vh[:, :<span style="color:#ae81ff">2</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print the reconstructed word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>print(word_co_occurrences_reconstructed)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Results</span>
</span></span><span style="display:flex;"><span>[[<span style="color:#f92672">-</span><span style="color:#ae81ff">0.50578521</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.25523155</span>]
</span></span><span style="display:flex;"><span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">0.58437383</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.60130182</span>]
</span></span><span style="display:flex;"><span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">0.63457746</span>  <span style="color:#ae81ff">0.75716113</span>]]
</span></span><span style="display:flex;"><span>[[<span style="color:#f92672">-</span><span style="color:#ae81ff">0.50578521</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.58437383</span>]
</span></span><span style="display:flex;"><span> [ <span style="color:#ae81ff">0.25523155</span>  <span style="color:#ae81ff">0.60130182</span>]
</span></span><span style="display:flex;"><span> [ <span style="color:#ae81ff">0.82403773</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.54492509</span>]]
</span></span></code></pre></div><h3 id="bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)</h3>
<p>BERT is a transformer-based language model that can learn word embeddings from unlabeled text, we need not to create skipgram pairs. BERT embeddings are particularly good at capturing <strong>contextual information</strong>. BERT embedding is good for MT, QA, Classification tasks.</p>
<h4 id="how-bert-does-embedding">How BERT does embedding?</h4>
<ul>
<li>Tokenization: The first step is to tokenize the sentence into words. This means splitting the sentence into individual words, including punctuation marks. The tokenized sentence is then represented as a sequence of integers (we create ids), where each integer represents a word in the vocabulary.</li>
<li>Word embedding lookup: BERT uses a pre-trained word embedding table to convert each word in the sequence into a vector of numbers. This vector represents the meaning of the word in a distributed manner.</li>
<li>Segment embedding lookup: BERT also uses a segment embedding table to encode the position of each word in the sentence. This is necessary because BERT is a bidirectional language model, and it needs to know the context of each word in order to learn meaningful embeddings.</li>
<li>Positional embedding lookup: BERT also uses a positional embedding table to encode the absolute position of each word in the sentence. This is necessary because BERT needs to know the order of the words in the sentence in order to learn meaningful embeddings.</li>
<li>Transformer encoding: The encoded sequence of word embeddings, segment embeddings, and positional embeddings is then passed to the transformer encoder. The transformer encoder is a neural network architecture that learns long-range dependencies between words in a sentence.</li>
<li>Output embedding: The output of the transformer encoder is a sequence of vectors, where each vector represents the embedding of the corresponding word in the sentence. These embeddings are then used for downstream natural language processing tasks, such as machine translation, text classification, and question answering.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Tokenize the sentence</span>
</span></span><span style="display:flex;"><span>sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The quick brown fox jump over the lazy fox&#34;</span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> sentence<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert each word to a word embedding vector</span>
</span></span><span style="display:flex;"><span>word_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens:
</span></span><span style="display:flex;"><span>    word_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_word_embedding(token))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create segment embeddings</span>
</span></span><span style="display:flex;"><span>segment_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&lt;</span> len(tokens) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>        segment_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_segment_embedding(<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        segment_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_segment_embedding(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create positional embeddings</span>
</span></span><span style="display:flex;"><span>positional_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>    positional_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_positional_embedding(i))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Encode the sentence</span>
</span></span><span style="display:flex;"><span>encoded_sentence <span style="color:#f92672">=</span> bert_model<span style="color:#f92672">.</span>encode(word_embeddings, segment_embeddings, positional_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output embeddings</span>
</span></span><span style="display:flex;"><span>output_embeddings <span style="color:#f92672">=</span> encoded_sentence
</span></span></code></pre></div><h3 id="fasttext-fast-text">FastText (Fast Text)</h3>
<p>FastText is a modification of Skipgram that can learn embeddings for words and subwords. This makes it better at representing rare words and out-of-vocabulary words. FastText is good for name-entity-recognition (NER) &amp; Question Answering (QA) tasks.</p>
<h3 id="elmo-embeddings-from-language-models">ELMo (Embeddings from Language Models)</h3>
<p>ELMo (Embeddings from Language Models) is a deep contextual word embedding technique that uses a bidirectional language model (biLM) to learn word representations. A biLM is a type of neural network that can learn to predict the next word in a sentence, as well as the previous word. Unlike skipgram, which predicts next words, biLM is bidirectional. From a target word biLM can predict next and previous words.</p>
<h1 id="resources">Resources</h1>
<p>If you want to understand all skipgram/cbow caluclation with excel and then you can use this <a href="https://docs.google.com/spreadsheets/d/1eU4EVtUzD1w_ILcpJVTc6oK2KH9vEDK7OuXFtyv1_gU/edit?usp=sharing">calculation sheet</a></p>

      </main>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/site/js/main.js"></script>
<script src='/site/js/prism.js'></script>
<script src='/site/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/site/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>