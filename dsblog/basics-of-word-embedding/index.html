<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Basics of Word Embedding | Agones</title><meta property="og:url" content="http://localhost:1313/dsblog/basics-of-word-embedding/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Basics of Word Embedding">
  <meta property="og:description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2023-11-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T15:25:42+05:30">
    <meta property="article:tag" content="Word Embedding">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Vector Representation">
    <meta property="article:tag" content="Text Processing">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Neural Networks">

  <meta itemprop="name" content="Basics of Word Embedding">
  <meta itemprop="description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence">
  <meta itemprop="datePublished" content="2023-11-11T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-08T15:25:42+05:30">
  <meta itemprop="wordCount" content="2212">
  <meta itemprop="keywords" content="Word Embeddings,Text Vectorization,Natural Language Processing,Word2Vec,GloVe,BERT Embeddings,Neural Language Models,Text Analysis">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Basics of Word Embedding">
  <meta name="twitter:description" content="Basics of Word Embedding What is Context, target and window? The “context” word is the surrounding word. The “target” word is the middle word. The “window distance” is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right. Let’s take a sentence">



<link rel="stylesheet" href="/css/prism.css"/>

<link href="/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>


<link rel="stylesheet" href="/css/custom.css">

<script src="/js/lunr.js"></script>


    <style>
       
      .td-main img {
        max-width: 100%;
        height: auto;
      }
      .td-main {
        padding-top: 60px;  
      }
       
      .td-sidebar-right {
          padding-left: 20px;  
      }
    </style>
  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/samskrutyatra/"><span>Samskrut Yatra Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search position-relative">
  <div class="td-search__icon"></div>
  <input
    id="agones-search"
    type="search"
    class="td-search__input form-control td-search-input"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
  >
  <ul id="agones-search-results" class="list-group position-absolute w-100" style="z-index:1000; top:100%; left:0;"></ul>
</div>

<script>
let lunrIndex, pagesIndex;

async function initLunr() {
  const response = await fetch('/index.json');
  pagesIndex = await response.json();
  lunrIndex = lunr(function () {
    this.ref('url');
    this.field('title', { boost: 10 });
    this.field('content');
    pagesIndex.forEach(function (doc) {
      this.add(doc);
    }, this);
  });
}

function search(query) {
  if (!lunrIndex || !query) return [];
  return lunrIndex.search(query).map(result =>
    pagesIndex.find(page => page.url === result.ref)
  );
}

document.addEventListener('DOMContentLoaded', function () {
  initLunr();
  const input = document.getElementById('agones-search');
  const resultsList = document.getElementById('agones-search-results');
  input.addEventListener('input', function (e) {
    const query = e.target.value.trim();
    if (!query) {
      resultsList.innerHTML = '';
      resultsList.style.display = 'none';
      return;
    }
    const results = search(query);
    if (results.length === 0) {
      resultsList.innerHTML = '<li class="list-group-item">No results found.</li>';
      resultsList.style.display = 'block';
      return;
    }
    resultsList.innerHTML = results.map(page =>
      `<li class="list-group-item"><a href="${page.url}">${page.title}</a></li>`
    ).join('');
    resultsList.style.display = 'block';
  });
  
  input.addEventListener('blur', function() {
    setTimeout(() => { resultsList.style.display = 'none'; }, 200);
  });
  
  input.addEventListener('focus', function() {
    if (input.value.trim()) resultsList.style.display = 'block';
  });
});
</script></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <div class="row">
        <div class="col-md-3">
          
        </div>
        <main role="main" class="col-md-6 td-main">
          <p><img src="/assets/images/dspost/dsp6101-Basics-of-Word-Embedding.jpg" alt="Basics of Word Embedding"></p>
<h1 id="basics-of-word-embedding">Basics of Word Embedding</h1>
<h2 id="what-is-context-target-and-window">What is Context, target and window?</h2>
<ul>
<li>The &ldquo;context&rdquo; word is the surrounding word.</li>
<li>The &ldquo;target&rdquo; word is the middle word.</li>
<li>The &ldquo;window distance&rdquo; is number of words (including) between context words and target word. Window distance 1 means, one word surronding the target, one left side context word, one right context word. Two window distance means 2 words left and 2 words right.</li>
</ul>
<p>Let&rsquo;s take a sentence</p>
<blockquote>
<p>The quick brown fox jump over a lazy dog.</p></blockquote>
<p>R- Right, L - Left</p>
<table>
  <thead>
      <tr>
          <th>target</th>
          <th>context 1 window</th>
          <th>context 2 window</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>the</td>
          <td>quick (R)</td>
          <td>quick(R), brown(R)</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>the(L), brown(R)</td>
          <td>the(L), brown(R), fox(R)</td>
      </tr>
      <tr>
          <td>brown</td>
          <td>quick(L), fox(R)</td>
          <td>the(L), quick(L), fox(R), jump(R)</td>
      </tr>
      <tr>
          <td>fox</td>
          <td>brown(L), jump(R)</td>
          <td>quick(L), brown(L), jump(R), over(R)</td>
      </tr>
  </tbody>
</table>
<p>When creating dataset you don&rsquo;t write multiple words in one row, but you create multiple rows, as below.</p>
<table>
  <thead>
      <tr>
          <th>target</th>
          <th>context 2 window</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>the</td>
          <td>quick</td>
      </tr>
      <tr>
          <td>the</td>
          <td>brown</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>the</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>brown</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>fox</td>
      </tr>
  </tbody>
</table>
<h2 id="what-is-skipgram">What is Skipgram?</h2>
<p>Skipgram: <strong>With the help of target word</strong> we want to predict the context/surrounding word. From above example predicting &ldquo;quick&rdquo;, &ldquo;brown&rdquo;, &ldquo;the&rdquo;, &ldquo;brown&rdquo; etc with target word &ldquo;the&rdquo;, &ldquo;quick&rdquo;</p>
<h2 id="what-is-cbow-continuous-bag-of-words">What is CBOW (Continuous Bag of Words)</h2>
<p>CBOW : <strong>With the help of context</strong> we want to predict target. From above example, predicting &ldquo;the&rdquo;, &ldquo;quick&rdquo; when context words are &ldquo;quick&rdquo; or &ldquo;brown&rdquo;, &ldquo;the&rdquo;, &ldquo;fox&rdquo;.</p>
<h2 id="how-cbow-works">How CBOW works?</h2>
<p>For both, CBOW and Skipgram networks works in the same way as mentioned below. Only difference is when we are using CBOW we want to predict target word from context word. If you are using Skipgram then we want to predict context word from a target word.</p>
<h3 id="finalize-the-corpus-step-1">Finalize the corpus (Step 1)</h3>
<p>In reality corpus is extremely huge size, it is like entire wikipedia text or entire stakeoverflow text or entire quora text. For the illustration of skipgram we are taking a small example.</p>
<p><strong>Corpus</strong> : The quick brown fox jump over the dog</p>
<h3 id="create-skipgram-step-234-">Create Skipgram (Step 2+3+4 )</h3>
<p>As discussed earlier created 1 or 2 or 3 window skipgram from the corpus.</p>
<table>
  <thead>
      <tr>
          <th>Word</th>
          <th>(Step 3) onehot encoding for each word in the corpus</th>
          <th>(Step 4) random initial embedding, 4 dimensional</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>the</td>
          <td>[1,0,0,0,0,0,0,0]</td>
          <td>[0.11,0.12,0.14,0.15]</td>
      </tr>
      <tr>
          <td>quick</td>
          <td>[0,1,0,0,0,0,0,0]</td>
          <td>[0.21,0.23,0.24,0.26]</td>
      </tr>
      <tr>
          <td>brown</td>
          <td>[0,0,1,0,0,0,0,0]</td>
          <td>[0.31,0.34,0.36,0.38]</td>
      </tr>
      <tr>
          <td>fox</td>
          <td>[0,0,0,1,0,0,0,0]</td>
          <td>[0.51,0.12,0.14,0.15]</td>
      </tr>
      <tr>
          <td>jump</td>
          <td>[0,0,0,0,1,0,0,0]</td>
          <td>[0.21,0.63,0.24,0.26]</td>
      </tr>
      <tr>
          <td>over</td>
          <td>[0,0,0,0,0,1,0,0]</td>
          <td>[0.31,0.34,0.86,0.38]</td>
      </tr>
      <tr>
          <td>the</td>
          <td>[0,0,0,0,0,0,1,0]</td>
          <td>[0.71,0.12,0.14,0.15]</td>
      </tr>
      <tr>
          <td>dog</td>
          <td>[0,0,0,0,0,0,0,1]</td>
          <td>[0.21,0.93,0.24,0.26]</td>
      </tr>
  </tbody>
</table>
<h3 id="create-neural-network-step-5">Create Neural Network (Step 5)</h3>
<p>Create a neural network for learning embedding.</p>
<ul>
<li>One input layer which can accept token/words. Convert token (context and target words) into onehot encoding</li>
<li>One embedding layer, for example sake we are taking 4 dimensional embedding of words. These embedding are randomingly intiated number initally (there are other ways also).</li>
<li>One dense layer of 5 neuron (example)</li>
<li>Softmax function</li>
<li>Output layer (to predict the probability of the predicted word. If vocabulary size of the corpus is 10,000 words, then softmax will predict 10,000 probabilities)</li>
<li>Loss function - Cross entropy loss function. L = $$-  \sum_{i=1}^{N} y_{i} \cdot \log(p_{i})$$, N is vocab size.</li>
<li>4 numbers from embedding will go to each of the 5 neuron, Each neuron will have 4 weights to embedding layer. 5*4 = 20 weights are learned + 5 biases learned</li>
<li>Learning Rate LR = .0002</li>
</ul>
<h3 id="training---forward-propagation-step-678910">Training - Forward propagation (Step 6+7+8+9+10)</h3>
<ul>
<li>Randomly initialize all the weights and biases of the network.</li>
<li>Pass target and context word to the network.</li>
</ul>
<table>
  <thead>
      <tr>
          <th>Step 6</th>
          <th>-</th>
          <th>-</th>
          <th>Step 7</th>
          <th>Step 8</th>
          <th>-</th>
          <th>Step 9</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Input layer</td>
          <td>Embedding layer</td>
          <td>Hidden layer (5 neuron, random init w&amp;b), dense layer</td>
          <td>matmul between weights and inputs (embedding)</td>
          <td>softmax (8 vocab size)</td>
          <td>actual vector for &ldquo;quick&rdquo;</td>
          <td>cross entropy loss</td>
      </tr>
      <tr>
          <td>The (context), quick (target)</td>
          <td>context (The)  = [.11,.12,.14,.15]</td>
          <td>n1=[.11,.12,.13,.14]</td>
          <td>0.0657</td>
          <td>0.1867</td>
          <td>0</td>
          <td>0.0897</td>
      </tr>
      <tr>
          <td>target (quick) = [.21,.23,.24,.26]</td>
          <td>n2=[.13,.14,.15,.16]</td>
          <td>0.0761</td>
          <td>0.1886</td>
          <td>1</td>
          <td>0.7244</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>n3=[.21..22,.23,.24]</td>
          <td>0.1177</td>
          <td>0.1966</td>
          <td>0</td>
          <td>0.0951</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>n4=[.32,.33,.34,.35]</td>
          <td>0.1749</td>
          <td>0.2082</td>
          <td>0</td>
          <td>0.1014</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>n5=[.42,.43,.45,.46]</td>
          <td>0.2298</td>
          <td>0.2199</td>
          <td>0</td>
          <td>0.1079</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td><strong>Step 10</strong></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td>Total Loss</td>
          <td>1.1185</td>
          <td></td>
      </tr>
  </tbody>
</table>
<h3 id="training---backward-propagation-step-1112">Training - backward propagation (Step 11+12)</h3>
<p>Updating weights of network neurons</p>
<table>
  <thead>
      <tr>
          <th>Step 11 (Gradient Calculation for 20 weights)</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>dL/dw1</td>
          <td>10.17</td>
          <td>9.32</td>
          <td>8.60</td>
          <td>7.99</td>
      </tr>
      <tr>
          <td>dL/dw2</td>
          <td>8.60</td>
          <td>7.99</td>
          <td>7.46</td>
          <td>6.99</td>
      </tr>
      <tr>
          <td>dL/dw3</td>
          <td>5.33</td>
          <td>5.08</td>
          <td>4.66</td>
          <td>4.66</td>
      </tr>
      <tr>
          <td>dL/dw4</td>
          <td>3.50</td>
          <td>3.39</td>
          <td>3.20</td>
          <td>3.20</td>
      </tr>
      <tr>
          <td>dL/dw5</td>
          <td>2.66</td>
          <td>2.60</td>
          <td>2.43</td>
          <td>2.43</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Step 12 Updated Weights</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>new w1</td>
          <td>0.11</td>
          <td>0.12</td>
          <td>0.13</td>
          <td>0.14</td>
      </tr>
      <tr>
          <td>new w2</td>
          <td>0.13</td>
          <td>0.14</td>
          <td>0.15</td>
          <td>0.16</td>
      </tr>
      <tr>
          <td>new w3</td>
          <td>0.21</td>
          <td>0.22</td>
          <td>0.23</td>
          <td>0.24</td>
      </tr>
      <tr>
          <td>new d4</td>
          <td>0.32</td>
          <td>0.33</td>
          <td>0.34</td>
          <td>0.35</td>
      </tr>
      <tr>
          <td>new w5</td>
          <td>0.42</td>
          <td>0.43</td>
          <td>0.45</td>
          <td>0.46</td>
      </tr>
  </tbody>
</table>
<p>new weight = old weight - Learning Rate * DL/dW</p>
<h3 id="update-embedding-step-1314-">Update Embedding (Step 13+14 )</h3>
<table>
  <thead>
      <tr>
          <th>Old Embedding (Vector)</th>
          <th>1</th>
          <th>2</th>
          <th>3</th>
          <th>4</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>context (The)  = [.11,.12,.14,.15]</td>
          <td>0.110</td>
          <td>0.120</td>
          <td>0.140</td>
          <td>0.150</td>
      </tr>
      <tr>
          <td>target (quick) = [.21,.23,.24,.26]</td>
          <td>0.210</td>
          <td>0.230</td>
          <td>0.240</td>
          <td>0.260</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Step 13 (Gradient of Old Embedding)</th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>dL/context</td>
          <td>10.17</td>
          <td>9.32</td>
          <td>7.99</td>
      </tr>
      <tr>
          <td>dL/target</td>
          <td>5.33</td>
          <td>4.86</td>
          <td>4.66</td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>Step 14 (Updated Embedding)</th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>context (The)</td>
          <td>0.108</td>
          <td>0.118</td>
          <td>0.138</td>
      </tr>
      <tr>
          <td>target (quick)</td>
          <td>0.209</td>
          <td>0.229</td>
          <td>0.239</td>
      </tr>
  </tbody>
</table>
<h3 id="complete-the-training">Complete the Training</h3>
<ul>
<li>Perform training forward and backword propagation in batch, multiple words at a time.</li>
<li>Everytime update w&amp;b and also update embedding.</li>
<li>Trained embedding can be used in future without these training steps.</li>
<li>Let entire dataset of paired words go through this network. One it goes through it is called one epoch.</li>
<li>Let embedding get updated over multiple epoch say 50 or 100. More epoch, will cause better embedding. It will cost more money.</li>
<li>More dimentional vector will have better represenation but will cost more computation and more money.</li>
</ul>
<h2 id="other-methods-of-embedding">Other Methods of Embedding</h2>
<h3 id="tf-idf">TF-IDF</h3>
<p>TF-IDF - Term Frequency - Inverse Document Frequency, is an old, traditional, frequency based text embedding technique. It is not based on neural network architecture therefore does not need expensive hardware to create these embedding and use TF-IDF embedding. Like skipgram or CBOW it is not vector based but frequency based, therefore understandign symantic of the text is not possible with TF-IDF. There is no use of pretrained embedding, everytime we have a corpus we need to create embedding for that and it is used only for that. We cannot use TF-IDF embedding, which was created using news text for something else, say history or enterainment. Thus, embedding transfer is meaninless but task transfer can be done. It means TF-IDF embedding which is used for classficittion purpose can be used for other task like topic modelling, sentiment analysis etc. Obviously there is a limit, we cannot use it for other task like translation or summarization.</p>
<h4 id="how-tf-idf-works">How TF-IDF works?</h4>
<ul>
<li>Term frequency (TF): The number of times a word appears in a document.</li>
<li>Inverse document frequency (IDF): The logarithm of the number of documents in the collection divided by the number of documents that contain the word.</li>
<li>The TF-IDF score for a word in a document is calculated as follows:</li>
<li>TF-IDF = TF * IDF (The higher the TF-IDF score, the more important the word is to the document.)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Document_1: <span style="color:#e6db74">&#34;The quick brown fox jumps over the lazy dog.&#34;</span>
</span></span><span style="display:flex;"><span>Document_2: <span style="color:#e6db74">&#34;The dog is lazy, but the fox is quick.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;quick&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF(quick, Document_1) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;quick&#34;</span>
</span></span><span style="display:flex;"><span>IDF(quick) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;quick&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(quick, Document_1) <span style="color:#f92672">=</span> TF(quick, Document_1) <span style="color:#f92672">*</span> IDF(quick) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;quick&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF(quick, Document_2) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;quick&#34;</span>
</span></span><span style="display:flex;"><span>IDF(quick) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;quick&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(quick, Document_2) <span style="color:#f92672">=</span> TF(quick, Document_2) <span style="color:#f92672">*</span> IDF(quick) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;lazy&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF(lazy, Document_1) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;lazy&#34;</span>
</span></span><span style="display:flex;"><span>IDF(lazy) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;lazy&#34; in Document 1</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(lazy, Document_1) <span style="color:#f92672">=</span> TF(lazy, Document_1) <span style="color:#f92672">*</span> IDF(lazy) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Term frequency for the word &#34;lazy&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF(lazy, Document_2) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inverse document frequency for the word &#34;lazy&#34;</span>
</span></span><span style="display:flex;"><span>IDF(lazy) <span style="color:#f92672">=</span> log(<span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TF-IDF score for the word &#34;lazy&#34; in Document 2</span>
</span></span><span style="display:flex;"><span>TF<span style="color:#f92672">-</span>IDF(lazy, Document_2) <span style="color:#f92672">=</span> TF(lazy, Document_2) <span style="color:#f92672">*</span> IDF(lazy) <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><h3 id="glove-global-vectors">GloVe (Global Vectors)</h3>
<p>GloVe is a method that learns word embeddings from global word-word co-occurrence statistics. It is similar to Skipgram and CBOW, but it is better at capturing long-range semantic relationships between words. GloVe embedding is good for text classification, and machine translation (MT).</p>
<h4 id="how-glove-embedding-works">How GloVe embedding works?</h4>
<ul>
<li>Tokenize the corpus: Split the corpus into individual words and punctuation marks.</li>
<li>Count word co-occurrences: For each word in the vocabulary, count how many times it co-occurs with other words in a given window size.</li>
<li>Build a word-word co-occurrence matrix: The word-word co-occurrence matrix is a square matrix, where each row and column represents a word in the vocabulary. The value at each cell in the matrix represents the number of times the two corresponding words co-occur in the corpus.</li>
<li>Factorize the word-word co-occurrence matrix: Factorize the word-word co-occurrence matrix into two lower-dimensional matrices, one for <strong>word embeddings</strong> (relationship between words) and one for <strong>context embeddings</strong> (relationship between words in the context). We can factorize the word-word co-occurrence matrix using a variety of matrix factorization techniques, such as singular value decomposition (SVD) or nonnegative matrix factorization (NMF).</li>
<li>Normalize the word embeddings: Normalize the word embeddings so that they have a unit length. We can normalize the word embeddings by dividing each embedding by its L2 norm. This will ensure that all of the embeddings have a unit length.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> gensim.models <span style="color:#f92672">import</span> KeyedVectors
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load the corpus</span>
</span></span><span style="display:flex;"><span>corpus <span style="color:#f92672">=</span> open(<span style="color:#e6db74">&#34;corpus.txt&#34;</span>, <span style="color:#e6db74">&#34;r&#34;</span>)<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Tokenize the corpus</span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> corpus<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Count word co-occurrences</span>
</span></span><span style="display:flex;"><span>word_co_occurrences <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((len(tokens), len(tokens)))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> tokens[i] <span style="color:#f92672">!=</span> tokens[j]:
</span></span><span style="display:flex;"><span>            word_co_occurrences[i, j] <span style="color:#f92672">=</span> tokens<span style="color:#f92672">.</span>count(tokens[i] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> tokens[j])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Factorize the word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>glove_model <span style="color:#f92672">=</span> KeyedVectors(word_vectors<span style="color:#f92672">=</span>word_co_occurrences, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save the word embeddings</span>
</span></span><span style="display:flex;"><span>glove_model<span style="color:#f92672">.</span>save(<span style="color:#e6db74">&#34;glove_embeddings.txt&#34;</span>)
</span></span></code></pre></div><p>How SVD (Singular Value Decomposition) works?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>word_co_occurrences <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform SVD</span>
</span></span><span style="display:flex;"><span>U, S, Vh <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(word_co_occurrences)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Truncate the singular values</span>
</span></span><span style="display:flex;"><span>S_truncated <span style="color:#f92672">=</span> S[:<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reconstruct the word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>word_co_occurrences_reconstructed <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(U[:, :<span style="color:#ae81ff">2</span>], np<span style="color:#f92672">.</span>dot(S_truncated, Vh[:, :<span style="color:#ae81ff">2</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print the reconstructed word-word co-occurrence matrix</span>
</span></span><span style="display:flex;"><span>print(word_co_occurrences_reconstructed)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Results</span>
</span></span><span style="display:flex;"><span>[[<span style="color:#f92672">-</span><span style="color:#ae81ff">0.50578521</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.25523155</span>]
</span></span><span style="display:flex;"><span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">0.58437383</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.60130182</span>]
</span></span><span style="display:flex;"><span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">0.63457746</span>  <span style="color:#ae81ff">0.75716113</span>]]
</span></span><span style="display:flex;"><span>[[<span style="color:#f92672">-</span><span style="color:#ae81ff">0.50578521</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.58437383</span>]
</span></span><span style="display:flex;"><span> [ <span style="color:#ae81ff">0.25523155</span>  <span style="color:#ae81ff">0.60130182</span>]
</span></span><span style="display:flex;"><span> [ <span style="color:#ae81ff">0.82403773</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.54492509</span>]]
</span></span></code></pre></div><h3 id="bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)</h3>
<p>BERT is a transformer-based language model that can learn word embeddings from unlabeled text, we need not to create skipgram pairs. BERT embeddings are particularly good at capturing <strong>contextual information</strong>. BERT embedding is good for MT, QA, Classification tasks.</p>
<h4 id="how-bert-does-embedding">How BERT does embedding?</h4>
<ul>
<li>Tokenization: The first step is to tokenize the sentence into words. This means splitting the sentence into individual words, including punctuation marks. The tokenized sentence is then represented as a sequence of integers (we create ids), where each integer represents a word in the vocabulary.</li>
<li>Word embedding lookup: BERT uses a pre-trained word embedding table to convert each word in the sequence into a vector of numbers. This vector represents the meaning of the word in a distributed manner.</li>
<li>Segment embedding lookup: BERT also uses a segment embedding table to encode the position of each word in the sentence. This is necessary because BERT is a bidirectional language model, and it needs to know the context of each word in order to learn meaningful embeddings.</li>
<li>Positional embedding lookup: BERT also uses a positional embedding table to encode the absolute position of each word in the sentence. This is necessary because BERT needs to know the order of the words in the sentence in order to learn meaningful embeddings.</li>
<li>Transformer encoding: The encoded sequence of word embeddings, segment embeddings, and positional embeddings is then passed to the transformer encoder. The transformer encoder is a neural network architecture that learns long-range dependencies between words in a sentence.</li>
<li>Output embedding: The output of the transformer encoder is a sequence of vectors, where each vector represents the embedding of the corresponding word in the sentence. These embeddings are then used for downstream natural language processing tasks, such as machine translation, text classification, and question answering.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Tokenize the sentence</span>
</span></span><span style="display:flex;"><span>sentence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;The quick brown fox jump over the lazy fox&#34;</span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> sentence<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert each word to a word embedding vector</span>
</span></span><span style="display:flex;"><span>word_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens:
</span></span><span style="display:flex;"><span>    word_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_word_embedding(token))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create segment embeddings</span>
</span></span><span style="display:flex;"><span>segment_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&lt;</span> len(tokens) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>        segment_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_segment_embedding(<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        segment_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_segment_embedding(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create positional embeddings</span>
</span></span><span style="display:flex;"><span>positional_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(tokens)):
</span></span><span style="display:flex;"><span>    positional_embeddings<span style="color:#f92672">.</span>append(bert_model<span style="color:#f92672">.</span>get_positional_embedding(i))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Encode the sentence</span>
</span></span><span style="display:flex;"><span>encoded_sentence <span style="color:#f92672">=</span> bert_model<span style="color:#f92672">.</span>encode(word_embeddings, segment_embeddings, positional_embeddings)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output embeddings</span>
</span></span><span style="display:flex;"><span>output_embeddings <span style="color:#f92672">=</span> encoded_sentence
</span></span></code></pre></div><h3 id="fasttext-fast-text">FastText (Fast Text)</h3>
<p>FastText is a modification of Skipgram that can learn embeddings for words and subwords. This makes it better at representing rare words and out-of-vocabulary words. FastText is good for name-entity-recognition (NER) &amp; Question Answering (QA) tasks.</p>
<h3 id="elmo-embeddings-from-language-models">ELMo (Embeddings from Language Models)</h3>
<p>ELMo (Embeddings from Language Models) is a deep contextual word embedding technique that uses a bidirectional language model (biLM) to learn word representations. A biLM is a type of neural network that can learn to predict the next word in a sentence, as well as the previous word. Unlike skipgram, which predicts next words, biLM is bidirectional. From a target word biLM can predict next and previous words.</p>
<h1 id="resources">Resources</h1>
<p>If you want to understand all skipgram/cbow caluclation with excel and then you can use this <a href="https://docs.google.com/spreadsheets/d/1eU4EVtUzD1w_ILcpJVTc6oK2KH9vEDK7OuXFtyv1_gU/edit?usp=sharing">calculation sheet</a></p>
<div class="category-section">
    <h4 class="category-section__title">Categories:</h4>
    <div class="category-badges"><a href="/categories/dsblog" class="category-badge">dsblog</a></div>
  </div><div class="td-tags">
    <h4 class="td-tags__title">Tags:</h4>
    <div class="category-badges"><a href="/tags/word-embedding" class="category-badge">Word Embedding</a><a href="/tags/nlp" class="category-badge">NLP</a><a href="/tags/vector-representation" class="category-badge">Vector Representation</a><a href="/tags/text-processing" class="category-badge">Text Processing</a><a href="/tags/machine-learning" class="category-badge">Machine Learning</a><a href="/tags/neural-networks" class="category-badge">Neural Networks</a><a href="/tags/language-models" class="category-badge">Language Models</a></div>
  </div><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Hari Thapliyaal's avatar" class="author-image" >
      </div><div class="td-author-box__info">
      <h4 class="td-author-box__name">Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>

<div class="td-social-share">
  <h4 class="td-social-share__title">Share this article:</h4>
  <ul class="td-social-share__list"><div class="social-share">
        <a href="https://twitter.com/intent/tweet?text=Basics%20of%20Word%20Embedding&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f" target="_blank" rel="noopener" aria-label="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f" target="_blank" rel="noopener" aria-label="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f&title=Basics%20of%20Word%20Embedding" target="_blank" rel="noopener" aria-label="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
        <a href="https://www.reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f&title=Basics%20of%20Word%20Embedding" target="_blank" rel="noopener" aria-label="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
        <a href="mailto:?subject=Basics%20of%20Word%20Embedding&body=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fbasics-of-word-embedding%2f" aria-label="Share via Email">
          <i class="fas fa-envelope"></i>
        </a>
      </div></ul>
</div>


<div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>

<ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href="/dsblog/graph-of-thoughts/" aria-label="Previous page">
            
            <div class="td-pager__meta">
              <i class="fa-solid fa-angle-left"></i>
              <span class="td-pager__meta-label"><b>Previous:</b></span>
              <span class="td-pager__meta-title">Graph of Thoughts</span>
            </div>
          </a><a class="td-pager__link td-pager__link--next" href="/dsblog/topic-modeling-with-bert/" aria-label="Next page">
            <div class="td-pager__meta">
              <span class="td-pager__meta-label"><b>Next:</b></span>
              <span class="td-pager__meta-title">Topic Modeling with BERT</span>
              <i class="fa-solid fa-angle-right"></i>
            </div>
          </a></ul>

        </main>
        <div class="col-md-3">
          
          
            <aside class="td-sidebar-right td-sidebar--flush">
              <div class="td-sidebar__inner">
                <div class="custom-toc">
                  <h5 class="custom-toc__heading">On This Page</h5>
                  <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-context-target-and-window">What is Context, target and window?</a></li>
    <li><a href="#what-is-skipgram">What is Skipgram?</a></li>
    <li><a href="#what-is-cbow-continuous-bag-of-words">What is CBOW (Continuous Bag of Words)</a></li>
    <li><a href="#how-cbow-works">How CBOW works?</a>
      <ul>
        <li><a href="#finalize-the-corpus-step-1">Finalize the corpus (Step 1)</a></li>
        <li><a href="#create-skipgram-step-234-">Create Skipgram (Step 2+3+4 )</a></li>
        <li><a href="#create-neural-network-step-5">Create Neural Network (Step 5)</a></li>
        <li><a href="#training---forward-propagation-step-678910">Training - Forward propagation (Step 6+7+8+9+10)</a></li>
        <li><a href="#training---backward-propagation-step-1112">Training - backward propagation (Step 11+12)</a></li>
        <li><a href="#update-embedding-step-1314-">Update Embedding (Step 13+14 )</a></li>
        <li><a href="#complete-the-training">Complete the Training</a></li>
      </ul>
    </li>
    <li><a href="#other-methods-of-embedding">Other Methods of Embedding</a>
      <ul>
        <li><a href="#tf-idf">TF-IDF</a></li>
        <li><a href="#glove-global-vectors">GloVe (Global Vectors)</a></li>
        <li><a href="#bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)</a></li>
        <li><a href="#fasttext-fast-text">FastText (Fast Text)</a></li>
        <li><a href="#elmo-embeddings-from-language-models">ELMo (Embeddings from Language Models)</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </aside>
          
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      
      <div class="col-2">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="http://localhost:1313/assets/images/site-logo.png" alt="dasarpAI" width="100" style="border-radius: 12px;">
        </a>
      </div>
      <div class="col-8"><div class="row"><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Key Links</h4>
                    <ul><li><a href="/aboutme">About Me</a></li><li><a href="/dscourses">My Data Science Courses/Services</a></li><li><a href="/summary-of-al-ml-projects">MyWork by Business Domain</a></li><li><a href="/summary-of-my-technology-stacks">MyWork by Tech Stack</a></li><li><a href="/summary-of-management-projects">MyWork in Project Management</a></li><li><a href="/clients">Clients</a></li><li><a href="/testimonials">Testimonial</a></li><li><a href="/terms-of-service">Terms &amp; Condition</a></li><li><a href="/privacy">Privacy Policy</a></li><li><a href="/comment-policy">Comment Policy</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>My Blogs</h4>
                    <ul><li><a href="/dsblog">Data Science Blog</a></li><li><a href="/booksumary">Books/Interviews Blog</a></li><li><a href="/news">AI and Business News</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li><li><a href="/wiaposts">Wisdom in Awareness Blog</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/mychanting">My Chantings</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>All Resources</h4>
                    <ul><li><a href="/datascience-tags#ds-resources">DS Resources</a></li><li><a href="https://aibenchmark-explorer.dasarpai.com">AI Benchmark Explorer</a></li><li><a href="/dsblog/ds-ai-ml-books">Data Science-Books</a></li><li><a href="/dsblog/data-science-cheatsheets">Data Science/AI Cheatsheets</a></li><li><a href="/dsblog/best-youtube-channels-for-ds">Video Channels to Learn DS/AI</a></li><li><a href="/dsblog/ds-ai-ml-interview-resources">DS/AI Interview Questions</a></li><li><a href="https://github.com/dasarpai/DAI-Datasets">GitHub DAI-Datasets</a></li><li><a href="/pmi-templates">PMBOK6 Templates</a></li><li><a href="/prince2-templates">PRINCE2 Templates</a></li><li><a href="/microsoft-pm-templates">Microsoft PM Templates</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Project Management</h4>
                    <ul><li><a href="/pmlogy-home">PMLOGY Home</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmglossary">PM Glossary</a></li><li><a href="/pmlogy-tags">PM Topics</a></li><li><a href="/pmbok6-tags">PMBOK6 Topics</a></li><li><a href="/pmbok6-summary">PMBOK6</a></li><li><a href="/pmbok6">PMBOK6 Explorer</a></li><li><a href="/pmbok6hi-tags">PMBOK6 Hindi Topics</a></li><li><a href="/pmbok6hi-summary">PMBoK6 Hindi</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li></ul>
                  </div>
                </div></div>
      


      <div class="row"><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Wisdom in Awareness</h4>
                  <ul><li><a href="/wia-home">WIA Home</a></li><li><a href="/wiaposts">WIA Blog</a></li><li><a href="/wia-tags">WIA Topics</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li><li><a href="/gk-tags">GK Topic</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Samskrutyatra</h4>
                  <ul><li><a href="/samskrutyatra-home">SamskrutYatra Home</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/samskrutyatra-tags">Samskrut Topics</a></li><li><a href="/mychanting">My Vedic Chantings</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>My Gallery</h4>
                  <ul><li><a href="/gallary/slider-online-sessions1">Online AI Classes 1</a></li><li><a href="/gallary/slider-online-sessions2">Online AI Classes 2</a></li><li><a href="/gallary/slider-online-sessions3">Online AI Classes 3</a></li><li><a href="/gallary/slider-online-sessions4">Online AI Classes 4</a></li><li><a href="/gallary/slider-pm-selected-photos">Management Classes</a></li><li><a href="/gallary/slider-pm-workshops">PM &amp; DS Workshop</a></li></ul>
                </div>
              </div></div>
    </div>

    <div class="col-2">

    </div>

      
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.js"></script>
<script src='/js/prism.js'></script>
<script src='/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>
