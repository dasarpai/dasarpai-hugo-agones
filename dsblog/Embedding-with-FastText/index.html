<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Embedding with FastText | Agones</title><meta property="og:url" content="http://localhost:1313/dsblog/Embedding-with-FastText/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Embedding with FastText">
  <meta property="og:description" content="Embedding with FastText What is Embedding? What are Different Types of Embedding
What is FastText? FastText is an open-source library for efficient learning of word representations and sentence classification developed by Facebook AI Research. It is designed to handle large-scale text data and provides tools for training and using word embeddings.
FastText is an extension of the popular Word2Vec model that not only learns word embeddings but also considers subword information. It represents each word as a bag of character n-grams (subword units), which allows it to capture morphological variations and handle out-of-vocabulary words more effectively.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2023-07-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T15:25:42+05:30">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Word Embeddings">
    <meta property="article:tag" content="FastText">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Text Processing">
    <meta property="article:tag" content="Deep Learning">

  <meta itemprop="name" content="Embedding with FastText">
  <meta itemprop="description" content="Embedding with FastText What is Embedding? What are Different Types of Embedding
What is FastText? FastText is an open-source library for efficient learning of word representations and sentence classification developed by Facebook AI Research. It is designed to handle large-scale text data and provides tools for training and using word embeddings.
FastText is an extension of the popular Word2Vec model that not only learns word embeddings but also considers subword information. It represents each word as a bag of character n-grams (subword units), which allows it to capture morphological variations and handle out-of-vocabulary words more effectively.">
  <meta itemprop="datePublished" content="2023-07-15T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-08T15:25:42+05:30">
  <meta itemprop="wordCount" content="1739">
  <meta itemprop="keywords" content="FastText,Word Embeddings,Natural Language Processing,Text Analysis,Vector Representations,Word Vectors,Text Classification,Multilingual NLP">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Embedding with FastText">
  <meta name="twitter:description" content="Embedding with FastText What is Embedding? What are Different Types of Embedding
What is FastText? FastText is an open-source library for efficient learning of word representations and sentence classification developed by Facebook AI Research. It is designed to handle large-scale text data and provides tools for training and using word embeddings.
FastText is an extension of the popular Word2Vec model that not only learns word embeddings but also considers subword information. It represents each word as a bag of character n-grams (subword units), which allows it to capture morphological variations and handle out-of-vocabulary words more effectively.">



<link rel="stylesheet" href="/css/prism.css"/>

<link href="/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>


<link rel="stylesheet" href="/css/custom.css">

<script src="/js/lunr.js"></script>


    <style>
       
      .td-main img {
        max-width: 100%;
        height: auto;
      }
      .td-main {
        padding-top: 60px;  
      }
       
      .td-sidebar-right {
          padding-left: 20px;  
      }
    </style>
  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/samskrutyatra/"><span>Samskrut Yatra Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search position-relative">
  <div class="td-search__icon"></div>
  <input
    id="agones-search"
    type="search"
    class="td-search__input form-control td-search-input"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
  >
  <ul id="agones-search-results" class="list-group position-absolute w-100" style="z-index:1000; top:100%; left:0;"></ul>
</div>

<script>
let lunrIndex, pagesIndex;

async function initLunr() {
  const response = await fetch('/index.json');
  pagesIndex = await response.json();
  lunrIndex = lunr(function () {
    this.ref('url');
    this.field('title', { boost: 10 });
    this.field('content');
    pagesIndex.forEach(function (doc) {
      this.add(doc);
    }, this);
  });
}

function search(query) {
  if (!lunrIndex || !query) return [];
  return lunrIndex.search(query).map(result =>
    pagesIndex.find(page => page.url === result.ref)
  );
}

document.addEventListener('DOMContentLoaded', function () {
  initLunr();
  const input = document.getElementById('agones-search');
  const resultsList = document.getElementById('agones-search-results');
  input.addEventListener('input', function (e) {
    const query = e.target.value.trim();
    if (!query) {
      resultsList.innerHTML = '';
      resultsList.style.display = 'none';
      return;
    }
    const results = search(query);
    if (results.length === 0) {
      resultsList.innerHTML = '<li class="list-group-item">No results found.</li>';
      resultsList.style.display = 'block';
      return;
    }
    resultsList.innerHTML = results.map(page =>
      `<li class="list-group-item"><a href="${page.url}">${page.title}</a></li>`
    ).join('');
    resultsList.style.display = 'block';
  });
  
  input.addEventListener('blur', function() {
    setTimeout(() => { resultsList.style.display = 'none'; }, 200);
  });
  
  input.addEventListener('focus', function() {
    if (input.value.trim()) resultsList.style.display = 'block';
  });
});
</script></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <div class="row">
        <div class="col-md-3">
          
        </div>
        <main role="main" class="col-md-6 td-main">
          <p><img src="/assets/images/dspost/dsp6073-Embedding-with-FastText.jpg" alt="Embedding with FastText"></p>
<h1 id="embedding-with-fasttext">Embedding with FastText</h1>
<p><a href="/dsblog/what-is-nlp#what-is-embedding">What is Embedding?</a> <br>
<a href="/dsblog/what-is-nlp#what-are-different-embedding-types">What are Different Types of Embedding</a></p>
<h2 id="what-is-fasttext">What is FastText?</h2>
<p>FastText is an open-source library for efficient learning of word representations and sentence classification developed by Facebook AI Research. It is designed to handle large-scale text data and provides tools for <strong>training</strong> and <strong>using word embeddings</strong>.</p>
<p>FastText is an extension of the popular Word2Vec model that not only learns word embeddings but also <strong>considers subword</strong> information. It represents each word as a bag of character n-grams (subword units), which allows it to capture morphological variations and <strong>handle out-of-vocabulary</strong> words more effectively.</p>
<p>Key features and functionalities of FastText include:</p>
<ul>
<li>
<p>Word Embeddings: FastText can learn high-quality vector representations (embeddings) for words in a given text corpus. These embeddings capture semantic and syntactic relationships between words, enabling various downstream NLP tasks.</p>
</li>
<li>
<p>Subword Information: FastText incorporates subword information by representing words as a sum of character n-grams. This allows it to generate embeddings for words that were not present in the training data and handle morphologically rich languages effectively.</p>
</li>
<li>
<p>Efficiency: FastText is designed for scalability and efficiency, enabling training on large-scale datasets. It uses hierarchical softmax and the negative sampling techniques to accelerate training and reduce computational requirements.</p>
</li>
<li>
<p>Supervised Text Classification: FastText includes functionality for text classification tasks. It can learn <strong>text classifiers using the same word embeddings by averaging word vectors within a text</strong> and training on labeled data.</p>
</li>
<li>
<p>Pretrained Models: Pretrained FastText models are available for various languages and domains, allowing you to leverage pre-trained embeddings in your applications without the need for training from scratch.</p>
</li>
<li>
<p>The FastText library provides APIs for training models, loading pre-trained models, and performing operations with word embeddings. It also offers command-line tools for training and evaluating FastText models on custom datasets.</p>
</li>
</ul>
<h2 id="sentence-embedding-with-fasttext">Sentence Embedding with FastText</h2>
<p>FastText is primarily designed for generating word embeddings, but it can also be used to obtain sentence or document embeddings by averaging the word embeddings within a sentence or document. Here&rsquo;s how you can generate sentence or chapter embeddings using the FastText model:</p>
<ul>
<li>Preprocess the text:
<ul>
<li>Tokenize the text into sentences or chapters, depending on the granularity you want for the embeddings.</li>
<li>Clean and preprocess the sentences or chapters by removing punctuation, stop words, and performing any other necessary text normalization steps.</li>
</ul>
</li>
<li>Load the FastText model:
<ul>
<li>You can either train your own FastText model on a large corpus or download a pre-trained FastText model. Pre-trained FastText models are available for various languages on the official <a href="https://fasttext.cc/docs/en/crawl-vectors.html">FastText website</a>.</li>
<li>Generate sentence or chapter embeddings:</li>
</ul>
</li>
<li>Iterate through each sentence or chapter and tokenize it into words.
<ul>
<li>For each word, retrieve its corresponding word embedding from the FastText model.</li>
<li>Average the word embeddings to obtain a single vector representation for the sentence or chapter. This can be done by simply summing the word embeddings and dividing by the number of words.</li>
</ul>
</li>
</ul>
<p>Here&rsquo;s a Python code example using the fasttext library to generate sentence embeddings:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> fasttext
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load the pre-trained FastText model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> fasttext<span style="color:#f92672">.</span>load_model(<span style="color:#e6db74">&#39;path/to/pretrained_model.bin&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example text</span>
</span></span><span style="display:flex;"><span>text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;This is an example sentence.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Tokenize the text into sentences</span>
</span></span><span style="display:flex;"><span>sentences <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;. &#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generate sentence embeddings</span>
</span></span><span style="display:flex;"><span>sentence_embeddings <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> sentences:
</span></span><span style="display:flex;"><span>    words <span style="color:#f92672">=</span> sentence<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> [model<span style="color:#f92672">.</span>get_word_vector(word) <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> words]
</span></span><span style="display:flex;"><span>    avg_embedding <span style="color:#f92672">=</span> sum(embeddings) <span style="color:#f92672">/</span> len(embeddings)
</span></span><span style="display:flex;"><span>    sentence_embeddings<span style="color:#f92672">.</span>append(avg_embedding)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print the sentence embeddings</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> sentence, embedding <span style="color:#f92672">in</span> zip(sentences, sentence_embeddings):
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Sentence: </span><span style="color:#e6db74">{</span>sentence<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Embedding: </span><span style="color:#e6db74">{</span>embedding<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print()
</span></span></code></pre></div><p>The quality of sentence or chapter embeddings obtained by averaging word embeddings may not capture the full context or meaning of the text. There are more advanced techniques and models, such as transformers like BERT or Universal Sentence Encoder (USE), that are specifically designed for generating high-quality sentence or document embeddings.</p>
<h2 id="fasttext-vs-other-embedding-model">FastText vs Other Embedding Model</h2>
<p>FastText and BERT have different strengths and are suitable for different business scenarios. Here are some scenarios where FastText might be a better choice than BERT:</p>
<ul>
<li>Efficiency and Speed: FastText is known for its efficiency and faster training times compared to BERT. If you have limited computational resources or a large-scale dataset, FastText can be a more practical option due to its faster training and inference times.</li>
<li>Resource Constraints: BERT models are larger and require more memory compared to FastText models. If you have resource-constrained environments, such as mobile applications or embedded systems, FastText&rsquo;s smaller model size and lower memory requirements make it a better fit.</li>
<li>Morphologically Rich Languages: FastText performs well in languages with complex morphology, where words can have multiple variations based on affixes, inflections, or conjugations. By considering subword information, FastText can capture morphological variations effectively. BERT, on the other hand, might struggle with out-of-vocabulary words and morphologically rich languages unless it has been specifically fine-tuned for those languages.</li>
<li>Word-level Embeddings: If your application primarily relies on word-level embeddings, FastText can be more suitable. FastText provides efficient and effective word embeddings, allowing you to capture semantic relationships and similarities between words.</li>
<li>Sentence Classification: FastText has built-in functionality for sentence classification tasks. If your business scenario involves tasks such as sentiment analysis, spam detection, or topic classification, FastText&rsquo;s ability to generate sentence-level embeddings and perform classification tasks efficiently can be advantageous.</li>
</ul>
<p>It&rsquo;s important to note that BERT, with its deep contextual understanding, excels in tasks that require fine-grained understanding of language, such as question answering, natural language understanding, and text generation. BERT&rsquo;s ability to capture context and long-range dependencies is particularly useful in complex language tasks.</p>
<p>Ultimately, the choice between FastText and BERT depends on your specific requirements, available resources, and the nature of the language and tasks involved in your business scenario.</p>
<h2 id="is-there-any-better-way-of-doing-sentence-embedding">Is there any better way of doing sentence embedding?</h2>
<p>There are several advanced techniques for generating sentence embeddings that often outperform simple averaging of word embeddings. Here are a few popular approaches:</p>
<h3 id="universal-sentence-encoder-use">Universal Sentence Encoder (USE):</h3>
<p>The Universal Sentence Encoder, developed by Google, is a widely used model for generating sentence embeddings. It is based on a transformer architecture and is trained on a large amount of data for various NLP tasks.
You can use the pre-trained models available in the <a href="https://tfhub.dev/google/universal-sentence-encoder">TensorFlow Hub</a> and easily generate high-quality sentence embeddings using the encoder.</p>
<h3 id="bert-based-models">BERT-based models:</h3>
<p>Models based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, such as Sentence-BERT, can also generate effective sentence embeddings.</p>
<p>These models are typically trained on large-scale language modeling or natural language understanding tasks and have achieved state-of-the-art performance in many NLP benchmarks.</p>
<h3 id="infersent">Infersent:</h3>
<p>Infersent is a sentence embeddings method introduced by Facebook Research. It utilizes a BiLSTM-based architecture and is trained on a large-scale supervised dataset.
Infersent is particularly effective for capturing semantic meaning and can generate informative sentence embeddings.</p>
<h3 id="doc2vec">Doc2Vec:</h3>
<p>Doc2Vec, an extension of Word2Vec, is a model that can generate document-level embeddings. Doc2Vec considers the context of words within a document and learns fixed-length vector representations for the entire document.</p>
<p>By using Doc2Vec, you can obtain embeddings at the document or chapter level.
These methods generally provide better sentence embeddings compared to simple averaging of word embeddings because they take into account the context and meaning of the entire sentence. They are often pre-trained on large corpora and fine-tuned for specific downstream tasks. The choice of method depends on your specific requirements and the available resources, such as pre-trained models and computational capabilities.</p>
<h2 id="what-is-the-best-embedding-for-indian-english">What is the Best Embedding for Indian English?</h2>
<p>You may have heard terms like Indian English, British English, US English, Australian English etc. We know all English are written in Roman script. If so, then what is the difference? Any language&rsquo;s grammer involves noun. Noun includes name of people, placess, food, relationship name, festivals, books etc. Suppose you pickup Mahabharat or Bagwat Purana and do the translation in English and write that in Roman script. The noun in this text going to be complete different than a book coming from Europe. If you are working on some project which has Roman Script and English language you need to understand what kind of corpus you have. If you want to use existing embedding which was not built using Indian English then you are going to face OOV (out of vocabulary) problem. In that situation what to do?</p>
<p>For example, question answering tasks on a corpus of ancient Indian literature with Sanskrit words, I would recommend using a multilingual language model like mBERT (multilingual BERT) or XLM-R (Cross-lingual Language Model - RoBERTa). These models are trained on a wide range of languages, including English and Sanskrit, and have the ability to capture the context and meaning of text in multiple languages.</p>
<p>XLM-R (XLM-RoBERTa, Unsupervised Cross-lingual Representation Learning at Scale) is a scaled cross lingual sentence encoder. It is trained on 2.5TB (10^12) of data across 100 languages data filtered from Common Crawl. XLM-R achieves state-of-the-arts results on multiple cross lingual benchmarks.</p>
<p>mBERT and XLM-R have been shown to be effective in cross-lingual tasks and can handle Sanskrit words, which may not be well-covered in models trained solely on English data. They can generate word embeddings that capture the semantic meaning of the text and are suitable for tasks like question answering.</p>
<p>To use these models, you can either fine-tune them on your specific question answering task using a labeled dataset or use them as feature extractors to obtain embeddings for your question and text pairs. You can then compute similarity scores between the question and passages to identify the most relevant answers.</p>
<p>Here&rsquo;s a general outline of the steps:</p>
<ul>
<li>Preprocess the corpus:
<ul>
<li>Clean and preprocess the text, removing any unwanted characters or symbols.</li>
<li>Tokenize the text into sentences or paragraphs.</li>
</ul>
</li>
<li>Load the multilingual language model:
<ul>
<li>You can use the <a href="https://huggingface.co/transformers/">Hugging Face Transformers library</a> to load and work with mBERT or XLM-R.</li>
</ul>
</li>
<li>Generate word or sentence embeddings:
<ul>
<li>For each sentence or paragraph, pass it through the model to obtain word or sentence embeddings.</li>
<li>You can choose to use the final hidden states or pool them in a specific way, depending on your requirements.</li>
</ul>
</li>
<li>Question processing:
<ul>
<li>Preprocess and tokenize the questions in a similar way to the text.</li>
</ul>
</li>
<li>Compute similarity:
<ul>
<li>Compute the similarity scores between the question embeddings and the embeddings of the text passages.</li>
<li>You can use various similarity metrics such as cosine similarity or dot product.</li>
</ul>
</li>
<li>Select the most relevant answer:
<ul>
<li>Based on the similarity scores, select the passage or sentence with the highest score as the most relevant answer to the question.</li>
</ul>
</li>
</ul>
<p>Using a multilingual language model like mBERT or XLM-R will help you leverage the knowledge encoded in the embeddings, considering both English and Sanskrit words present in your corpus.</p>
<div class="category-section">
    <h4 class="category-section__title">Categories:</h4>
    <div class="category-badges"><a href="/categories/dsblog" class="category-badge">dsblog</a></div>
  </div><div class="td-tags">
    <h4 class="td-tags__title">Tags:</h4>
    <div class="category-badges"><a href="/tags/nlp" class="category-badge">NLP</a><a href="/tags/word-embeddings" class="category-badge">Word Embeddings</a><a href="/tags/fasttext" class="category-badge">FastText</a><a href="/tags/machine-learning" class="category-badge">Machine Learning</a><a href="/tags/text-processing" class="category-badge">Text Processing</a><a href="/tags/deep-learning" class="category-badge">Deep Learning</a></div>
  </div><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Hari Thapliyaal's avatar" class="author-image" >
      </div><div class="td-author-box__info">
      <h4 class="td-author-box__name">Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>

<div class="td-social-share">
  <h4 class="td-social-share__title">Share this article:</h4>
  <ul class="td-social-share__list"><div class="social-share">
        <a href="https://twitter.com/intent/tweet?text=Embedding%20with%20FastText&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fEmbedding-with-FastText%2f" target="_blank" rel="noopener" aria-label="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fEmbedding-with-FastText%2f" target="_blank" rel="noopener" aria-label="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fEmbedding-with-FastText%2f&title=Embedding%20with%20FastText" target="_blank" rel="noopener" aria-label="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
        <a href="https://www.reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fEmbedding-with-FastText%2f&title=Embedding%20with%20FastText" target="_blank" rel="noopener" aria-label="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
        <a href="mailto:?subject=Embedding%20with%20FastText&body=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fEmbedding-with-FastText%2f" aria-label="Share via Email">
          <i class="fas fa-envelope"></i>
        </a>
      </div></ul>
</div>


<div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>

<ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href="/dsblog/Major-LLM-Developers-Reshaping-NLP-Advancements/" aria-label="Previous page">
            
            <div class="td-pager__meta">
              <i class="fa-solid fa-angle-left"></i>
              <span class="td-pager__meta-label"><b>Previous:</b></span>
              <span class="td-pager__meta-title">Major LLM Developers Shaping the AI Landscape</span>
            </div>
          </a><a class="td-pager__link td-pager__link--next" href="/dsblog/AWS-SageMaker-Jumpstart-Models/" aria-label="Next page">
            <div class="td-pager__meta">
              <span class="td-pager__meta-label"><b>Next:</b></span>
              <span class="td-pager__meta-title">AWS SageMaker Jumpstart Models</span>
              <i class="fa-solid fa-angle-right"></i>
            </div>
          </a></ul>

        </main>
        <div class="col-md-3">
          
          
            <aside class="td-sidebar-right td-sidebar--flush">
              <div class="td-sidebar__inner">
                <div class="custom-toc">
                  <h5 class="custom-toc__heading">On This Page</h5>
                  <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-fasttext">What is FastText?</a></li>
    <li><a href="#sentence-embedding-with-fasttext">Sentence Embedding with FastText</a></li>
    <li><a href="#fasttext-vs-other-embedding-model">FastText vs Other Embedding Model</a></li>
    <li><a href="#is-there-any-better-way-of-doing-sentence-embedding">Is there any better way of doing sentence embedding?</a>
      <ul>
        <li><a href="#universal-sentence-encoder-use">Universal Sentence Encoder (USE):</a></li>
        <li><a href="#bert-based-models">BERT-based models:</a></li>
        <li><a href="#infersent">Infersent:</a></li>
        <li><a href="#doc2vec">Doc2Vec:</a></li>
      </ul>
    </li>
    <li><a href="#what-is-the-best-embedding-for-indian-english">What is the Best Embedding for Indian English?</a></li>
  </ul>
</nav>
                </div>
              </div>
            </aside>
          
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      
      <div class="col-2">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="http://localhost:1313/assets/images/site-logo.png" alt="dasarpAI" width="100" style="border-radius: 12px;">
        </a>
      </div>
      <div class="col-8"><div class="row"><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Key Links</h4>
                    <ul><li><a href="/aboutme">About Me</a></li><li><a href="/dscourses">My Data Science Courses/Services</a></li><li><a href="/summary-of-al-ml-projects">MyWork by Business Domain</a></li><li><a href="/summary-of-my-technology-stacks">MyWork by Tech Stack</a></li><li><a href="/summary-of-management-projects">MyWork in Project Management</a></li><li><a href="/clients">Clients</a></li><li><a href="/testimonials">Testimonial</a></li><li><a href="/terms-of-service">Terms &amp; Condition</a></li><li><a href="/privacy">Privacy Policy</a></li><li><a href="/comment-policy">Comment Policy</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>My Blogs</h4>
                    <ul><li><a href="/dsblog">Data Science Blog</a></li><li><a href="/booksumary">Books/Interviews Blog</a></li><li><a href="/news">AI and Business News</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li><li><a href="/wiaposts">Wisdom in Awareness Blog</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/mychanting">My Chantings</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>All Resources</h4>
                    <ul><li><a href="/datascience-tags#ds-resources">DS Resources</a></li><li><a href="https://aibenchmark-explorer.dasarpai.com">AI Benchmark Explorer</a></li><li><a href="/dsblog/ds-ai-ml-books">Data Science-Books</a></li><li><a href="/dsblog/data-science-cheatsheets">Data Science/AI Cheatsheets</a></li><li><a href="/dsblog/best-youtube-channels-for-ds">Video Channels to Learn DS/AI</a></li><li><a href="/dsblog/ds-ai-ml-interview-resources">DS/AI Interview Questions</a></li><li><a href="https://github.com/dasarpai/DAI-Datasets">GitHub DAI-Datasets</a></li><li><a href="/pmi-templates">PMBOK6 Templates</a></li><li><a href="/prince2-templates">PRINCE2 Templates</a></li><li><a href="/microsoft-pm-templates">Microsoft PM Templates</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Project Management</h4>
                    <ul><li><a href="/pmlogy-home">PMLOGY Home</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmglossary">PM Glossary</a></li><li><a href="/pmlogy-tags">PM Topics</a></li><li><a href="/pmbok6-tags">PMBOK6 Topics</a></li><li><a href="/pmbok6-summary">PMBOK6</a></li><li><a href="/pmbok6">PMBOK6 Explorer</a></li><li><a href="/pmbok6hi-tags">PMBOK6 Hindi Topics</a></li><li><a href="/pmbok6hi-summary">PMBoK6 Hindi</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li></ul>
                  </div>
                </div></div>
      


      <div class="row"><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Wisdom in Awareness</h4>
                  <ul><li><a href="/wia-home">WIA Home</a></li><li><a href="/wiaposts">WIA Blog</a></li><li><a href="/wia-tags">WIA Topics</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li><li><a href="/gk-tags">GK Topic</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Samskrutyatra</h4>
                  <ul><li><a href="/samskrutyatra-home">SamskrutYatra Home</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/samskrutyatra-tags">Samskrut Topics</a></li><li><a href="/mychanting">My Vedic Chantings</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>My Gallery</h4>
                  <ul><li><a href="/gallary/slider-online-sessions1">Online AI Classes 1</a></li><li><a href="/gallary/slider-online-sessions2">Online AI Classes 2</a></li><li><a href="/gallary/slider-online-sessions3">Online AI Classes 3</a></li><li><a href="/gallary/slider-online-sessions4">Online AI Classes 4</a></li><li><a href="/gallary/slider-pm-selected-photos">Management Classes</a></li><li><a href="/gallary/slider-pm-workshops">PM &amp; DS Workshop</a></li></ul>
                </div>
              </div></div>
    </div>

    <div class="col-2">

    </div>

      
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.js"></script>
<script src='/js/prism.js'></script>
<script src='/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>
