<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Understanding Contextual Embedding in Transformers | Agones</title><meta property="og:url" content="http://localhost:1313/dsblog/understanding-contextual-embedding-in-transformers/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Understanding Contextual Embedding in Transformers">
  <meta property="og:description" content="Introduction Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.
What is Embedding? What is Fixed Embedding? How Transformers Handle Context How this token ‘bank’ and corresponding embedding is stored in embedding database? How contextural embedding is generated? What will be the output size of attention formula softmax? What is meaning of a LLM has context length of 2 million tokens? How many attention layers we keep in transformer like gpt4? What is the meaning of 96 attention layers, are they attention head count? What is Embedding? An embedding is a way to represent discrete data (like words or tokens) as continuous vectors of numbers.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2025-01-30T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T15:25:42+05:30">
    <meta property="article:tag" content="Transformers">
    <meta property="article:tag" content="Embeddings">
    <meta property="article:tag" content="Deep Learning">

  <meta itemprop="name" content="Understanding Contextual Embedding in Transformers">
  <meta itemprop="description" content="Introduction Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.
What is Embedding? What is Fixed Embedding? How Transformers Handle Context How this token ‘bank’ and corresponding embedding is stored in embedding database? How contextural embedding is generated? What will be the output size of attention formula softmax? What is meaning of a LLM has context length of 2 million tokens? How many attention layers we keep in transformer like gpt4? What is the meaning of 96 attention layers, are they attention head count? What is Embedding? An embedding is a way to represent discrete data (like words or tokens) as continuous vectors of numbers.">
  <meta itemprop="datePublished" content="2025-01-30T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-08T15:25:42+05:30">
  <meta itemprop="wordCount" content="1911">
  <meta itemprop="keywords" content="Contextual Embedding in Transformers,How Transformers Handle Context,What is Fixed Embedding,How Contextural Embedding is Generated,What will be the output size of attention formula softmax,What is meaning of a LLM has context length of 2 million tokens,How many attention layers we keep in transformer like gpt4">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Understanding Contextual Embedding in Transformers">
  <meta name="twitter:description" content="Introduction Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.
What is Embedding? What is Fixed Embedding? How Transformers Handle Context How this token ‘bank’ and corresponding embedding is stored in embedding database? How contextural embedding is generated? What will be the output size of attention formula softmax? What is meaning of a LLM has context length of 2 million tokens? How many attention layers we keep in transformer like gpt4? What is the meaning of 96 attention layers, are they attention head count? What is Embedding? An embedding is a way to represent discrete data (like words or tokens) as continuous vectors of numbers.">



<link rel="stylesheet" href="/css/prism.css"/>

<link href="/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>


<link rel="stylesheet" href="/css/custom.css">

<script src="/js/lunr.js"></script>


    <style>
       
      .td-main img {
        max-width: 100%;
        height: auto;
      }
      .td-main {
        padding-top: 60px;  
      }
       
      .td-sidebar-right {
          padding-left: 20px;  
      }
    </style>
  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/samskrutyatra/"><span>Samskrut Yatra Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search position-relative">
  <div class="td-search__icon"></div>
  <input
    id="agones-search"
    type="search"
    class="td-search__input form-control td-search-input"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
  >
  <ul id="agones-search-results" class="list-group position-absolute w-100" style="z-index:1000; top:100%; left:0;"></ul>
</div>

<script>
let lunrIndex, pagesIndex;

async function initLunr() {
  const response = await fetch('/index.json');
  pagesIndex = await response.json();
  lunrIndex = lunr(function () {
    this.ref('url');
    this.field('title', { boost: 10 });
    this.field('content');
    pagesIndex.forEach(function (doc) {
      this.add(doc);
    }, this);
  });
}

function search(query) {
  if (!lunrIndex || !query) return [];
  return lunrIndex.search(query).map(result =>
    pagesIndex.find(page => page.url === result.ref)
  );
}

document.addEventListener('DOMContentLoaded', function () {
  initLunr();
  const input = document.getElementById('agones-search');
  const resultsList = document.getElementById('agones-search-results');
  input.addEventListener('input', function (e) {
    const query = e.target.value.trim();
    if (!query) {
      resultsList.innerHTML = '';
      resultsList.style.display = 'none';
      return;
    }
    const results = search(query);
    if (results.length === 0) {
      resultsList.innerHTML = '<li class="list-group-item">No results found.</li>';
      resultsList.style.display = 'block';
      return;
    }
    resultsList.innerHTML = results.map(page =>
      `<li class="list-group-item"><a href="${page.url}">${page.title}</a></li>`
    ).join('');
    resultsList.style.display = 'block';
  });
  
  input.addEventListener('blur', function() {
    setTimeout(() => { resultsList.style.display = 'none'; }, 200);
  });
  
  input.addEventListener('focus', function() {
    if (input.value.trim()) resultsList.style.display = 'block';
  });
});
</script></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <div class="row">
        <div class="col-md-3">
          
        </div>
        <main role="main" class="col-md-6 td-main">
          <p><img src="/assets/images/dspost/dsp6214-Understanding-Contextual-Embedding-in-Transformer.jpg" alt="Understanding Contextual Embedding in Transformers"></p>
<h2 id="introduction">Introduction</h2>
<p>Embedding can be confusing for many people, and contextual embedding performed by transformers can be even more perplexing. Even after gaining an understanding, many questions remain. In this article, we aim to address the following questions.</p>
<ul>
<li>What is Embedding?</li>
<li>What is Fixed Embedding?</li>
<li>How Transformers Handle Context</li>
<li>How this token &lsquo;bank&rsquo; and corresponding embedding is stored in embedding database?</li>
<li>How contextural embedding is generated?</li>
<li>What will be the output size of attention formula softmax?</li>
<li>What is meaning of a LLM has context length of 2 million tokens?</li>
<li>How many attention layers we keep in transformer like gpt4?</li>
<li>What is the meaning of 96 attention layers, are they attention head count?</li>
</ul>
<h2 id="what-is-embedding">What is Embedding?</h2>
<p>An embedding is a way to represent discrete data (like words or tokens) as continuous vectors of numbers.</p>
<p>for example</p>
<pre tabindex="0"><code>&#34;cat&#34; → [0.2, -0.5, 0.1, 0.8, ...]  # e.g., 100 dimensions vector
&#34;dog&#34; → [0.3, -0.4, 0.2, 0.7, ...]
</code></pre><p>Each dimension potentially represents some feature, they may be Masculinity/femininity, Animate/inanimate, Abstract/concrete etc.</p>
<p>Embedding helps</p>
<ul>
<li>convert discrete symbols into a numbers which can be processed by neural networks.</li>
<li>These numbers can also capture the relationships between words and in sementic operations like Queen = King - Man + Woman.</li>
<li>Reduce dimensionality (compared to one-hot encoding)</li>
</ul>
<h2 id="what-is-fixed-embedding">What is Fixed Embedding?</h2>
<p>A word &ldquo;bank&rdquo; can have multiple meaning linking to finance, dependence or river. In LLM when we do the tokenization in all case the token for this word will be same. But, what about embedding when &lsquo;bank&rsquo; word appears in different contexts</p>
<p><strong>Word Embeddings vs. Contextual Embeddings</strong></p>
<p>In traditional word embeddings (like Word2Vec or GloVe):</p>
<ul>
<li>Each word has a single, static embedding vector</li>
<li>&ldquo;bank&rdquo; would have the same embedding regardless of context</li>
<li>This is a limitation, as it can&rsquo;t distinguish between financial bank vs. river bank</li>
</ul>
<p>In contextual embedding (transormer models like BERT, GPT):</p>
<ul>
<li>Words get contextual embeddings that change based on surrounding text</li>
<li>&ldquo;bank&rdquo; gets different embedding representations depending on its usage</li>
<li>The model learns to create distinct representations for different meanings</li>
</ul>
<h2 id="how-transformers-handle-context">How Transformers Handle Context</h2>
<p>Let&rsquo;s look at examples:</p>
<pre tabindex="0"><code>&#34;I went to the bank to deposit money&#34;
&#34;The river bank was muddy&#34;
</code></pre><p>In these sentences:</p>
<ul>
<li>The initial token embeddings are combined with positional encodings</li>
<li>Each self-attention layer considers the relationships between &ldquo;bank&rdquo; and other words</li>
<li>Words like &ldquo;deposit,&rdquo; &ldquo;money,&rdquo; &ldquo;river,&rdquo; and &ldquo;muddy&rdquo; influence how &ldquo;bank&rdquo; is represented</li>
<li>The resulting contextual embeddings for &ldquo;bank&rdquo; will be different in each case</li>
</ul>
<p>Step 1. Initial Embedding:</p>
<ul>
<li>The word &ldquo;bank&rdquo; is first tokenized</li>
<li>It gets a base embedding from the embedding layer (typically there are different models for this work, these models are called embedding models)</li>
</ul>
<p>Step 2. Contextual Processing:</p>
<ul>
<li>Self-attention mechanisms look at surrounding words</li>
<li>Each attention head can focus on different aspects of meaning</li>
<li>Multiple transformer layers progressively refine the representation</li>
</ul>
<p>Step 3. Final Representation:</p>
<ul>
<li>The final embedding captures the specific meaning in that context</li>
<li>The financial &ldquo;bank&rdquo; embedding will be closer to other financial terms</li>
<li>The geographical &ldquo;bank&rdquo; embedding will be closer to other geographical terms</li>
</ul>
<p>Real-world Example</p>
<p>Consider these vectors (simplified for illustration):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Hypothetical embedding dimensions</span>
</span></span><span style="display:flex;"><span>bank <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#f92672">...</span>]  <span style="color:#75715e"># if this is financal bank then it will be close to the words like &#34;money&#34;, &#34;deposit&#34;</span>
</span></span><span style="display:flex;"><span>bank <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.9</span>, <span style="color:#f92672">...</span>]  <span style="color:#75715e"># if this is related to river then it will be close to words like &#34;water&#34;, swimming,&#34;river&#34;, &#34;shore&#34;</span>
</span></span></code></pre></div><p>The transformer model automatically generates these different representations based on context, allowing it to:</p>
<ul>
<li>Understand the appropriate meaning</li>
<li>Make relevant predictions</li>
<li>Handle ambiguity effectively</li>
</ul>
<p>This is why transformers are so powerful at handling polysemy - they don&rsquo;t just look up static word meanings but dynamically construct meanings based on context, much like humans do.</p>
<h2 id="how-this-token-bank-and-corresponding-embedding-is-stored-in-embedding-database">How this token &lsquo;bank&rsquo; and corresponding embedding is stored in embedding database?</h2>
<p><strong>1. Token Storage (Vocabulary)</strong></p>
<ul>
<li>The tokenizer maintains a fixed vocabulary mapping</li>
<li>&ldquo;bank&rdquo; as a token is stored in a vocabulary dictionary/lookup table</li>
<li>Each token has a unique integer ID</li>
<li>Example vocabulary entry:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>vocab <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;bank&#34;</span>: <span style="color:#ae81ff">2847</span>,  <span style="color:#75715e"># unique ID</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;bank&#34;</span>: <span style="color:#e6db74">&#34;▁bank&#34;</span>,  <span style="color:#75715e"># actual token (might include special chars for word boundaries. Plus actual token need not be a complete word, for example you will not time one token for a word &#34;simultaneously&#34;)</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><strong>2. Embedding Storage:</strong></p>
<ul>
<li>The embedding layer is implemented as a matrix/lookup table</li>
<li>Dimensions: (vocab_size × embedding_dim)</li>
<li>Each row corresponds to a token&rsquo;s base embedding vector</li>
<li>Example structure:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>embedding_matrix <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>FloatTensor(vocab_size, embedding_dim)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># For token &#34;bank&#34; with ID 2847:</span>
</span></span><span style="display:flex;"><span>base_embedding <span style="color:#f92672">=</span> embedding_matrix[<span style="color:#ae81ff">2847</span>]  <span style="color:#75715e"># Gets base embedding vector</span>
</span></span></code></pre></div><p>Key Points:</p>
<ul>
<li>There is only ONE base embedding vector per token</li>
<li>The contextual embeddings are generated on-the-fly during processing</li>
<li>The model doesn&rsquo;t store different embeddings for different meanings</li>
<li>The context-specific meanings emerge from the transformer layers</li>
</ul>
<p><strong>3. What&rsquo;s Actually Stored:</strong></p>
<pre tabindex="0"><code>Token Storage:
&#34;bank&#34; -&gt; 2847 (ID)

Embedding Matrix:
Row 2847: [0.1, 0.3, -0.2, ...] (base embedding vector)
</code></pre><p><strong>4. During Processing:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># When processing &#34;financial bank&#34;:</span>
</span></span><span style="display:flex;"><span>input_ids <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;I went to the bank to deposit money&#34;</span>)
</span></span><span style="display:flex;"><span>base_embeddings <span style="color:#f92672">=</span> embedding_matrix[input_ids]  <span style="color:#75715e"># Look up base embeddings</span>
</span></span><span style="display:flex;"><span>contextual_embeddings <span style="color:#f92672">=</span> transformer_layers(base_embeddings)  <span style="color:#75715e"># Generate context-specific embeddings</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># When processing &#34;river bank&#34;:</span>
</span></span><span style="display:flex;"><span>input_ids <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;The river bank was muddy&#34;</span>)
</span></span><span style="display:flex;"><span>base_embeddings <span style="color:#f92672">=</span> embedding_matrix[input_ids]  <span style="color:#75715e"># Same base embeddings</span>
</span></span><span style="display:flex;"><span>contextual_embeddings <span style="color:#f92672">=</span> transformer_layers(base_embeddings)  <span style="color:#75715e"># Different context-specific embeddings</span>
</span></span></code></pre></div><p>The different meanings of &ldquo;bank&rdquo; emerge from:</p>
<ul>
<li>The transformer&rsquo;s attention mechanisms</li>
<li>Layer-by-layer contextual processing</li>
<li>Interaction with surrounding tokens</li>
</ul>
<p>Important Note:</p>
<ul>
<li>The model doesn&rsquo;t explicitly store different embeddings for different meanings</li>
<li>It learns to transform the base embedding based on context</li>
<li>This makes the system more efficient and flexible</li>
<li>The meaning disambiguation happens dynamically during processing</li>
</ul>
<p>Token &ldquo;bank&rdquo; → Base Embedding → Transformer Layers → Contextual Embedding
↑
(considers surrounding context)</p>
<p><strong>5. Post Processing</strong></p>
<ul>
<li>The contextual embeddings are used temporarily for the current task</li>
<li>They exist only during processing (encoding/decoding)</li>
<li>After the task is complete, only the results are kept, not the intermediate contextual embeddings</li>
</ul>
<h2 id="how-contextural-embedding-is-generated">How contextural embedding is generated?</h2>
<p>To generate that contextual embedding we take help of that formula of Query (Q), Key (K), and Value (V) given in &ldquo;Attention is all you need&rdquo; paper?</p>
<p><strong>1. For each token&rsquo;s base embedding, the model creates:</strong></p>
<pre tabindex="0"><code>Q = W_q × base_embedding  (Query)
K = W_k × base_embedding  (Key)
V = W_v × base_embedding  (Value)
</code></pre><p><strong>2. The attention formula then is:</strong></p>
<pre tabindex="0"><code>Attention(Q, K, V) = softmax(QK^T/√d_k)V
</code></pre><p>Where:</p>
<ul>
<li>QK^T computes compatibility scores between tokens</li>
<li>√d_k is the scaling factor to prevent vanishing gradients</li>
<li>softmax creates attention weights</li>
<li>Final multiplication with V produces the contextual representation</li>
</ul>
<p><strong>3. Example for &ldquo;bank&rdquo;:</strong></p>
<ul>
<li>
<p>When processing &ldquo;bank&rdquo; in &ldquo;river bank&rdquo;:</p>
<ul>
<li>Q for &ldquo;bank&rdquo; will attend more strongly to &ldquo;river&rdquo;</li>
<li>The resulting contextual embedding shifts toward geographical meaning</li>
</ul>
</li>
<li>
<p>When processing &ldquo;bank&rdquo; in &ldquo;deposit money at the bank&rdquo;:</p>
<ul>
<li>Q for &ldquo;bank&rdquo; will attend more strongly to &ldquo;deposit&rdquo;, &ldquo;money&rdquo;</li>
<li>The resulting contextual embedding shifts toward financial meaning</li>
</ul>
</li>
</ul>
<p><strong>4. Multi-head attention:</strong></p>
<ul>
<li>Multiple sets of Q, K, V transformations</li>
<li>Each head can focus on different aspects of context</li>
<li>Results are concatenated and linearly transformed</li>
</ul>
<p>This mechanism allows the model to dynamically weigh different aspects of context when creating the contextual embeddings for each token.</p>
<h2 id="what-will-be-the-output-size-of-attention-formula-softmax">What will be the output size of attention formula softmax?</h2>
<p>If d_k=1024 and based embedding is 1024 then in that what will be the output size of attention formula softmax?</p>
<ol>
<li>Initial dimensions:</li>
</ol>
<ul>
<li>Base embedding dimension = 1024</li>
<li>d_k = 1024</li>
<li>Let&rsquo;s say we have a sequence length of n tokens</li>
</ul>
<ol start="2">
<li>Creating Q, K, V matrices:</li>
</ol>
<ul>
<li>Q: (n × d_k) = (n × 1024)</li>
<li>K: (n × d_k) = (n × 1024)</li>
<li>V: (n × d_k) = (n × 1024)</li>
</ul>
<ol start="3">
<li>In the attention formula:</li>
</ol>
<pre tabindex="0"><code>QK^T/√d_k
</code></pre><ul>
<li>Q: (n × 1024)</li>
<li>K^T: (1024 × n)</li>
<li>QK^T results in: (n × n) matrix</li>
<li>This represents attention scores between every token pair. There are n token and it is nxn correlation matrix.</li>
</ul>
<ol start="4">
<li>After softmax:</li>
</ol>
<ul>
<li>Input: (n × n) matrix</li>
<li>Output: (n × n) matrix of attention weights</li>
<li>Each row sums to 1</li>
</ul>
<ol start="5">
<li>Final multiplication with V:</li>
</ol>
<pre tabindex="0"><code>softmax(QK^T/√d_k)V
</code></pre><ul>
<li>Attention weights: (n × n)</li>
<li>V: (n × 1024)</li>
<li>Result: (n × 1024)</li>
</ul>
<p>So the softmax actually operates on an (n × n) attention matrix. The 1024 dimension (d_k) is used for the intermediate calculation but doesn&rsquo;t determine the final attention matrix size. The attention matrix size depends on the sequence length n (how many tokens are in your input).</p>
<p>For example, if you&rsquo;re processing 5 tokens:</p>
<ul>
<li>Attention matrix would be 5×5</li>
<li>Each token can attend to all other tokens</li>
<li>Final output preserves the sequence length and embedding dimension: 5×1024</li>
</ul>
<h2 id="what-is-meaning-of-a-llm-has-context-length-of-2-million-tokens">What is meaning of a LLM has context length of 2 million tokens?</h2>
<p>In the case of LLM like gemini where they say context length is 2 million tokens, in that case this n is 2 millions?</p>
<p>Yes, theoretically in that case n would be 2 million, which reveals one of the biggest challenges in transformer architecture - the quadratic complexity problem.</p>
<p>Let&rsquo;s break down what this means:</p>
<ol>
<li>Memory Requirements:</li>
</ol>
<ul>
<li>For 2M tokens, attention matrix is 2M × 2M</li>
<li>That&rsquo;s 4 trillion (4 × 10^12) elements</li>
<li>Even with float16 precision, this needs ~8TB of memory just for one attention layer</li>
</ul>
<ol start="2">
<li>Computational Complexity:</li>
</ol>
<ul>
<li>Computing QK^T is O(n²)</li>
<li>For n = 2M, that&rsquo;s 4 trillion operations per attention layer</li>
<li>Multiple this by number of heads and layers</li>
</ul>
<p>This is why models like Gemini likely use optimization techniques to handle such long contexts:</p>
<ol>
<li>Sparse Attention:</li>
</ol>
<ul>
<li>Not all tokens attend to all other tokens</li>
<li>Use patterns like local attention, sliding windows</li>
<li>Reduces from O(n²) to O(n log n) or even O(n)</li>
</ul>
<ol start="2">
<li>Memory-efficient techniques:</li>
</ol>
<ul>
<li>Flash Attention</li>
<li>Sparse or linear transformers</li>
<li>State space models</li>
<li>Rotary position embeddings</li>
</ul>
<ol start="3">
<li>Attention Patterns like:</li>
</ol>
<ul>
<li>Sliding window attention</li>
<li>Dilated attention</li>
<li>Hierarchical attention</li>
</ul>
<p>Without these optimizations, the original transformer attention mechanism would be computationally infeasible for such long contexts.</p>
<h2 id="how-many-attention-layers-we-keep-in-transformer-like-gpt4">How many attention layers we keep in transformer like gpt4?</h2>
<p>The exact architecture details of GPT-4 aren&rsquo;t public, but we can discuss what&rsquo;s known about transformer architectures in general and earlier models:</p>
<ol>
<li>Typical Structure:</li>
</ol>
<ul>
<li>Multiple transformer blocks/layers (anywhere from 12 to hundreds)</li>
<li>Each block contains:
<ul>
<li>Multi-head attention layer</li>
<li>Feed-forward neural network</li>
<li>Layer normalization</li>
<li>Residual connections</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Known Examples:</li>
</ol>
<ul>
<li>GPT-3 (175B parameter model):
<ul>
<li>96 attention layers</li>
<li>96 heads per layer</li>
</ul>
</li>
<li>GPT-2:
<ul>
<li>Large variant: 48 layers</li>
<li>Medium: 24 layers</li>
<li>Small: 12 layers</li>
</ul>
</li>
</ul>
<ol start="3">
<li>General Pattern:</li>
</ol>
<ul>
<li>Larger models tend to have more layers</li>
<li>Common to use powers of 2 for layer counts</li>
<li>Often balanced with number of attention heads</li>
<li>More layers = deeper understanding but more compute</li>
</ul>
<ol start="4">
<li>Trade-offs:</li>
</ol>
<ul>
<li>More layers = better performance but:
<ul>
<li>Higher computational cost</li>
<li>More memory usage</li>
<li>Longer inference time</li>
<li>Risk of vanishing gradients</li>
</ul>
</li>
</ul>
<h2 id="what-is-the-meaning-of-96-attention-layers-are-they-attention-head-count">What is the meaning of 96 attention layers, are they attention head count?</h2>
<p>N</p>
<ol>
<li>Layer vs Head Count:</li>
</ol>
<ul>
<li>Number of layers and heads are independent parameters</li>
<li>A model with 96 layers might have 12, 20, or 96 heads per layer</li>
<li>The head count doesn&rsquo;t need to match layer count</li>
</ul>
<ol start="2">
<li>Example Configurations:</li>
</ol>
<pre tabindex="0"><code>GPT-3:
- 96 layers
- 96 attention heads per layer

BERT-base:
- 12 layers
- 12 attention heads per layer

BERT-large:
- 24 layers
- 16 attention heads per layer
</code></pre><ol start="3">
<li>Each Layer&rsquo;s Structure:</li>
</ol>
<pre tabindex="0"><code>Transformer Layer
│
├── Multi-Head Attention (e.g., 12 heads)
│   ├── Head 1: (Q1,K1,V1)
│   ├── Head 2: (Q2,K2,V2)
│   └── ... up to Head 12
│
├── Feed Forward Network
└── Layer Normalization
</code></pre><ol start="4">
<li>Process Flow:</li>
</ol>
<ul>
<li>Input → Layer 1 (12 heads) → Layer 2 (12 heads) → &hellip; → Layer 96 (12 heads)</li>
<li>Each head in a layer processes the same input differently</li>
<li>Results from all heads are concatenated and projected</li>
</ul>
<p><strong>The number of heads is more about parallel processing of different attention patterns, while the number of layers is about depth of processing.</strong></p>
<div class="category-section">
    <h4 class="category-section__title">Categories:</h4>
    <div class="category-badges"><a href="/categories/dsblog" class="category-badge">dsblog</a></div>
  </div><div class="td-tags">
    <h4 class="td-tags__title">Tags:</h4>
    <div class="category-badges"><a href="/tags/transformers" class="category-badge">Transformers</a><a href="/tags/embeddings" class="category-badge">Embeddings</a><a href="/tags/deep-learning" class="category-badge">Deep Learning</a></div>
  </div><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Hari Thapliyaal's avatar" class="author-image" >
      </div><div class="td-author-box__info">
      <h4 class="td-author-box__name">Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>

<div class="td-social-share">
  <h4 class="td-social-share__title">Share this article:</h4>
  <ul class="td-social-share__list"><div class="social-share">
        <a href="https://twitter.com/intent/tweet?text=Understanding%20Contextual%20Embedding%20in%20Transformers&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f" target="_blank" rel="noopener" aria-label="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f" target="_blank" rel="noopener" aria-label="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f&title=Understanding%20Contextual%20Embedding%20in%20Transformers" target="_blank" rel="noopener" aria-label="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
        <a href="https://www.reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f&title=Understanding%20Contextual%20Embedding%20in%20Transformers" target="_blank" rel="noopener" aria-label="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
        <a href="mailto:?subject=Understanding%20Contextual%20Embedding%20in%20Transformers&body=http%3a%2f%2flocalhost%3a1313%2fdsblog%2funderstanding-contextual-embedding-in-transformers%2f" aria-label="Share via Email">
          <i class="fas fa-envelope"></i>
        </a>
      </div></ul>
</div>


<div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>

<ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href="/dsblog/understanding-working-of-cnn/" aria-label="Previous page">
            
            <div class="td-pager__meta">
              <i class="fa-solid fa-angle-left"></i>
              <span class="td-pager__meta-label"><b>Previous:</b></span>
              <span class="td-pager__meta-title">Understanding the Working of CNN</span>
            </div>
          </a><a class="td-pager__link td-pager__link--next" href="/dsblog/exploring-tokenization-and-embedding-in-nlp/" aria-label="Next page">
            <div class="td-pager__meta">
              <span class="td-pager__meta-label"><b>Next:</b></span>
              <span class="td-pager__meta-title">Exploring Tokenization and Embedding in NLP</span>
              <i class="fa-solid fa-angle-right"></i>
            </div>
          </a></ul>

        </main>
        <div class="col-md-3">
          
          
            <aside class="td-sidebar-right td-sidebar--flush">
              <div class="td-sidebar__inner">
                <div class="custom-toc">
                  <h5 class="custom-toc__heading">On This Page</h5>
                  <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#what-is-embedding">What is Embedding?</a></li>
    <li><a href="#what-is-fixed-embedding">What is Fixed Embedding?</a></li>
    <li><a href="#how-transformers-handle-context">How Transformers Handle Context</a></li>
    <li><a href="#how-this-token-bank-and-corresponding-embedding-is-stored-in-embedding-database">How this token &lsquo;bank&rsquo; and corresponding embedding is stored in embedding database?</a></li>
    <li><a href="#how-contextural-embedding-is-generated">How contextural embedding is generated?</a></li>
    <li><a href="#what-will-be-the-output-size-of-attention-formula-softmax">What will be the output size of attention formula softmax?</a></li>
    <li><a href="#what-is-meaning-of-a-llm-has-context-length-of-2-million-tokens">What is meaning of a LLM has context length of 2 million tokens?</a></li>
    <li><a href="#how-many-attention-layers-we-keep-in-transformer-like-gpt4">How many attention layers we keep in transformer like gpt4?</a></li>
    <li><a href="#what-is-the-meaning-of-96-attention-layers-are-they-attention-head-count">What is the meaning of 96 attention layers, are they attention head count?</a></li>
  </ul>
</nav>
                </div>
              </div>
            </aside>
          
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      
      <div class="col-2">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="http://localhost:1313/assets/images/site-logo.png" alt="dasarpAI" width="100" style="border-radius: 12px;">
        </a>
      </div>
      <div class="col-8"><div class="row"><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Key Links</h4>
                    <ul><li><a href="/aboutme">About Me</a></li><li><a href="/dscourses">My Data Science Courses/Services</a></li><li><a href="/summary-of-al-ml-projects">MyWork by Business Domain</a></li><li><a href="/summary-of-my-technology-stacks">MyWork by Tech Stack</a></li><li><a href="/summary-of-management-projects">MyWork in Project Management</a></li><li><a href="/clients">Clients</a></li><li><a href="/testimonials">Testimonial</a></li><li><a href="/terms-of-service">Terms &amp; Condition</a></li><li><a href="/privacy">Privacy Policy</a></li><li><a href="/comment-policy">Comment Policy</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>My Blogs</h4>
                    <ul><li><a href="/dsblog">Data Science Blog</a></li><li><a href="/booksumary">Books/Interviews Blog</a></li><li><a href="/news">AI and Business News</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li><li><a href="/wiaposts">Wisdom in Awareness Blog</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/mychanting">My Chantings</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>All Resources</h4>
                    <ul><li><a href="/datascience-tags#ds-resources">DS Resources</a></li><li><a href="https://aibenchmark-explorer.dasarpai.com">AI Benchmark Explorer</a></li><li><a href="/dsblog/ds-ai-ml-books">Data Science-Books</a></li><li><a href="/dsblog/data-science-cheatsheets">Data Science/AI Cheatsheets</a></li><li><a href="/dsblog/best-youtube-channels-for-ds">Video Channels to Learn DS/AI</a></li><li><a href="/dsblog/ds-ai-ml-interview-resources">DS/AI Interview Questions</a></li><li><a href="https://github.com/dasarpai/DAI-Datasets">GitHub DAI-Datasets</a></li><li><a href="/pmi-templates">PMBOK6 Templates</a></li><li><a href="/prince2-templates">PRINCE2 Templates</a></li><li><a href="/microsoft-pm-templates">Microsoft PM Templates</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Project Management</h4>
                    <ul><li><a href="/pmlogy-home">PMLOGY Home</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmglossary">PM Glossary</a></li><li><a href="/pmlogy-tags">PM Topics</a></li><li><a href="/pmbok6-tags">PMBOK6 Topics</a></li><li><a href="/pmbok6-summary">PMBOK6</a></li><li><a href="/pmbok6">PMBOK6 Explorer</a></li><li><a href="/pmbok6hi-tags">PMBOK6 Hindi Topics</a></li><li><a href="/pmbok6hi-summary">PMBoK6 Hindi</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li></ul>
                  </div>
                </div></div>
      


      <div class="row"><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Wisdom in Awareness</h4>
                  <ul><li><a href="/wia-home">WIA Home</a></li><li><a href="/wiaposts">WIA Blog</a></li><li><a href="/wia-tags">WIA Topics</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li><li><a href="/gk-tags">GK Topic</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Samskrutyatra</h4>
                  <ul><li><a href="/samskrutyatra-home">SamskrutYatra Home</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/samskrutyatra-tags">Samskrut Topics</a></li><li><a href="/mychanting">My Vedic Chantings</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>My Gallery</h4>
                  <ul><li><a href="/gallary/slider-online-sessions1">Online AI Classes 1</a></li><li><a href="/gallary/slider-online-sessions2">Online AI Classes 2</a></li><li><a href="/gallary/slider-online-sessions3">Online AI Classes 3</a></li><li><a href="/gallary/slider-online-sessions4">Online AI Classes 4</a></li><li><a href="/gallary/slider-pm-selected-photos">Management Classes</a></li><li><a href="/gallary/slider-pm-workshops">PM &amp; DS Workshop</a></li></ul>
                </div>
              </div></div>
    </div>

    <div class="col-2">

    </div>

      
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.js"></script>
<script src='/js/prism.js'></script>
<script src='/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>
