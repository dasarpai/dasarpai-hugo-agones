<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Dimensionality Reduction and Visualization | Agones</title><meta property="og:url" content="http://localhost:1313/dsblog/Dimensionality-Reduction-and-Visualization/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Dimensionality Reduction and Visualization">
  <meta property="og:description" content="Dimensionality Reduction and Visualization What are the popular methods of dimensionality reduction? Dimensionality reduction is a crucial step in data preprocessing, particularly when dealing with high-dimensional datasets. It helps in reducing the number of features while retaining the essential information, improving computational efficiency, and facilitating data visualization. Here are some popular methods of dimensionality reduction:
Linear Methods Principal Component Analysis (PCA):
Description: PCA transforms the data into a set of linearly uncorrelated components, ordered by the amount of variance they explain. Use Case: Useful for datasets where the directions of maximum variance are important. Implementation: sklearn.decomposition.PCA Linear Discriminant Analysis (LDA):">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2024-07-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-08T15:25:42+05:30">
    <meta property="article:tag" content="Data Science">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="Data Visualization">
    <meta property="article:tag" content="Dimensionality Reduction">
    <meta property="article:tag" content="PCA">
    <meta property="article:tag" content="T-SNE">

  <meta itemprop="name" content="Dimensionality Reduction and Visualization">
  <meta itemprop="description" content="Dimensionality Reduction and Visualization What are the popular methods of dimensionality reduction? Dimensionality reduction is a crucial step in data preprocessing, particularly when dealing with high-dimensional datasets. It helps in reducing the number of features while retaining the essential information, improving computational efficiency, and facilitating data visualization. Here are some popular methods of dimensionality reduction:
Linear Methods Principal Component Analysis (PCA):
Description: PCA transforms the data into a set of linearly uncorrelated components, ordered by the amount of variance they explain. Use Case: Useful for datasets where the directions of maximum variance are important. Implementation: sklearn.decomposition.PCA Linear Discriminant Analysis (LDA):">
  <meta itemprop="datePublished" content="2024-07-24T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-05-08T15:25:42+05:30">
  <meta itemprop="wordCount" content="2750">
  <meta itemprop="keywords" content="Dimensionality Reduction,Data Visualization,PCA Analysis,t-SNE,UMAP,Feature Selection,Data Analysis Methods,Machine Learning Techniques">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Dimensionality Reduction and Visualization">
  <meta name="twitter:description" content="Dimensionality Reduction and Visualization What are the popular methods of dimensionality reduction? Dimensionality reduction is a crucial step in data preprocessing, particularly when dealing with high-dimensional datasets. It helps in reducing the number of features while retaining the essential information, improving computational efficiency, and facilitating data visualization. Here are some popular methods of dimensionality reduction:
Linear Methods Principal Component Analysis (PCA):
Description: PCA transforms the data into a set of linearly uncorrelated components, ordered by the amount of variance they explain. Use Case: Useful for datasets where the directions of maximum variance are important. Implementation: sklearn.decomposition.PCA Linear Discriminant Analysis (LDA):">



<link rel="stylesheet" href="/css/prism.css"/>

<link href="/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>


<link rel="stylesheet" href="/css/custom.css">

<script src="/js/lunr.js"></script>


    <style>
       
      .td-main img {
        max-width: 100%;
        height: auto;
      }
      .td-main {
        padding-top: 60px;  
      }
       
      .td-sidebar-right {
          padding-left: 20px;  
      }
    </style>
  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/samskrutyatra/"><span>Samskrut Yatra Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search position-relative">
  <div class="td-search__icon"></div>
  <input
    id="agones-search"
    type="search"
    class="td-search__input form-control td-search-input"
    placeholder="Search this site…"
    aria-label="Search this site…"
    autocomplete="off"
  >
  <ul id="agones-search-results" class="list-group position-absolute w-100" style="z-index:1000; top:100%; left:0;"></ul>
</div>

<script>
let lunrIndex, pagesIndex;

async function initLunr() {
  const response = await fetch('/index.json');
  pagesIndex = await response.json();
  lunrIndex = lunr(function () {
    this.ref('url');
    this.field('title', { boost: 10 });
    this.field('content');
    pagesIndex.forEach(function (doc) {
      this.add(doc);
    }, this);
  });
}

function search(query) {
  if (!lunrIndex || !query) return [];
  return lunrIndex.search(query).map(result =>
    pagesIndex.find(page => page.url === result.ref)
  );
}

document.addEventListener('DOMContentLoaded', function () {
  initLunr();
  const input = document.getElementById('agones-search');
  const resultsList = document.getElementById('agones-search-results');
  input.addEventListener('input', function (e) {
    const query = e.target.value.trim();
    if (!query) {
      resultsList.innerHTML = '';
      resultsList.style.display = 'none';
      return;
    }
    const results = search(query);
    if (results.length === 0) {
      resultsList.innerHTML = '<li class="list-group-item">No results found.</li>';
      resultsList.style.display = 'block';
      return;
    }
    resultsList.innerHTML = results.map(page =>
      `<li class="list-group-item"><a href="${page.url}">${page.title}</a></li>`
    ).join('');
    resultsList.style.display = 'block';
  });
  
  input.addEventListener('blur', function() {
    setTimeout(() => { resultsList.style.display = 'none'; }, 200);
  });
  
  input.addEventListener('focus', function() {
    if (input.value.trim()) resultsList.style.display = 'block';
  });
});
</script></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <div class="row">
        <div class="col-md-3">
          
        </div>
        <main role="main" class="col-md-6 td-main">
          <p><img src="/assets/images/dspost/dsp6126-Dimensionality-Reduction-and-Visualization.jpg" alt="Dimensionality-Reduction-and-Visualization"></p>
<h1 id="dimensionality-reduction-and-visualization">Dimensionality Reduction and Visualization</h1>
<h2 id="what-are-the-popular-methods-of-dimensionality-reduction">What are the popular methods of dimensionality reduction?</h2>
<p>Dimensionality reduction is a crucial step in data preprocessing, particularly when dealing with high-dimensional datasets. It helps in reducing the number of features while retaining the essential information, improving computational efficiency, and facilitating data visualization. Here are some popular methods of dimensionality reduction:</p>
<h3 id="linear-methods">Linear Methods</h3>
<ol>
<li>
<p><strong>Principal Component Analysis (PCA)</strong>:</p>
<ul>
<li><strong>Description</strong>: PCA transforms the data into a set of linearly uncorrelated components, ordered by the amount of variance they explain.</li>
<li><strong>Use Case</strong>: Useful for datasets where the directions of maximum variance are important.</li>
<li><strong>Implementation</strong>: <code>sklearn.decomposition.PCA</code></li>
</ul>
</li>
<li>
<p><strong>Linear Discriminant Analysis (LDA)</strong>:</p>
<ul>
<li><strong>Description</strong>: LDA aims to find a linear combination of features that best separate two or more classes.</li>
<li><strong>Use Case</strong>: Supervised learning, particularly in classification tasks.</li>
<li><strong>Implementation</strong>: <code>sklearn.discriminant_analysis.LinearDiscriminantAnalysis</code></li>
</ul>
</li>
<li>
<p><strong>Factor Analysis</strong>:</p>
<ul>
<li><strong>Description</strong>: Assumes that the observed variables are linear combinations of potential factors plus noise.</li>
<li><strong>Use Case</strong>: Used for identifying hidden variables that explain observed data.</li>
<li><strong>Implementation</strong>: <code>sklearn.decomposition.FactorAnalysis</code></li>
</ul>
</li>
</ol>
<h3 id="non-linear-methods">Non-Linear Methods</h3>
<ol>
<li>
<p><strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong>:</p>
<ul>
<li><strong>Description</strong>: A non-linear technique that reduces dimensions while preserving the local structure of the data.</li>
<li><strong>Use Case</strong>: Visualization of high-dimensional data, especially for clustering.</li>
<li><strong>Implementation</strong>: <code>sklearn.manifold.TSNE</code></li>
</ul>
</li>
<li>
<p><strong>Uniform Manifold Approximation and Projection (UMAP)</strong>:</p>
<ul>
<li><strong>Description</strong>: A non-linear method that preserves both local and global data structure, often faster than t-SNE.</li>
<li><strong>Use Case</strong>: Visualization and understanding of high-dimensional data.</li>
<li><strong>Implementation</strong>: <code>umap.UMAP</code></li>
</ul>
</li>
<li>
<p><strong>Kernel PCA</strong>:</p>
<ul>
<li><strong>Description</strong>: An extension of PCA using kernel methods to capture non-linear relationships.</li>
<li><strong>Use Case</strong>: When the data has non-linear relationships that standard PCA cannot capture.</li>
<li><strong>Implementation</strong>: <code>sklearn.decomposition.KernelPCA</code></li>
</ul>
</li>
<li>
<p><strong>Isomap</strong>:</p>
<ul>
<li><strong>Description</strong>: Combines PCA and multi-dimensional scaling (MDS) to preserve global geometric structures.</li>
<li><strong>Use Case</strong>: Non-linear dimensionality reduction maintaining global relationships.</li>
<li><strong>Implementation</strong>: <code>sklearn.manifold.Isomap</code></li>
</ul>
</li>
<li>
<p><strong>Locally Linear Embedding (LLE)</strong>:</p>
<ul>
<li><strong>Description</strong>: Preserves local structure by linearizing local patches of the manifold.</li>
<li><strong>Use Case</strong>: When the data lies on a non-linear manifold.</li>
<li><strong>Implementation</strong>: <code>sklearn.manifold.LocallyLinearEmbedding</code></li>
</ul>
</li>
</ol>
<h3 id="autoencoders">Autoencoders</h3>
<ol>
<li><strong>Autoencoders</strong>:
<ul>
<li><strong>Description</strong>: Neural networks that learn to compress data into a lower-dimensional representation and then reconstruct it.</li>
<li><strong>Use Case</strong>: Complex non-linear relationships in large datasets.</li>
<li><strong>Implementation</strong>: Libraries like TensorFlow or PyTorch</li>
</ul>
</li>
</ol>
<h3 id="others">Others</h3>
<ol>
<li>
<p><strong>Independent Component Analysis (ICA)</strong>:</p>
<ul>
<li><strong>Description</strong>: Separates a multivariate signal into additive, independent components.</li>
<li><strong>Use Case</strong>: Situations where the goal is to find underlying factors that are statistically independent.</li>
<li><strong>Implementation</strong>: <code>sklearn.decomposition.FastICA</code></li>
</ul>
</li>
<li>
<p><strong>Random Projection</strong>:</p>
<ul>
<li><strong>Description</strong>: Projects data to a lower-dimensional space using a random matrix.</li>
<li><strong>Use Case</strong>: When computational efficiency is more critical than exact dimensionality reduction.</li>
<li><strong>Implementation</strong>: <code>sklearn.random_projection</code></li>
</ul>
</li>
<li>
<p><strong>Non-negative Matrix Factorization (NMF)</strong>:</p>
<ul>
<li><strong>Description</strong>: Factorizes the data matrix into two matrices with non-negative entries.</li>
<li><strong>Use Case</strong>: When the data is non-negative and parts-based representation is meaningful.</li>
<li><strong>Implementation</strong>: <code>sklearn.decomposition.NMF</code></li>
</ul>
</li>
</ol>
<h3 id="practical-considerations">Practical Considerations</h3>
<ul>
<li><strong>Data Size</strong>: Large datasets might require more computationally efficient methods like PCA or Random Projection.</li>
<li><strong>Non-Linearity</strong>: Use non-linear methods like t-SNE, UMAP, or Kernel PCA if the data has complex non-linear relationships.</li>
<li><strong>Supervised vs. Unsupervised</strong>: LDA is a supervised method useful for classification, whereas methods like PCA, t-SNE, and UMAP are unsupervised.</li>
</ul>
<h3 id="example-implementations">Example Implementations</h3>
<h4 id="pca-example">PCA Example</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming X is your data matrix</span>
</span></span><span style="display:flex;"><span>pca <span style="color:#f92672">=</span> PCA(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>X_reduced <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>fit_transform(X)
</span></span></code></pre></div><h4 id="t-sne-example">t-SNE Example</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.manifold <span style="color:#f92672">import</span> TSNE
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming X is your data matrix</span>
</span></span><span style="display:flex;"><span>tsne <span style="color:#f92672">=</span> TSNE(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, perplexity<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, n_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>)
</span></span><span style="display:flex;"><span>X_reduced <span style="color:#f92672">=</span> tsne<span style="color:#f92672">.</span>fit_transform(X)
</span></span></code></pre></div><h4 id="umap-example">UMAP Example</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> umap
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming X is your data matrix</span>
</span></span><span style="display:flex;"><span>umap_reducer <span style="color:#f92672">=</span> umap<span style="color:#f92672">.</span>UMAP(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>X_reduced <span style="color:#f92672">=</span> umap_reducer<span style="color:#f92672">.</span>fit_transform(X)
</span></span></code></pre></div><p>Selecting the appropriate dimensionality reduction technique depends on the specific requirements of your analysis, such as the size and nature of the data, computational resources, and the intended use of the reduced dimensions (e.g., visualization, further modeling).</p>
<h2 id="discuss-dimensionality-reduction-using-t-sne">Discuss Dimensionality Reduction using t-SNE</h2>
<p>tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)</p>
<p>The <code>TSNE</code> function is from the <code>scikit-learn</code> library and stands for t-distributed Stochastic Neighbor Embedding. It is used for dimensionality reduction, particularly for the visualization of high-dimensional data. Here&rsquo;s the explanation of each parameter:</p>
<ol>
<li>
<p><strong><code>n_components</code></strong>: This parameter determines the number of dimensions in the embedded space. In this case, <code>n_components=2</code> means that the data will be reduced to 2 dimensions, which is suitable for 2D visualization.</p>
</li>
<li>
<p><strong><code>random_state</code></strong>: This parameter sets the seed for the random number generator. Providing a fixed <code>random_state=42</code> ensures that the results are reproducible, meaning you will get the same output each time you run the code with this seed.</p>
</li>
<li>
<p><strong><code>n_iter</code></strong>: This parameter specifies the number of iterations for optimization. The default value is usually 1000, but here it is set to <code>5000</code>, which means the optimization process will run for 5000 iterations. More iterations can lead to a more accurate embedding but will take more time.</p>
</li>
<li>
<p><strong><code>perplexity</code></strong>: This parameter is related to the number of nearest neighbors that is used in other manifold learning algorithms. It is a measure of the effective number of neighbors. A lower perplexity like <code>5</code> might be useful for smaller datasets, while higher values are suitable for larger datasets.</p>
</li>
</ol>
<p>So, this line of code configures the t-SNE algorithm to reduce the data to 2 dimensions, with a fixed random seed for reproducibility, performing 5000 iterations of optimization, and considering 5 nearest neighbors for the perplexity parameter.</p>
<h2 id="what-is-perplexity-in-t-sne">What is perplexity in t-SNE?</h2>
<h3 id="perplexity-and-its-role-in-t-sne">Perplexity and its Role in t-SNE</h3>
<p>Perplexity is a parameter in t-SNE that balances the attention between local and global aspects of your data. It is related to the number of nearest neighbors considered when computing the pairwise similarities in the high-dimensional space.</p>
<h4 id="detailed-explanation">Detailed Explanation:</h4>
<ol>
<li>
<p><strong>Probabilistic Interpretation</strong>:</p>
<ul>
<li>In t-SNE, each data point ( i ) has a probability distribution over all other points ( j ), indicating how likely it is that ( j ) is a neighbor of ( i ).</li>
<li>Perplexity is a measure of how concentrated this distribution is, which directly impacts how many neighbors influence the positioning of each point in the embedded space.</li>
</ul>
</li>
<li>
<p><strong>Mathematical Definition</strong>:</p>
<ul>
<li>Perplexity ( P ) is defined as ( 2^{H(P_i)} ), where ( H(P_i) ) is the Shannon entropy of the probability distribution ( P_i ) over other points.</li>
<li>Shannon entropy ( H(P_i) = - \sum_{j} P_{ij} \log P_{ij} ).</li>
<li>In simpler terms, perplexity can be thought of as a smooth measure of the effective number of neighbors for each point.</li>
</ul>
</li>
<li>
<p><strong>Effect on Embedding</strong>:</p>
<ul>
<li><strong>Low Perplexity (e.g., 5)</strong>:
<ul>
<li>The algorithm focuses more on local structure.</li>
<li>Each point considers fewer neighbors.</li>
<li>Useful for small datasets or when you want to capture fine-grained local relationships.</li>
</ul>
</li>
<li><strong>High Perplexity (e.g., 30-50)</strong>:
<ul>
<li>The algorithm takes into account a broader neighborhood.</li>
<li>Each point considers more neighbors.</li>
<li>Suitable for larger datasets or when you want to preserve more global structure.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="practical-implications">Practical Implications:</h4>
<ul>
<li><strong>Smaller Datasets</strong>:
<ul>
<li>Fewer data points mean each point has fewer potential neighbors. A lower perplexity (e.g., 5) ensures that t-SNE captures the local structure well without being too influenced by distant points.</li>
</ul>
</li>
<li><strong>Larger Datasets</strong>:
<ul>
<li>More data points mean more potential neighbors. A higher perplexity (e.g., 30-50) helps in capturing the global structure by considering a wider neighborhood around each point, preventing the embedding from being too noisy.</li>
</ul>
</li>
</ul>
<h3 id="choosing-the-right-perplexity">Choosing the Right Perplexity:</h3>
<ul>
<li>There is no one-size-fits-all value for perplexity; it often requires some experimentation.</li>
<li>Common practice is to try a few different values (e.g., 5, 10, 30, 50) and visually inspect the resulting embeddings to choose the best one.</li>
<li>The default value in many implementations is around 30, which generally works well for medium-sized datasets.</li>
</ul>
<p>In summary, perplexity in t-SNE is a crucial parameter that controls the balance between local and global structure in the data. Lower perplexity values emphasize local relationships, while higher values help in capturing more global patterns.</p>
<h2 id="how-to-set-perplexity-size">How to set perplexity size?</h2>
<p><strong>I have small dataset of 1000 samples and I keep perplexity high (like 70) then the embedding or generated new records from t-sne algorithm will not have effective embedding of those 1000 records?</strong></p>
<p>Yes, that&rsquo;s correct. If you have a small dataset of 1000 records and set the perplexity to a high value like 70, the t-SNE embedding might not be as effective. Here&rsquo;s why:</p>
<h3 id="why-high-perplexity-can-be-ineffective-for-small-datasets">Why High Perplexity Can Be Ineffective for Small Datasets:</h3>
<ol>
<li>
<p><strong>Overly Broad Neighborhoods</strong>:</p>
<ul>
<li>Perplexity of 70 implies considering a neighborhood of around 70 points for each data point.</li>
<li>In a dataset with only 1000 points, each point&rsquo;s neighborhood would encompass a significant fraction of the entire dataset.</li>
<li>This dilutes the local structure, as the algorithm tries to consider too many points as neighbors, which can lead to less meaningful embeddings.</li>
</ul>
</li>
<li>
<p><strong>Reduced Sensitivity to Local Differences</strong>:</p>
<ul>
<li>High perplexity values make t-SNE focus more on global structure.</li>
<li>For small datasets, the algorithm might not adequately capture fine-grained local patterns because it averages over too many points.</li>
<li>This can cause points that are locally distinct to be embedded too closely together, losing important details.</li>
</ul>
</li>
<li>
<p><strong>Potential for Overlap and Clustering Issues</strong>:</p>
<ul>
<li>With high perplexity, the embedding might become too smoothed out.</li>
<li>Points that should be distinct clusters could overlap, making it harder to distinguish different groups within the data.</li>
</ul>
</li>
</ol>
<h3 id="practical-example">Practical Example:</h3>
<p>Imagine you have a dataset of 1000 customer records, each with several features. You want to visualize these customers to understand different segments (clusters) based on their behavior.</p>
<ul>
<li>
<p><strong>Low Perplexity (e.g., 5-10)</strong>:</p>
<ul>
<li>Each customer is compared with a smaller number of neighbors.</li>
<li>Clusters representing different segments are more likely to be distinct.</li>
<li>Fine details and local variations are preserved.</li>
</ul>
</li>
<li>
<p><strong>High Perplexity (e.g., 70)</strong>:</p>
<ul>
<li>Each customer is compared with many other customers.</li>
<li>Clusters may blur together, making it hard to identify distinct segments.</li>
<li>Local differences might be lost, and the visualization could be less informative.</li>
</ul>
</li>
</ul>
<h3 id="choosing-perplexity-for-small-datasets">Choosing Perplexity for Small Datasets:</h3>
<p>For a small dataset like 1000 records, it&rsquo;s usually better to start with a lower perplexity, such as 5, 10, or 20. This helps ensure that the local structure is captured well, and distinct clusters are identifiable. You can experiment with different values and visually inspect the results to find the most meaningful embedding.</p>
<p>In summary, setting a high perplexity value for a small dataset can lead to less effective embeddings, as it can obscure local patterns and make clusters less distinct. For small datasets, lower perplexity values are generally more appropriate to capture the local structure effectively.</p>
<h2 id="to-visulize-cluster-from-high-dimensional-data-i-can-use-pca-or-t-sne-which-one-make-more-sense">To visulize cluster from high dimensional data I can use PCA or t-SNE, which one make more sense?</h2>
<p>Both PCA (Principal Component Analysis) and t-SNE (t-distributed Stochastic Neighbor Embedding) are commonly used techniques for visualizing high-dimensional data, but they serve different purposes and have different strengths. Choosing between PCA and t-SNE depends on the specific characteristics of your data and the goals of your analysis. Here&rsquo;s a comparison to help you decide which one makes more sense for your situation:</p>
<h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3>
<p><strong>Advantages:</strong></p>
<ol>
<li><strong>Linear Transformations</strong>:
<ul>
<li>PCA reduces dimensionality by finding the principal components, which are linear combinations of the original features. This means PCA is good for capturing the global structure and variance in the data.</li>
</ul>
</li>
<li><strong>Speed and Simplicity</strong>:
<ul>
<li>PCA is computationally less intensive compared to t-SNE. It can handle large datasets quickly.</li>
</ul>
</li>
<li><strong>Interpretability</strong>:
<ul>
<li>The principal components can be interpreted as the directions of maximum variance. This makes it easier to understand the relationship between the original features and the reduced dimensions.</li>
</ul>
</li>
<li><strong>Deterministic</strong>:
<ul>
<li>PCA produces the same result every time it is run on the same dataset (it is not dependent on a random seed).</li>
</ul>
</li>
</ol>
<p><strong>Disadvantages:</strong></p>
<ol>
<li><strong>Captures Only Linear Relationships</strong>:
<ul>
<li>PCA may not perform well if the data has complex, non-linear relationships.</li>
</ul>
</li>
<li><strong>Less Effective for Complex Structures</strong>:
<ul>
<li>For visualizing complex manifolds or clusters in high-dimensional space, PCA might not be very effective.</li>
</ul>
</li>
</ol>
<h3 id="t-distributed-stochastic-neighbor-embedding-t-sne">t-Distributed Stochastic Neighbor Embedding (t-SNE)</h3>
<p><strong>Advantages:</strong></p>
<ol>
<li><strong>Captures Non-Linear Relationships</strong>:
<ul>
<li>t-SNE is designed to capture complex, non-linear relationships in the data. It excels at preserving the local structure of the data, making it very effective for visualizing clusters.</li>
</ul>
</li>
<li><strong>Good for Clustering</strong>:
<ul>
<li>t-SNE tends to produce clear, well-separated clusters, which is particularly useful for visualizing distinct groups within the data.</li>
</ul>
</li>
</ol>
<p><strong>Disadvantages:</strong></p>
<ol>
<li><strong>Computationally Intensive</strong>:
<ul>
<li>t-SNE is more computationally expensive and slower, especially for large datasets.</li>
</ul>
</li>
<li><strong>Non-Deterministic</strong>:
<ul>
<li>The results of t-SNE can vary between runs unless the random seed is fixed.</li>
</ul>
</li>
<li><strong>Interpretability</strong>:
<ul>
<li>The resulting dimensions from t-SNE are not easily interpretable in terms of the original features.</li>
</ul>
</li>
<li><strong>Parameter Sensitivity</strong>:
<ul>
<li>t-SNE has several hyperparameters (like perplexity, learning rate) that can significantly affect the results and may require tuning.</li>
</ul>
</li>
</ol>
<h3 id="when-to-use-each">When to Use Each:</h3>
<p><strong>Use PCA When:</strong></p>
<ul>
<li>You want a quick and computationally efficient way to reduce dimensionality.</li>
<li>You are interested in understanding the global variance structure of your data.</li>
<li>Your data likely has linear relationships.</li>
</ul>
<p><strong>Use t-SNE When:</strong></p>
<ul>
<li>You are primarily interested in visualizing clusters and the local structure of the data.</li>
<li>Your data has complex, non-linear relationships.</li>
<li>You are working with smaller datasets (typically up to a few thousand points) or can afford the computational cost.</li>
</ul>
<h3 id="combined-approach">Combined Approach:</h3>
<p>Sometimes, a combined approach can be useful:</p>
<ol>
<li><strong>PCA for Initial Reduction</strong>:
<ul>
<li>Use PCA to reduce the dimensionality to a manageable level (e.g., from thousands of dimensions to 50 or 100).</li>
</ul>
</li>
<li><strong>t-SNE for Final Visualization</strong>:
<ul>
<li>Then apply t-SNE on the reduced dimensions for detailed visualization. This can improve the performance and effectiveness of t-SNE.</li>
</ul>
</li>
</ol>
<h3 id="summary">Summary:</h3>
<ul>
<li><strong>PCA</strong> is generally more suitable for capturing global structures and is computationally efficient, making it a good first step for dimensionality reduction or when dealing with very large datasets.</li>
<li><strong>t-SNE</strong> is more effective for visualizing clusters and capturing non-linear relationships, making it a powerful tool for detailed exploration of complex datasets, especially for visualizing clusters.</li>
</ul>
<p>In many practical cases, starting with PCA and then using t-SNE on the reduced data can combine the strengths of both methods for effective visualization.</p>
<h2 id="how-will-i-know-my-dataset-of-1000-samples-and-80-features-is-complex-or-simple">How will I know my dataset of 1000 samples and 80 features is complex or simple?</h2>
<p>Determining whether your dataset has complex, non-linear relationships or is relatively simple and linear can be approached through a combination of exploratory data analysis and applying different techniques to see how well they perform. Here are some steps and methods you can use to assess the complexity of your dataset:</p>
<h3 id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</h3>
<ol>
<li>
<p><strong>Visualizing Pairwise Relationships</strong>:</p>
<ul>
<li>Use scatter plots or pair plots to visualize the relationships between pairs of variables. Libraries like Seaborn (<code>sns.pairplot()</code>) can be helpful for this.</li>
<li>Look for patterns that are not linear, such as curves or clusters that are not well-separated by straight lines.</li>
</ul>
</li>
<li>
<p><strong>Correlation Matrix</strong>:</p>
<ul>
<li>Calculate the correlation matrix of your variables. Strong linear correlations (close to 1 or -1) suggest linear relationships.</li>
<li>Use heatmaps to visualize the correlation matrix (<code>sns.heatmap()</code> in Seaborn).</li>
</ul>
</li>
<li>
<p><strong>Non-Linear Visualization</strong>:</p>
<ul>
<li>Apply non-linear visualization techniques like t-SNE or UMAP and see if they reveal structure or clusters that were not apparent with linear techniques like PCA.</li>
</ul>
</li>
</ol>
<h3 id="applying-different-techniques">Applying Different Techniques</h3>
<ol>
<li>
<p><strong>PCA Analysis</strong>:</p>
<ul>
<li>Apply PCA to your data and plot the explained variance ratio for the principal components.</li>
<li>If a few components explain a large portion of the variance, your data might have a simpler structure. If you need many components to explain the variance, the data might be more complex.</li>
</ul>
</li>
<li>
<p><strong>Residual Analysis</strong>:</p>
<ul>
<li>Fit a linear model (e.g., linear regression) to the data and analyze the residuals.</li>
<li>Large or systematic patterns in the residuals suggest non-linear relationships.</li>
</ul>
</li>
<li>
<p><strong>Model Performance</strong>:</p>
<ul>
<li>Fit both linear and non-linear models (e.g., linear regression vs. decision trees or random forests) to your data.</li>
<li>Compare their performance metrics (e.g., R^2, mean squared error) on a validation set. If non-linear models perform significantly better, it indicates non-linear relationships.</li>
</ul>
</li>
</ol>
<h3 id="specific-methods">Specific Methods</h3>
<ol>
<li>
<p><strong>Kernel PCA</strong>:</p>
<ul>
<li>Apply Kernel PCA, which can capture non-linear structures by using kernel functions.</li>
<li>Compare the results with standard PCA to see if Kernel PCA provides a significantly better embedding.</li>
</ul>
</li>
<li>
<p><strong>t-SNE</strong>:</p>
<ul>
<li>Apply t-SNE directly to your data and examine the resulting plot.</li>
<li>Clear clusters or complex structures in the t-SNE plot suggest non-linear relationships.</li>
</ul>
</li>
<li>
<p><strong>UMAP (Uniform Manifold Approximation and Projection)</strong>:</p>
<ul>
<li>Similar to t-SNE but often faster and can capture both local and global structure.</li>
<li>Visualize the data using UMAP and see if it reveals more structure compared to PCA.</li>
</ul>
</li>
</ol>
<h3 id="practical-steps-with-code-examples">Practical Steps with Code Examples</h3>
<h4 id="visualizing-pairwise-relationships">Visualizing Pairwise Relationships</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming df is your DataFrame with 80 variables</span>
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>pairplot(df<span style="color:#f92672">.</span>sample(<span style="color:#ae81ff">200</span>))  <span style="color:#75715e"># sample if the dataset is large for visualization</span>
</span></span></code></pre></div><h4 id="correlation-matrix-and-heatmap">Correlation Matrix and Heatmap</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>correlation_matrix <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>corr()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>heatmap(correlation_matrix, annot<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;coolwarm&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h4 id="pca-explained-variance">PCA Explained Variance</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pca <span style="color:#f92672">=</span> PCA()
</span></span><span style="display:flex;"><span>pca<span style="color:#f92672">.</span>fit(df)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(np<span style="color:#f92672">.</span>cumsum(pca<span style="color:#f92672">.</span>explained_variance_ratio_))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Number of Components&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Explained Variance&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h4 id="residual-analysis">Residual Analysis</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_squared_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;target&#39;</span>, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># assuming &#39;target&#39; is your target variable</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;target&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(X, y)
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>residuals <span style="color:#f92672">=</span> y <span style="color:#f92672">-</span> predictions
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(predictions, residuals)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Predictions&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Residuals&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>By following these steps, you can get a good sense of whether your dataset exhibits complex, non-linear relationships or if it is simpler and more linear. This will guide you in choosing the appropriate dimensionality reduction technique and other analysis methods.</p>
<p><strong>Author</strong><br>
Dr Hari Thapliyaal<br>
dasarpai.com <br>
linkedin.com/in/harithapliyal</p>
<div class="category-section">
    <h4 class="category-section__title">Categories:</h4>
    <div class="category-badges"><a href="/categories/dsblog" class="category-badge">dsblog</a></div>
  </div><div class="td-tags">
    <h4 class="td-tags__title">Tags:</h4>
    <div class="category-badges"><a href="/tags/data-science" class="category-badge">Data Science</a><a href="/tags/machine-learning" class="category-badge">Machine Learning</a><a href="/tags/data-visualization" class="category-badge">Data Visualization</a><a href="/tags/dimensionality-reduction" class="category-badge">Dimensionality Reduction</a><a href="/tags/pca" class="category-badge">PCA</a><a href="/tags/t-sne" class="category-badge">t-SNE</a><a href="/tags/data-analysis" class="category-badge">Data Analysis</a></div>
  </div><div class="td-author-box"><div class="td-author-box__avatar">
        <img src="/assets/images/myphotos/Profilephoto1.jpg" alt="Hari Thapliyaal's avatar" class="author-image" >
      </div><div class="td-author-box__info">
      <h4 class="td-author-box__name">Hari Thapliyaal</h4><p class="td-author-box__bio">Dr. Hari Thapliyal is a seasoned professional and prolific blogger with a multifaceted background that spans the realms of Data Science, Project Management, and Advait-Vedanta Philosophy. Holding a Doctorate in AI/NLP from SSBM (Geneva, Switzerland), Hari has earned Master&#39;s degrees in Computers, Business Management, Data Science, and Economics, reflecting his dedication to continuous learning and a diverse skill set.

With over three decades of experience in management and leadership, Hari has proven expertise in training, consulting, and coaching within the technology sector. His extensive 16&#43; years in all phases of software product development are complemented by a decade-long focus on course design, training, coaching, and consulting in Project Management.

 In the dynamic field of Data Science, Hari stands out with more than three years of hands-on experience in software development, training course development, training, and mentoring professionals. His areas of specialization include Data Science, AI, Computer Vision, NLP, complex machine learning algorithms, statistical modeling, pattern identification, and extraction of valuable insights.

Hari&#39;s professional journey showcases his diverse experience in planning and executing multiple types of projects. He excels in driving stakeholders to identify and resolve business problems, consistently delivering excellent results. Beyond the professional sphere, Hari finds solace in long meditation, often seeking secluded places or immersing himself in the embrace of nature.</p></div>
  </div>

<div class="td-social-share">
  <h4 class="td-social-share__title">Share this article:</h4>
  <ul class="td-social-share__list"><div class="social-share">
        <a href="https://twitter.com/intent/tweet?text=Dimensionality%20Reduction%20and%20Visualization&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fDimensionality-Reduction-and-Visualization%2f" target="_blank" rel="noopener" aria-label="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
        <a href="https://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fDimensionality-Reduction-and-Visualization%2f" target="_blank" rel="noopener" aria-label="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
        <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fDimensionality-Reduction-and-Visualization%2f&title=Dimensionality%20Reduction%20and%20Visualization" target="_blank" rel="noopener" aria-label="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
        <a href="https://www.reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fDimensionality-Reduction-and-Visualization%2f&title=Dimensionality%20Reduction%20and%20Visualization" target="_blank" rel="noopener" aria-label="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
        <a href="mailto:?subject=Dimensionality%20Reduction%20and%20Visualization&body=http%3a%2f%2flocalhost%3a1313%2fdsblog%2fDimensionality-Reduction-and-Visualization%2f" aria-label="Share via Email">
          <i class="fas fa-envelope"></i>
        </a>
      </div></ul>
</div>


<div class="td-comments">
      <h4 class="td-comments__title">Comments:</h4>
      <script src="https://giscus.app/client.js"
              data-repo="dasarpai/dasarpai-comments"
              data-repo-id="R_kgDOOGVFpA"
              data-category="General"
              data-category-id="DIC_kwDOOGVFpM4CnzHR"
              data-mapping="url"
              data-reactions-enabled="1"
              data-theme="light"
              data-strict="1"
              data-input-position="top"
              data-emit-metadata="1"
              data-lang="en"
              crossorigin="anonymous"
              async>
      </script>
    </div>

<ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><a class="td-pager__link td-pager__link--prev" href="/dsblog/Serverless-databases/" aria-label="Previous page">
            
            <div class="td-pager__meta">
              <i class="fa-solid fa-angle-left"></i>
              <span class="td-pager__meta-label"><b>Previous:</b></span>
              <span class="td-pager__meta-title">Serverless databases</span>
            </div>
          </a><a class="td-pager__link td-pager__link--next" href="/dsblog/transformers-demystified-a-step-by-step-guide/" aria-label="Next page">
            <div class="td-pager__meta">
              <span class="td-pager__meta-label"><b>Next:</b></span>
              <span class="td-pager__meta-title">Transformers Demystified A Step-by-Step Guide</span>
              <i class="fa-solid fa-angle-right"></i>
            </div>
          </a></ul>

        </main>
        <div class="col-md-3">
          
          
            <aside class="td-sidebar-right td-sidebar--flush">
              <div class="td-sidebar__inner">
                <div class="custom-toc">
                  <h5 class="custom-toc__heading">On This Page</h5>
                  <nav id="TableOfContents">
  <ul>
    <li><a href="#what-are-the-popular-methods-of-dimensionality-reduction">What are the popular methods of dimensionality reduction?</a>
      <ul>
        <li><a href="#linear-methods">Linear Methods</a></li>
        <li><a href="#non-linear-methods">Non-Linear Methods</a></li>
        <li><a href="#autoencoders">Autoencoders</a></li>
        <li><a href="#others">Others</a></li>
        <li><a href="#practical-considerations">Practical Considerations</a></li>
        <li><a href="#example-implementations">Example Implementations</a></li>
      </ul>
    </li>
    <li><a href="#discuss-dimensionality-reduction-using-t-sne">Discuss Dimensionality Reduction using t-SNE</a></li>
    <li><a href="#what-is-perplexity-in-t-sne">What is perplexity in t-SNE?</a>
      <ul>
        <li><a href="#perplexity-and-its-role-in-t-sne">Perplexity and its Role in t-SNE</a></li>
        <li><a href="#choosing-the-right-perplexity">Choosing the Right Perplexity:</a></li>
      </ul>
    </li>
    <li><a href="#how-to-set-perplexity-size">How to set perplexity size?</a>
      <ul>
        <li><a href="#why-high-perplexity-can-be-ineffective-for-small-datasets">Why High Perplexity Can Be Ineffective for Small Datasets:</a></li>
        <li><a href="#practical-example">Practical Example:</a></li>
        <li><a href="#choosing-perplexity-for-small-datasets">Choosing Perplexity for Small Datasets:</a></li>
      </ul>
    </li>
    <li><a href="#to-visulize-cluster-from-high-dimensional-data-i-can-use-pca-or-t-sne-which-one-make-more-sense">To visulize cluster from high dimensional data I can use PCA or t-SNE, which one make more sense?</a>
      <ul>
        <li><a href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
        <li><a href="#t-distributed-stochastic-neighbor-embedding-t-sne">t-Distributed Stochastic Neighbor Embedding (t-SNE)</a></li>
        <li><a href="#when-to-use-each">When to Use Each:</a></li>
        <li><a href="#combined-approach">Combined Approach:</a></li>
        <li><a href="#summary">Summary:</a></li>
      </ul>
    </li>
    <li><a href="#how-will-i-know-my-dataset-of-1000-samples-and-80-features-is-complex-or-simple">How will I know my dataset of 1000 samples and 80 features is complex or simple?</a>
      <ul>
        <li><a href="#exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</a></li>
        <li><a href="#applying-different-techniques">Applying Different Techniques</a></li>
        <li><a href="#specific-methods">Specific Methods</a></li>
        <li><a href="#practical-steps-with-code-examples">Practical Steps with Code Examples</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
              </div>
            </aside>
          
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      
      <div class="col-2">
        <a href="https://dasarpai.com" target="_blank" rel="noopener">
          <img src="http://localhost:1313/assets/images/site-logo.png" alt="dasarpAI" width="100" style="border-radius: 12px;">
        </a>
      </div>
      <div class="col-8"><div class="row"><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Key Links</h4>
                    <ul><li><a href="/aboutme">About Me</a></li><li><a href="/dscourses">My Data Science Courses/Services</a></li><li><a href="/summary-of-al-ml-projects">MyWork by Business Domain</a></li><li><a href="/summary-of-my-technology-stacks">MyWork by Tech Stack</a></li><li><a href="/summary-of-management-projects">MyWork in Project Management</a></li><li><a href="/clients">Clients</a></li><li><a href="/testimonials">Testimonial</a></li><li><a href="/terms-of-service">Terms &amp; Condition</a></li><li><a href="/privacy">Privacy Policy</a></li><li><a href="/comment-policy">Comment Policy</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>My Blogs</h4>
                    <ul><li><a href="/dsblog">Data Science Blog</a></li><li><a href="/booksumary">Books/Interviews Blog</a></li><li><a href="/news">AI and Business News</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li><li><a href="/wiaposts">Wisdom in Awareness Blog</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/mychanting">My Chantings</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>All Resources</h4>
                    <ul><li><a href="/datascience-tags#ds-resources">DS Resources</a></li><li><a href="https://aibenchmark-explorer.dasarpai.com">AI Benchmark Explorer</a></li><li><a href="/dsblog/ds-ai-ml-books">Data Science-Books</a></li><li><a href="/dsblog/data-science-cheatsheets">Data Science/AI Cheatsheets</a></li><li><a href="/dsblog/best-youtube-channels-for-ds">Video Channels to Learn DS/AI</a></li><li><a href="/dsblog/ds-ai-ml-interview-resources">DS/AI Interview Questions</a></li><li><a href="https://github.com/dasarpai/DAI-Datasets">GitHub DAI-Datasets</a></li><li><a href="/pmi-templates">PMBOK6 Templates</a></li><li><a href="/prince2-templates">PRINCE2 Templates</a></li><li><a href="/microsoft-pm-templates">Microsoft PM Templates</a></li></ul>
                  </div>
                </div><div class="col-md-3">
                  <div class="td-footer__menu">
                    <h4>Project Management</h4>
                    <ul><li><a href="/pmlogy-home">PMLOGY Home</a></li><li><a href="/pmblog">PMLOGY Blog</a></li><li><a href="/pmglossary">PM Glossary</a></li><li><a href="/pmlogy-tags">PM Topics</a></li><li><a href="/pmbok6-tags">PMBOK6 Topics</a></li><li><a href="/pmbok6-summary">PMBOK6</a></li><li><a href="/pmbok6">PMBOK6 Explorer</a></li><li><a href="/pmbok6hi-tags">PMBOK6 Hindi Topics</a></li><li><a href="/pmbok6hi-summary">PMBoK6 Hindi</a></li><li><a href="/pmbok6hi">PMBOK6 Hindi Explorer</a></li></ul>
                  </div>
                </div></div>
      


      <div class="row"><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Wisdom in Awareness</h4>
                  <ul><li><a href="/wia-home">WIA Home</a></li><li><a href="/wiaposts">WIA Blog</a></li><li><a href="/wia-tags">WIA Topics</a></li><li><a href="/quotations-blog">WIA Quotes</a></li><li><a href="/gk">GK Blog</a></li><li><a href="/gk-tags">GK Topic</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>Samskrutyatra</h4>
                  <ul><li><a href="/samskrutyatra-home">SamskrutYatra Home</a></li><li><a href="/samskrutyatra">Samskrut Blog</a></li><li><a href="/samskrutyatra-tags">Samskrut Topics</a></li><li><a href="/mychanting">My Vedic Chantings</a></li></ul>
                </div>
              </div><div class="col-md-3">
                <div class="td-footer__menu">
                  <h4>My Gallery</h4>
                  <ul><li><a href="/gallary/slider-online-sessions1">Online AI Classes 1</a></li><li><a href="/gallary/slider-online-sessions2">Online AI Classes 2</a></li><li><a href="/gallary/slider-online-sessions3">Online AI Classes 3</a></li><li><a href="/gallary/slider-online-sessions4">Online AI Classes 4</a></li><li><a href="/gallary/slider-pm-selected-photos">Management Classes</a></li><li><a href="/gallary/slider-pm-workshops">PM &amp; DS Workshop</a></li></ul>
                </div>
              </div></div>
    </div>

    <div class="col-2">

    </div>

      
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/js/main.js"></script>
<script src='/js/prism.js'></script>
<script src='/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>
