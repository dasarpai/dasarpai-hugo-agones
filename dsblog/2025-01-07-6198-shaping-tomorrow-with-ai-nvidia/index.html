<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/site/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=site/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/site/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/site/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/site/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence | Agones</title><meta property="og:url" content="http://localhost:1313/site/dsblog/2025-01-07-6198-shaping-tomorrow-with-ai-nvidia/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence">
  <meta property="og:description" content="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence This article is based on various online resources, including articles and YouTube videos, but is heavily influenced by the NVIDIA CES 2025 Keynote Speech by Jensen Huang.
What are tokens in AI, and how do they serve as building blocks of intelligence? In AI, tokens are fundamental units like words or characters that models process to understand and generate human language, serving as the building blocks of intelligence.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dsblog">
    <meta property="article:published_time" content="2025-01-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-07T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="NVIDIA">
    <meta property="article:tag" content="Innovations">

  <meta itemprop="name" content="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence">
  <meta itemprop="description" content="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence This article is based on various online resources, including articles and YouTube videos, but is heavily influenced by the NVIDIA CES 2025 Keynote Speech by Jensen Huang.
What are tokens in AI, and how do they serve as building blocks of intelligence? In AI, tokens are fundamental units like words or characters that models process to understand and generate human language, serving as the building blocks of intelligence.">
  <meta itemprop="datePublished" content="2025-01-07T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-01-07T00:00:00+00:00">
  <meta itemprop="wordCount" content="3735">
  <meta itemprop="keywords" content="Nvidia&#39;s AI innovations in graphics and robotics,Intelligence in GPU architecture for AI development,Nvidia&#39;s AI-powered graphics processing,Robotics and artificial intelligence innovations by Nvidia,Nvidia&#39;s AI-driven innovations in GPU technology,AI in GPU technology for computer vision and natural language processing,Nvidia&#39;s latest advancements in AI and GPU technology">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence">
  <meta name="twitter:description" content="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence This article is based on various online resources, including articles and YouTube videos, but is heavily influenced by the NVIDIA CES 2025 Keynote Speech by Jensen Huang.
What are tokens in AI, and how do they serve as building blocks of intelligence? In AI, tokens are fundamental units like words or characters that models process to understand and generate human language, serving as the building blocks of intelligence.">



<link rel="stylesheet" href="/site/css/prism.css"/>

<link href="/site/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/site/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/site/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/site/dsblog/"><span class="active">Data Science Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/docs/"><span>Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete="off">
</div></div>
</nav>

    </header>
    <div class="container-fluid td-default td-outer">
      <main role="main" class="td-main">
        <p><img src="/assets/images/dspost/dsp6198-shaping-tomorrow-with-ai-nvidia.jpg" alt="Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence"></p>
<h1 id="shaping-tomorrow-with-ai-nvidias-innovations-in-graphics-robotics-and-intelligence">Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence</h1>
<p>This article is based on various online resources, including articles and YouTube videos, but is heavily influenced by the NVIDIA CES 2025 Keynote Speech by Jensen Huang.</p>
<h2 id="what-are-tokens-in-ai-and-how-do-they-serve-as-building-blocks-of-intelligence"><strong>What are tokens in AI, and how do they serve as building blocks of intelligence?</strong></h2>
<p>In AI, tokens are fundamental units like words or characters that models process to understand and generate human language, serving as the building blocks of intelligence.</p>
<h2 id="how-do-tokens-transform-words-into-knowledge-generate-images-and-create-videos"><strong>How do tokens transform words into knowledge, generate images, and create videos?</strong></h2>
<p>Tokens enable AI models to process and interpret text, facilitating the generation of images and videos by understanding and manipulating these textual inputs into embedding (a vector of floating numbers for each token).</p>
<h2 id="what-is-the-role-of-tokens-in-teaching-robots-predicting-dangers-and-finding-cures"><strong>What is the role of tokens in teaching robots, predicting dangers, and finding cures?</strong></h2>
<p>Tokens help robots comprehend instructions and environments, predict potential hazards, and assist in research by processing tokens to identify solutions.</p>
<h2 id="what-is-nvidia"><strong>What is Nvidia&rsquo;s history in AI and GPU innovation since 1993?</strong></h2>
<p>Since its founding in 1993, Nvidia has been a leader in GPU innovation, significantly advancing AI capabilities through its hardware and software solutions.</p>
<h2 id="what-was-nvidia"><strong>What was Nvidia&rsquo;s first programming architecture, and what applications did it support?</strong></h2>
<p>Nvidia&rsquo;s first programming architecture was CUDA (Compute Unified Device Architecture), which supported applications in gaming, professional visualization, and high-performance computing. CUDA is a parallel computing platform and application programming interface (API). Developers typically write CUDA programs using C, C++, or Fortran, extended with CUDA-specific APIs, to perform highly parallel computations by leveraging the GPU&rsquo;s architecture.</p>
<h2 id="how-did"><strong>How did &ldquo;Nvidia’s GPUs&rdquo; evolve from being programmable to supporting AI?</strong></h2>
<p>By integrating parallel processing capabilities it is enabling efficient AI computations.</p>
<h2 id="how-did-alexnet-use-cuda-and-why-is-it-historically-significant"><strong>How did AlexNet use CUDA, and why is it historically significant?</strong></h2>
<p>AlexNet utilized CUDA to accelerate deep learning computations, marking a significant milestone in AI by demonstrating the effectiveness of GPUs in training neural networks. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton used CUDA GPU of NVIDIA on Imagenet dataset to solve the image classification problem in 2012. With GPU, CUDA and Alexnet architecture they achieved top 5 error rate 15.3% which was much better than the second runner up&rsquo;s top 5 error rate 26.2%</p>
<h2 id="what-are-3-stages-of-ai-evolvolution"><strong>What are 3 stages of AI evolvolution?</strong></h2>
<ul>
<li>Perception AI: It was focussed on understanding and interpreting sensory data like images, audio, and video. It is capable of object recognition, facial recognition, speech-to-text conversion. Examples: Virtual assistants (e.g., Siri), computer vision systems, and speech recognition in apps like Google Translate.</li>
<li>Generative AI: It was focussed on creating new content, including text, images, videos, and more. It is capable of generating coherent text, realistic images, deepfake videos, music, and art. Examples: GPT models, DALL-E, Stable Diffusion, and music composition tools like Amper Music.</li>
<li>Agentic AI: It is focussed on decision-making and autonomous action-taking based on goals and contexts. Intelligent agents are capable of reasoning, planning, and adapting dynamically in real-time scenarios. Examples: Autonomous robots, self-driving cars, AI systems managing supply chains, or personal AI agents for scheduling and multitasking.</li>
</ul>
<h2 id="what-is-the-significance-of-googles-transformer-bert-in-ai"><strong>What is the significance of Google’s Transformer (BERT) in AI&rsquo;s evolution?</strong></h2>
<p>Google&rsquo;s Transformer (BERT) introduced a model architecture that significantly improved natural language understanding, advancing AI&rsquo;s capabilities in language processing. It was the first Transformer based architecture. It had attention layers.</p>
<h2 id="what-types-of-information-modalities-can-ai-understand-translate-and-generate"><strong>What types of information modalities can AI understand, translate, and generate?</strong></h2>
<p>AI can understand, translate, and generate various information modalities, including text, images, audio, and video.</p>
<h2 id="what-is-ray-tracing-and-how-does-ai-enhance-its-capabilities-how-it-used-by-nvidia"><strong>What is ray tracing, and how does AI enhance its capabilities? How it used by NVIDIA</strong></h2>
<p>Ray tracing is a rendering technique that simulates the way light interacts with objects to create realistic visual effects such as reflections, refractions, and shadows. It traces the path of light rays as they travel through a scene, calculating how they interact with surfaces to produce highly detailed and lifelike images. Nvidia integrates ray tracing into its RTX GPUs, leveraging AI and dedicated hardware such as RT cores (Ray Tracing cores) and Tensor cores.</p>
<h2 id="what-is-dlss-and-how-does-it-improve-graphical-performance"><strong>What is DLSS, and how does it improve graphical performance?</strong></h2>
<p>DLSS (Deep Learning Super Sampling) is an AI-powered image upscaling and rendering technology developed by Nvidia. It uses deep learning and neural networks to render lower-resolution frames and then upscale them to higher resolutions, delivering high-quality visuals while maintaining or improving performance.</p>
<h2 id="how-does-ai-generate-additional-frames-and-pixels-in-real-time-graphics"><strong>How does AI generate additional frames and pixels in real-time graphics?</strong></h2>
<p>AI generates additional frames and pixels in real-time graphics by predicting and creating intermediate frames, enhancing smoothness and detail in animations.</p>
<h2 id="how-does-ai-enabled-rendering-achieve-efficiency-and-high-performance"><strong>How does AI-enabled rendering achieve efficiency and high performance?</strong></h2>
<p>AI-enabled rendering achieves efficiency and high performance by optimizing rendering processes, reducing computational load, and accelerating image generation.</p>
<h2 id="what-are-the-key-features-of-the-rtx-blackwell-family-of-gpus"><strong>What are the key features of the RTX Blackwell family of GPUs?</strong></h2>
<p>The RTX Blackwell family of GPUs features advanced AI capabilities, enhanced ray tracing performance, and improved energy efficiency.</p>
<h2 id="what-is-neural-rendering-and-why-is-it-significant-for-computer-graphics"><strong>What is neural rendering, and why is it significant for computer graphics?</strong></h2>
<p>Neural rendering is a technique that uses AI to generate realistic images and animations, significantly advancing computer graphics by enabling more lifelike visuals.</p>
<h2 id="what-are-the-new-advancements-in-texture-compression-and-material-shading-using-ai"><strong>What are the new advancements in texture compression and material shading using AI?</strong></h2>
<p>New advancements in texture compression and material shading using AI include more efficient data representation and realistic material rendering, enhancing visual quality and performance.</p>
<h2 id="what-are-the-price-and-performance-comparisons-for-rtx-5070-5090-and-other-gpus"><strong>What are the price and performance comparisons for RTX 5070, 5090, and other GPUs?</strong></h2>
<p>The RTX 5070 offers performance comparable to the RTX 4090 at a lower price point, while the RTX 5090 provides top-tier performance for demanding applications.</p>
<h2 id="how-has-nvidia-brought-high-performance-gpus-into-laptops"><strong>How has Nvidia brought high-performance GPUs into laptops?</strong></h2>
<p>Nvidia has brought high-performance GPUs into laptops by developing mobile versions of their desktop GPUs, balancing power and efficiency for portable computing.</p>
<h2 id="what-is-agentic-ai"><strong>What is Agentic AI?</strong></h2>
<p>Agentic AI is a system of models that interact with users, retrieve information, and perform tasks autonomously. It involves breaking down problems into manageable steps and using various models to generate responses, enhancing the quality of answers through increased computational resources during inference. It can refer to appropriate code library, write its own code, execute, test, evalute, answer and take action.</p>
<h2 id="how-does-nvidia"><strong>How does Nvidia&rsquo;s technology support the development of Agentic AI?</strong></h2>
<p>Nvidia supports the development of Agentic AI by working with software developers to integrate its technology into applications. They provide tools like CUDA libraries and AI libraries that enable new capabilities, facilitating the creation of AI agents that can operate across various platforms and environments.</p>
<h2 id="what-are"><strong>What are <a href="https://developer.nvidia.com/nim">Nvidia Nims</a> and how do they function?</strong></h2>
<p>Nvidia Nims are AI microservices that package complex software and models into containers, making them easier to deploy across different cloud environments. They include models for various applications such as vision, language understanding, and digital biology, allowing integration into existing software systems.</p>
<h2 id="what-is"><strong>What is <a href="https://github.com/NVIDIA/NeMo">Nvidia Nemo</a> and its role in AI onboarding?</strong></h2>
<p>Nvidia Nemo is a framework designed for onboarding and training digital employees (AI agents). It allows organizations to customize these agents by providing examples of desired outputs and feedback during the training process, ensuring that the agents align with specific business processes and vocabulary.</p>
<p>NVIDIA NeMo Framework is a scalable and cloud-native generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (e.g. Automatic Speech Recognition and Text-to-Speech). It enables users to efficiently create, customize, and deploy new generative AI models by leveraging existing code and pre-trained model checkpoints.</p>
<h2 id="how-will-it-departments-change-in-relation-to-ai-agents"><strong>How will IT departments change in relation to AI agents?</strong></h2>
<p>IT departments are expected to evolve into the HR departments for AI agents, managing their onboarding, maintenance, and improvement just as they do for human employees. This shift will involve nurturing digital agents and provisioning them for use within organizations.</p>
<h2 id="what-are-the"><strong>What are the <a href="https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct">Llama nemotron</a> models and their significance for enterprises?</strong></h2>
<p>The Llama neotron models are fine-tuned versions of Meta&rsquo;s Llama models optimized for enterprise use. They are designed to enhance performance in various applications, serving as foundational models for developing specialized AI agents across different industries.</p>
<h2 id="what-types-of-ai-agents-can-be-developed-using-nvidia"><strong>What types of AI agents can be developed using Nvidia&rsquo;s technologies?</strong></h2>
<ol>
<li>
<p><strong>AI Research Assistant Agents</strong>: These agents can process complex documents such as lectures and journals, generating interactive learning materials like podcasts.</p>
</li>
<li>
<p><strong>Software Security AI Agents</strong>: They continuously scan software for vulnerabilities and alert developers about necessary actions to enhance security.</p>
</li>
<li>
<p><strong>Virtual Lab AI Agents</strong>: These agents assist researchers in designing and screening billions of compounds to identify promising drug candidates more efficiently.</p>
</li>
<li>
<p><strong>Analytics AI Agents</strong>: Built on Nvidia&rsquo;s Metropolis blueprint, these agents analyze video content from numerous cameras, enabling interactive search, summarization, and automated reporting.</p>
</li>
<li>
<p><strong>Metropolis Agents</strong>: A specific type of analytics agent that centralizes data from multiple cameras and can reroute workers or robots during incidents.</p>
</li>
<li>
<p><strong>Nvidia Nemo Agents</strong>: Digital employees that are onboarded and trained to work alongside human employees, tailored to specific company processes and vocabularies.</p>
</li>
<li>
<p><strong>Generative AI Agents</strong>: These agents can synthesize images from simple text prompts and assist in creative processes by refining compositions based on 3D objects.</p>
</li>
<li>
<p><strong>Physical AI Agents</strong>: Future agents designed to understand physical dynamics and spatial relationships, capable of performing tasks in the physical world based on action tokens rather than just text.</p>
</li>
</ol>
<h2 id="what-is-physical-ai-and-how-does-it-differ-from-traditional-ai-models"><strong>What is Physical AI and how does it differ from traditional AI models?</strong></h2>
<p>Physical AI refers to systems that understand and interact with the physical world by processing action tokens instead of generating text-based responses. Unlike traditional AI models, which primarily focus on language processing and generating text outputs, Physical AI is designed to comprehend physical dynamics, spatial relationships, and cause-and-effect scenarios. This allows it to perform tasks in the real world, such as picking up objects or navigating environments, by understanding the physical properties of objects and their interactions.</p>
<h2 id="how-do-future-ai-systems-integrate-with-physical-environments"><strong>How do future AI systems integrate with physical environments?</strong></h2>
<p>Future AI systems are expected to integrate with physical environments by utilizing models that understand the language of the world, including concepts like gravity, friction, and inertia. These systems will rely on a world model that processes input from the environment (like visual data) and translates it into actionable tasks (action tokens), enabling robots or AI agents to interact effectively with their surroundings.</p>
<h2 id="what-capabilities-do-nvidia"><strong>What capabilities do Nvidia&rsquo;s models provide for software developers?</strong></h2>
<p>Nvidia&rsquo;s models offer software developers capabilities such as:</p>
<ul>
<li>Access to optimized AI microservices (Nvidia Nims) for easy deployment across various cloud environments.</li>
<li>Tools for creating specialized AI agents tailored to specific business processes.</li>
<li>A framework (Nvidia Nemo) for onboarding and training digital employees (AI agents) that can work alongside human staff.</li>
<li>Pre-trained models for various applications, including vision, language understanding, and analytics.</li>
<li>Support for integrating these models into existing software packages, enhancing functionality and performance.</li>
</ul>
<h2 id="how-can-businesses-leverage-nvidia"><strong>How can businesses leverage Nvidia&rsquo;s AI technologies for operational efficiency?</strong></h2>
<p>Businesses can leverage Nvidia&rsquo;s AI technologies by:</p>
<ul>
<li>Integrating AI agents into their workflows to automate repetitive tasks and enhance productivity.</li>
<li>Utilizing specialized models for specific applications, such as software security monitoring or research assistance.</li>
<li>Onboarding digital employees through frameworks like Nvidia Nemo to improve training processes and ensure alignment with company-specific practices.</li>
<li>Deploying analytics agents that can analyze large volumes of data from various sources (like video feeds) to generate insights and recommendations for operational improvements.</li>
<li>Taking advantage of Nvidia&rsquo;s open-source blueprints to customize AI solutions tailored to their unique needs.</li>
</ul>
<h2 id="how-does-ai-democratize-technology-for-the-masses"><strong>How does AI democratize technology for the masses?</strong></h2>
<p>AI democratizes technology by making advanced tools and capabilities accessible to a broader audience, enabling more people to benefit from technological advancements.</p>
<h2 id="what-is-the-future-of-computer-graphics-with-neural-rendering"><strong>What is the future of computer graphics with neural rendering?</strong></h2>
<p>The future of computer graphics with neural rendering involves more realistic and efficient image generation, transforming industries</p>
<h2 id="why-do-we-need-humanoid-robots"><strong>Why do we need humanoid robots?</strong></h2>
<p>Humanoid robots are designed to adapt quickly to human-centric environments, assisting in tasks that are repetitive, physically demanding, or hazardous. They can alleviate labor shortages by automating processes in sectors like manufacturing and healthcare.</p>
<h2 id="what-are-the-major-challenges-in-building-humanoid-robots"><strong>What are the major challenges in building humanoid robots?</strong></h2>
<p>Key challenges include replicating human-like perception, dexterity, mobility, cognition, and whole-body control. Achieving seamless collaboration with humans and other machines requires advancements in AI, machine learning, sensor technologies, and mechatronics.</p>
<h2 id="why-cant-we-use-large-language-models-for-robots"><strong>Why can’t we use large language models for robots?</strong></h2>
<p>Large language models (LLMs) are primarily designed for processing and generating text. Robots require models that can understand and interact with the physical world, including perception, manipulation, and navigation, which LLMs are not specifically trained for.</p>
<h2 id="what-is-a-world-model-and-why-is-it-necessary-for-robots"><strong>What is a world model, and why is it necessary for robots?</strong></h2>
<p>A world model is a representation that allows robots to understand and predict the dynamics of their environment. It&rsquo;s essential for tasks like planning, decision-making, and adapting to new situations, enabling robots to operate effectively in the real world.</p>
<h2 id="how-does-a-world-foundation-model-differ-from-large-language-models"><strong>How does a world foundation model differ from large language models?</strong></h2>
<p>World foundation models are designed to understand and predict physical environments, incorporating sensory data and physics-based simulations. In contrast, LLMs focus on language processing and do not inherently understand physical dynamics.</p>
<h2 id="how-can-robots-help-solve-the-labor-shortage-crisis"><strong>How can robots help solve the labor shortage crisis?</strong></h2>
<p>By automating tasks in industries facing labor shortages, such as manufacturing and healthcare, robots can perform repetitive and physically demanding jobs, allowing human workers to focus on more complex and creative tasks.</p>
<h2 id="what-is-1"><strong>What is &ldquo;Isaac GR00T&rdquo;, and how does it help in training robots?</strong></h2>
<p>Isaac GR00T is NVIDIA&rsquo;s general-purpose foundation model for humanoid robots. It enables robots to learn tasks through imitation learning by observing human actions, facilitating the development of skills like coordination and dexterity.</p>
<h2 id="how-do"><strong>How do &ldquo;virtual environments&rdquo; accelerate robot training? Can you tell some technologies and examples?</strong></h2>
<p>Virtual environments, like NVIDIA&rsquo;s &ldquo;Isaac Sim&rdquo; and Omniverse, provide realistic simulations where robots can practice tasks without physical constraints. These platforms allow for rapid iteration and testing, reducing the time and cost associated with real-world training.</p>
<h2 id="what-is-nvidia-omniverse"><strong>What is NVIDIA OmniVerse?</strong></h2>
<p>NVIDIA Omniverse is a powerful, open platform designed for 3D simulation and collaborative design, enabling individuals and teams to create, simulate, and optimize 3D content in real-time. It integrates tools and workflows across various industries, including gaming, film, architecture, engineering, manufacturing, and robotics.</p>
<h3 id="key-features-of-nvidia-omniverse">Key Features of NVIDIA Omniverse:</h3>
<ol>
<li>
<p><strong>Real-Time Collaboration:</strong></p>
<ul>
<li>Allows multiple users to work simultaneously on 3D projects in a shared virtual space.</li>
<li>Supports live updates, enabling seamless collaboration across teams.</li>
</ul>
</li>
<li>
<p><strong>Universal Scene Description (USD):</strong></p>
<ul>
<li>Built on Pixar&rsquo;s USD framework, which acts as a common language for 3D data exchange.</li>
<li>Enables interoperability across different 3D design tools like Autodesk Maya, Blender, and Adobe Substance.</li>
</ul>
</li>
<li>
<p><strong>Physically Accurate Simulations:</strong></p>
<ul>
<li>Powered by NVIDIA RTX technology, offering realistic rendering with ray tracing and path tracing.</li>
<li>Provides real-world physics simulation for materials, lighting, and dynamics.</li>
</ul>
</li>
<li>
<p><strong>AI Integration:</strong></p>
<ul>
<li>Includes AI tools for automating repetitive tasks, enhancing productivity, and generating assets (e.g., textures or environments).</li>
<li>Features Omniverse Audio2Face, an AI tool for animating facial expressions from audio input.</li>
</ul>
</li>
<li>
<p><strong>Scalable Infrastructure:</strong></p>
<ul>
<li>Can run on individual workstations, enterprise data centers, or cloud services.</li>
<li>Compatible with NVIDIA GPUs for optimal performance.</li>
</ul>
</li>
<li>
<p><strong>Digital Twin Creation:</strong></p>
<ul>
<li>Enables building digital twins of real-world environments, devices, or processes.</li>
<li>Facilitates simulation, testing, and optimization before physical implementation.</li>
</ul>
</li>
<li>
<p><strong>Extensibility:</strong></p>
<ul>
<li>Developers can create custom extensions or applications using the Omniverse Kit.</li>
<li>Supports scripting and integration with other software pipelines.</li>
</ul>
</li>
<li>
<p><strong>Industry Applications:</strong></p>
<ul>
<li><strong>Entertainment:</strong> Virtual production, visual effects, and animation.</li>
<li><strong>Manufacturing:</strong> Designing and simulating factory layouts and robotics.</li>
<li><strong>Architecture:</strong> Real-time visualization and design collaboration.</li>
<li><strong>Robotics:</strong> Training AI models in simulated environments.</li>
</ul>
</li>
</ol>
<h3 id="popular-omniverse-applications">Popular Omniverse Applications:</h3>
<ul>
<li><strong>Omniverse Create:</strong> For designing and visualizing 3D environments.</li>
<li><strong>Omniverse View:</strong> For high-quality rendering and presentation of 3D content.</li>
<li><strong>Omniverse Machinima:</strong> For creating cinematic animations using game assets.</li>
<li><strong>Omniverse Isaac:</strong> For robotics simulation and training.</li>
</ul>
<p><strong>Side Note</strong><br>
The metaverse is a virtual world where people can interact with each other and the environment using virtual reality (VR), augmented reality (AR), and other technologies. It&rsquo;s a place where people can work, play, learn, and shop.</p>
<p>The multiverse is a hypothetical collection of all universes, including the space, time, matter, energy, and physical laws that exist within them. The term is used to describe the idea that there may be other universes beyond the observable universe that we can observe.</p>
<p>Omniverse is a concept that consists of everything from Multiverses to Metaverses. It is the largest existing concept in the technological world that includes all elements of Multiverses and Metaveverses.</p>
<h2 id="what-is-nvidia-cosmos"><strong>What is NVIDIA Cosmos?</strong></h2>
<p>NVIDIA Cosmos is a platform designed to accelerate the development of physical AI systems, such as autonomous vehicles and robots. It offers a suite of generative world foundation models (WFMs) capable of producing realistic, physics-aware video simulations. These simulations are essential for training AI models, enabling them to understand and navigate real-world environments more effectively.</p>
<p>Introduced at CES 2025, Cosmos provides developers with open access to these advanced models under NVIDIA&rsquo;s permissive open-source license. This approach democratizes AI technology, allowing researchers and developers to utilize these tools without significant entry costs.</p>
<p>Key features of NVIDIA Cosmos include:</p>
<ul>
<li>
<p><strong>Physics-Aware Video Generation</strong>: The platform can generate realistic simulations of the physical world, crucial for training robots and autonomous vehicles.</p>
</li>
<li>
<p><strong>Synthetic Data Generation</strong>: Cosmos can create vast amounts of synthetic data to augment real-world datasets, improving the training of AI agents.</p>
</li>
<li>
<p><strong>Simulation and Testing</strong>: It enables developers to test and debug their AI models in virtual environments before deploying them in the real world.</p>
</li>
<li>
<p><strong>Reinforcement Learning</strong>: The models can be used for reinforcement learning, allowing AI agents to learn and improve their performance in virtual worlds.</p>
</li>
</ul>
<p>By leveraging Cosmos, developers can accelerate the creation and deployment of AI systems, reducing reliance on costly real-world testing and enhancing the safety and efficiency of autonomous technologies.</p>
<h2 id="what-is-the"><strong>What is the &ldquo;sim-to-real&rdquo; (transferring virtual training to real-world) gap, and how can it be addressed?</strong></h2>
<p>The &ldquo;sim-to-real&rdquo; gap refers to the challenges robots face when applying skills learned in simulation to real-world scenarios. Techniques like domain randomization, where varied virtual conditions are simulated, help robots generalize their learning to real environments.</p>
<h2 id="what-role-do"><strong>What role do &ldquo;digital twins&rdquo; play in training robots?</strong></h2>
<p>Digital twins are virtual replicas of physical systems. In robotics, they allow for testing and training in a simulated environment that mirrors the real world, enabling safe experimentation and optimization before deployment.</p>
<h2 id="how-does"><strong>How does &ldquo;parallel training&rdquo; in &ldquo;virtual worlds&rdquo; work?</strong></h2>
<p>Parallel training involves running multiple simulations simultaneously, allowing robots to learn from diverse scenarios and conditions. This approach accelerates the learning process and enhances the robot&rsquo;s ability to handle various real-world situations.</p>
<h2 id="what-is-2"><strong>What is &ldquo;NVIDIA Drive AI&rdquo;, and how does it improve autonomous vehicles?</strong></h2>
<p>NVIDIA Drive AI is a platform that provides the computing power and software tools necessary for developing autonomous vehicles. It enables vehicles to process sensor data, make real-time decisions, and navigate safely.</p>
<h2 id="how-does-ai-improve-the-safety-of-autonomous-vehicles"><strong>How does AI improve the safety of autonomous vehicles?</strong></h2>
<p>AI enhances safety by enabling autonomous vehicles to perceive their environment, predict potential hazards, and make informed decisions. Advanced algorithms process data from sensors to detect obstacles, traffic signals, and pedestrians, reducing the risk of accidents.</p>
<h2 id="what-are-some-examples-of-partnerships-driving-autonomous-vehicle-advancements"><strong>What are some examples of partnerships driving autonomous vehicle advancements?</strong></h2>
<p>NVIDIA has partnered with companies like Toyota, Aurora, and Continental to develop autonomous vehicle technologies. These collaborations leverage NVIDIA&rsquo;s AI platforms to advance self-driving capabilities.</p>
<h2 id="what-is-the-relation-between-nvidia-architecture-names-and-gpu-names">What is the relation between NVIDIA architecture names and GPU names?</h2>
<p>NVIDIA typically develops a single GPU architecture and uses it across all the GPUs in a particular series, maintaining consistency within that generation. NVIDIA has developed several GPU architectures over the years, each introducing advancements in performance, efficiency, and features.</p>
<p>Each architecture is a blueprint for designing GPUs. It defines the underlying features, technologies, and performance improvements that all GPUs in a series share.</p>
<p>GPUs in a series (e.g., RTX 4090, RTX 4070, etc.) share the same architecture but vary in performance, core count, power consumption, and pricing to target different segments (e.g., gaming, professional workloads, budget users).</p>
<p>Here’s a list of NVIDIA’s major GPU architectures, along with their key innovations:</p>
<hr>
<h3 id="1-ada-lovelace-rtx-40-series"><strong>1. Ada Lovelace (RTX 40 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2022</li>
<li><strong>Key Features</strong>:
<ul>
<li>Introduced <strong>4th-gen Tensor Cores</strong> and <strong>3rd-gen Ray Tracing Cores</strong>.</li>
<li><strong>DLSS 3</strong> with AI-generated frames for improved gaming performance.</li>
<li>Improved efficiency and performance compared to the previous <strong>Ampere</strong> architecture.</li>
<li>Enhanced support for <strong>real-time ray tracing</strong>.</li>
</ul>
</li>
</ul>
<h3 id="2-ampere-rtx-30-series"><strong>2. Ampere (RTX 30 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2020</li>
<li><strong>Key Features</strong>:
<ul>
<li><strong>2nd-gen Ray Tracing Cores</strong> and <strong>3rd-gen Tensor Cores</strong> for better AI performance.</li>
<li>Significant boost in CUDA cores, improving both gaming and professional workloads.</li>
<li>Supported <strong>DLSS 2.0</strong>, enabling AI-driven upscaling for high-resolution gaming.</li>
<li>High performance for AI tasks and machine learning.</li>
</ul>
</li>
</ul>
<h3 id="3-turing-rtx-20-series-gtx-16-series"><strong>3. Turing (RTX 20 Series, GTX 16 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2018</li>
<li><strong>Key Features</strong>:
<ul>
<li>Introduced <strong>real-time ray tracing</strong> with <strong>RT Cores</strong> and <strong>Tensor Cores</strong>.</li>
<li>Enabled <strong>DLSS (Deep Learning Super Sampling)</strong> for the first time.</li>
<li>Aimed at AI and gaming convergence, offering better AI-based graphics rendering.</li>
<li>Focused on hybrid rendering with rasterization and ray tracing.</li>
</ul>
</li>
</ul>
<h3 id="4-volta"><strong>4. Volta</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2017</li>
<li><strong>Key Features</strong>:
<ul>
<li>Primarily targeted at data centers, AI research, and HPC (high-performance computing).</li>
<li>Introduced <strong>Tensor Cores</strong>, designed specifically for AI and deep learning workloads.</li>
<li>Not used in mainstream gaming GPUs but pivotal for AI-based applications like training neural networks.</li>
</ul>
</li>
</ul>
<h3 id="5-pascal-gtx-10-series"><strong>5. Pascal (GTX 10 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2016</li>
<li><strong>Key Features</strong>:
<ul>
<li>Significant performance improvements over Maxwell, with better energy efficiency.</li>
<li>Focused on gaming and professional markets.</li>
<li>Supported <strong>Simultaneous Multi-Projection (SMP)</strong>, allowing VR applications to run more efficiently.</li>
</ul>
</li>
</ul>
<h3 id="6-maxwell-gtx-900-series"><strong>6. Maxwell (GTX 900 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2014</li>
<li><strong>Key Features</strong>:
<ul>
<li>Improved power efficiency and better performance than the previous Kepler architecture.</li>
<li>Introduced <strong>Voxel Global Illumination (VXGI)</strong> for improved lighting and shadows.</li>
<li>First GPUs to support HDMI 2.0.</li>
</ul>
</li>
</ul>
<h3 id="7-kepler-gtx-600--700-series"><strong>7. Kepler (GTX 600 &amp; 700 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2012</li>
<li><strong>Key Features</strong>:
<ul>
<li>Introduced dynamic parallelism for improved GPU computing performance.</li>
<li>Focused on power efficiency and scalability.</li>
<li>Used for gaming and professional workloads.</li>
</ul>
</li>
</ul>
<h3 id="8-fermi-gtx-400--500-series"><strong>8. Fermi (GTX 400 &amp; 500 Series)</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2010</li>
<li><strong>Key Features</strong>:
<ul>
<li>Redesigned architecture for better general-purpose GPU computing (GPGPU).</li>
<li>Improved double-precision performance and introduced CUDA cores.</li>
<li>Used widely in both gaming and professional applications.</li>
</ul>
</li>
</ul>
<h3 id="9-tesla"><strong>9. Tesla</strong></h3>
<ul>
<li><strong>Release Year</strong>: 2008</li>
<li><strong>Key Features</strong>:
<ul>
<li>First architecture designed with <strong>CUDA</strong> in mind, enabling GPU programming for developers.</li>
<li>Primarily focused on high-performance computing and AI research.</li>
</ul>
</li>
</ul>
<h3 id="summary-of-architectures-and-series">Summary of Architectures and Series</h3>
<table>
  <thead>
      <tr>
          <th><strong>Architecture</strong></th>
          <th><strong>Key GPUs</strong></th>
          <th><strong>Main Focus</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Tesla (earliest)</td>
          <td>GTX 200 Series</td>
          <td>Early CUDA and GPGPU focus</td>
      </tr>
      <tr>
          <td>Fermi</td>
          <td>GTX 400 &amp; 500 Series</td>
          <td>General-purpose GPU computing</td>
      </tr>
      <tr>
          <td>Kepler</td>
          <td>GTX 600 &amp; 700 Series</td>
          <td>Energy efficiency and gaming</td>
      </tr>
      <tr>
          <td>Maxwell</td>
          <td>GTX 900 Series</td>
          <td>Power efficiency, VR support</td>
      </tr>
      <tr>
          <td>Pascal</td>
          <td>GTX 10 Series</td>
          <td>Gaming and professional workloads</td>
      </tr>
      <tr>
          <td>Volta</td>
          <td>Tesla V100</td>
          <td>AI and HPC workloads</td>
      </tr>
      <tr>
          <td>Turing</td>
          <td>RTX 20 Series</td>
          <td>AI, Ray Tracing</td>
      </tr>
      <tr>
          <td>Ampere</td>
          <td>RTX 30 Series</td>
          <td>AI, DLSS, Ray Tracing</td>
      </tr>
      <tr>
          <td>Ada Lovelace</td>
          <td>RTX 40 Series</td>
          <td>AI-Generated Frames (DLSS 3)</td>
      </tr>
      <tr>
          <td>Blackwell (latest)</td>
          <td>RTX 50 Series</td>
          <td>AI integration, next-gen GPUs</td>
      </tr>
  </tbody>
</table>
<p>Each architecture marks a milestone in NVIDIA’s journey to bridge gaming, AI, and high-performance computing.</p>

      </main>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/site/js/main.js"></script>
<script src='/site/js/prism.js'></script>
<script src='/site/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/site/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>