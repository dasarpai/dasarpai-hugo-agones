<!doctype html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en" class="no-js">
  <head><script src="/site/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=site/livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.147.0">

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">



<link rel="shortcut icon" href="/site/favicons/favicon.ico?v=1" >
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<link rel="icon" type="image/png" href="/site/favicons/favicon-16x16.png?v=1" sizes="16x16">
<link rel="icon" type="image/png" href="/site/favicons/favicon-32x32.png?v=1" sizes="32x32">
<link rel="apple-touch-icon" href="/site/favicons/apple-touch-icon-180x180.png?v=1" sizes="180x180">
<title>300 Important Statistical Terms | Agones</title><meta property="og:url" content="http://localhost:1313/site/docs/dsblog/2021-09-29-6011-300-important-statistical-terms/">
  <meta property="og:site_name" content="Agones">
  <meta property="og:title" content="300 Important Statistical Terms">
  <meta property="og:description" content="Important Statistical Terms Sno Term Definition 1 Affine transformation. See transformation. 2 Affirming the antecedent. A valid logical argument that concludes from the premise A → B and the premise A that therefore, B is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the antecedent (A) in the conditional. 3 Affirming the consequent. A logical fallacy that argues from the premise A → B and the premise B that therefore, A is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the consequent (B) in the conditional. 4 Alternative Hypothesis. In hypothesis testing, a null hypothesis (typically that there is no effect) is compared with an alternative hypothesis (typically that there is an effect, or that there is an effect of a particular sign). For example, in evaluating whether a new cancer remedy works, the null hypothesis typically would be that the remedy does not work, while the alternative hypothesis would be that the remedy does work. When the data are sufficiently improbable under the assumption that the null hypothesis is true, the null hypothesis is rejected in favor of the alternative hypothesis. (This does not imply that the data are probable under the assumption that the alternative hypothesis is true, nor that the null hypothesis is false, nor that the alternative hypothesis is true. Confused? Take a course in Statistics!) 5 Ante. The up-front cost of a bet: the money you must pay to play the game. From Latin for “before.” 6 Antecedent. In a conditional p → q, the antecedent is p. 7 Appeal to Ignorance. A logical fallacy: taking the absence of evidence to be evidence of absence. If something is not known to be false, assume that it is true; or if something is not known to be true, assume that it is false. For example, if I have no reason to think that anyone in Tajikistan wish me well, that is not evidence that nobody in Tajikistan wishes me well. 8 Association. Two variables are associated if some of the variability of one can be accounted for by the other. In a scatterplot of the two variables, if the scatter in the values of the variable plotted on the vertical axis is smaller in narrow ranges of the variable plotted on the horizontal axis (i.e., in vertical “slices”) than it is overall, the two variables are associated. The correlation coefficient is a measure of linear association, which is a special case of association in which large values of one variable tend to occur with large values of the other, and small values of one tend to occur with small values of the other (positive association), or in which large values of one tend to occur with small values of the other, and vice versa (negative association). 9 Average. An ambiguous term. It often denotes the arithmetic mean, but it can also denote the median, the mode, the geometric mean, and weighted means, among other things. Beware if something reports “the average” without making it clear which average. 10 Axioms of Probability. There are three axioms of probability: (1) Chances are always at least zero. (2) The chance that something happens is 100%. (3) If two events cannot both occur at the same time (if they are disjoint or mutually exclusive), the chance that either one occurs is the sum of the chances that each occurs. For example, consider an experiment that consists of tossing a coin once. The first axiom says that the chance that the coin lands heads, for instance, must be at least zero. The second axiom says that the chance that the coin either lands heads or lands tails or lands on its edge or doesn’t land at all is 100%. The third axiom says that the chance that the coin either lands heads or lands tails is the sum of the chance that the coin lands heads and the chance that the coin lands tails, because both cannot occur in the same coin toss. All other mathematical facts about probability can be derived from these three axioms. For example, it is true that the chance that an event does not occur is (100% − the chance that the event occurs). This is a consequence of the second and third axioms. 11 Base rate fallacy. The base rate fallacy consists of failing to take into account prior probabilities (base rates) when computing conditional probabilities from other conditional probabilities. It is related to the Prosecutor’s Fallacy. For instance, suppose that a test for the presence of some condition has a 1% chance of a false positive result (the test says the condition is present when it is not) and a 1% chance of a false negative result (the test says the condition is absent when the condition is present), so the exam is 99% accurate. What is the chance that an item that tests positive really has the condition? The intuitive answer is 99%, but that is not necessarily true: the correct answer depends on the fraction f of items in the population that have the condition (and on whether the item tested is selected at random from the population). The chance that a randomly selected item tests positive is 0.99×f/(0.99×f &#43; 0.01×(1−f)), which could be much smaller than 99% if f is small. See Bayes’ Rule. 12 Bayes’ Rule. Bayes’ rule expresses the conditional probability of the event A given the event B in terms of the conditional probability of the event B given the event A and the unconditional probability of A: P(A/B) = P(B/A) ×P(A)/( P(B/A)×P(A) &#43; P(B/Ac) ×P(Ac) ). In this expression, the unconditional probability of A is also called the prior probability of A, because it is the probability assigned to A prior to observing any data. Similarly, in this context, P(A/B) is called the posterior probability of A given B, because it is the probability of A updated to reflect (i.e., to condition on) the fact that B was observed to occur. 13 Bernoulli’s Inequality. The Bernoulli Inequality says that if x ≥ −1 then (1&#43;x)n ≥ 1 &#43; nx for every integer n ≥ 0. If n is even, the inequality holds for all x. 14 Bias. A measurement procedure or estimator is said to be biased if, on the average, it gives an answer that differs from the truth. The bias is the average (expected) difference between the measurement and the truth. For example, if you get on the scale with clothes on, that biases the measurement to be larger than your true weight (this would be a positive bias). The design of an experiment or of a survey can also lead to bias. Bias can be deliberate, but it is not necessarily so. See also nonresponse bias. 15 Bimodal. Having two modes. 16 Bin. See class interval. 17 Binomial Coefficient. See combinations. 18 Binomial Distribution. A random variable has a binomial distribution (with parameters n and p) if it is the number of “successes” in a fixed number n of independent random trials, all of which have the same probability p of resulting in “success.” Under these assumptions, the probability of k successes (and n−k failures) is nCk pk(1−p)n−k, where nCk is the number of combinations of n objects taken k at a time: nCk = n!/(k!(n−k)!). The expected value of a random variable with the Binomial distribution is n×p, and the standard error of a random variable with the Binomial distribution is (n×p×(1 − p))½. This page shows the probability histogram of the binomial distribution. 19 Binomial Theorem. The Binomial theorem says that (x&#43;y)n = xn &#43; nxn−1y &#43; … &#43; nCkxn−kyk &#43; … &#43; yn. 20 Bivariate. Having or having to do with two variables. For example, bivariate data are data where we have two measurements of each “individual.” These measurements might be the heights and weights of a group of people (an “individual” is a person), the heights of fathers and sons (an “individual” is a father-son pair), the pressure and temperature of a fixed volume of gas (an “individual” is the volume of gas under a certain set of experimental conditions), etc. Scatterplots, the correlation coefficient, and regression make sense for bivariate data but not univariate data. C.f. univariate. 21 Blind, Blind Experiment. In a blind experiment, the subjects do not know whether they are in the treatment group or the control group. In order to have a blind experiment with human subjects, it is usually necessary to administer a placebo to the control group. 22 Bootstrap estimate of Standard Error. The name for this idea comes from the idiom “to pull oneself up by one’s bootstraps,” which connotes getting out of a hole without anything to stand on. The idea of the bootstrap is to assume, for the purposes of estimating uncertainties, that the sample is the population, then use the SE for sampling from the sample to estimate the SE of sampling from the population. For sampling from a box of numbers, the SD of the sample is the bootstrap estimate of the SD of the box from which the sample is drawn. For sample percentages, this takes a particularly simple form: the SE of the sample percentage of n draws from a box, with replacement, is SD(box)/n½, where for a box that contains only zeros and ones, SD(box) = ((fraction of ones in box)×(fraction of zeros in box) )½. The bootstrap estimate of the SE of the sample percentage consists of estimating SD(box) by ((fraction of ones in sample)×(fraction of zeros in sample))½. When the sample size is large, this approximation is likely to be good. 23 Box model. An analogy between an experiment and drawing numbered tickets “at random” from a box with replacement. For example, suppose we are trying to evaluate a cold remedy by giving it or a placebo to a group of n individuals, randomly choosing half the individuals to receive the remedy and half to receive the placebo. Consider the median time to recovery for all the individuals (we assume everyone recovers from the cold eventually; to simplify things, we also assume that no one recovered in exactly the median time, and that n is even). By definition, half the individuals got better in less than the median time, and half in more than the median time. The individuals who received the treatment are a random sample of size n/2 from the set of n subjects, half of whom got better in less than median time, and half in longer than median time. If the remedy is ineffective, the number of subjects who received the remedy and who recovered in less than median time is like the sum of n/2 draws with replacement from a box with two tickets in it: one with a “1” on it, and one with a “0” on it. This page illustrates the sampling distribution of random draws with or without from a box of numbered tickets. 24 Breakdown Point. The breakdown point of an estimator is the smallest fraction of observations one must corrupt to make the estimator take any value one wants. 25 Categorical Variable. A variable whose value ranges over categories, such as {red, green, blue}, {male, female}, {Arizona, California, Montana, New York}, {short, tall}, {Asian, African-American, Caucasian, Hispanic, Native American, Polynesian}, {straight, curly}, etc. Some categorical variables are ordinal. The distinction between categorical variables and qualitative variables is a bit blurry. C.f. quantitative variable. 26 Causation, causal relation. Two variables are causally related if changes in the value of one cause the other to change. For example, if one heats a rigid container filled with a gas, that causes the pressure of the gas in the container to increase. Two variables can be associated without having any causal relation, and even if two variables have a causal relation, their correlation can be small or zero. 27 Central Limit Theorem. The central limit theorem states that the probability histograms of the sample mean and sample sum of n draws with replacement from a box of labeled tickets converge to a normal curve as the sample size n grows, in the following sense: As n grows, the area of the probability histogram for any range of values approaches the area under the normal curve for the same range of values, converted to standard units. See also the normal approximation. 28 Certain Event. An event is certain if its probability is 100%. Even if an event is certain, it might not occur. However, by the complement rule, the chance that it does not occur is 0%. 29 Chance variation, chance error. A random variable can be decomposed into a sum of its expected value and chance variation around its expected value. The expected value of the chance variation is zero; the standard error of the chance variation is the same as the standard error of the random variable—the size of a “typical” difference between the random variable and its expected value. See also sampling error. 30 Change of Units or Variables. See also transformation. 31 Chebychev’s Inequality. For lists: For every number k&gt;0, the fraction of elements in a list that are k SD’s or further from the arithmetic mean of the list is at most 1/k2. For random variables: For every number k&gt;0, the probability that a random variable X is k SEs or further from its expected value is at most 1/k2. 32 Chi-square curve. The chi-square curve is a family of curves that depend on a parameter called degrees of freedom (d.f.). The chi-square curve is an approximation to the probability histogram of the chi-square statistic for multinomial model if the expected number of outcomes in each category is large. The chi-square curve is positive, and its total area is 100%, so we can think of it as the probability histogram of a random variable. The balance point of the curve is d.f., so the expected value of the corresponding random variable would equal d.f.. The standard error of the corresponding random variable would be (2×d.f.)½. As d.f. grows, the shape of the chi-square curve approaches the shape of the normal curve. This page shows the chi-square curve. 33 Chi-square Statistic. The chi-square statistic is used to measure the agreement between categorical data and a multinomial model that predicts the relative frequency of outcomes in each possible category. Suppose there are n independent trials, each of which can result in one of k possible outcomes. Suppose that in each trial, the probability that outcome i occurs is pi, for i = 1, 2, … , k, and that these probabilities are the same in every trial. The expected number of times outcome 1 occurs in the n trials is n×p1; more generally, the expected number of times outcome i occurs is expectedi = n×pi. If the model be correct, we would expect the n trials to result in outcome i about n×pi times, give or take a bit. Let observedi denote the number of times an outcome of type i occurs in the n trials, for i = 1, 2, … , k. The chi-squared statistic summarizes the discrepancies between the expected number of times each outcome occurs (assuming that the model is true) and the observed number of times each outcome occurs, by summing the squares of the discrepancies, normalized by the expected numbers, over all the categories: chi-squared = (observed1 − expected1)2/expected1 &#43; (observed2 − expected2)2/expected2 &#43; … &#43; (observedk − expectedk)2/expectedk. As the sample size n increases, if the model is correct, the sampling distribution of the chi-squared statistic is approximated increasingly well by the chi-squared curve with (#categories − 1) = k − 1 degrees of freedom (d.f.), in the sense that the chance that the chi-squared statistic is in any given range grows closer and closer to the area under the Chi-Squared curve over the same range. This page illustrates the sampling distribution of the chi-square statistic. 34 Class Boundary. A point that is the left endpoint of one class interval, and the right endpoint of another class interval. 35 Class Interval. In plotting a histogram, one starts by dividing the range of values into a set of non-overlapping intervals, called class intervals, in such a way that every datum is contained in some class interval. See the related entries class boundary and endpoint convention. 36 Cluster Sample. In a cluster sample, the sampling unit is a collection of population units, not single population units. For example, techniques for adjusting the U.S. census start with a sample of geographic blocks, then (try to) enumerate all inhabitants of the blocks in the sample to obtain a sample of people. This is an example of a cluster sample. (The blocks are chosen separately from different strata, so the overall design is a stratified cluster sample.) 37 Combinations. The number of combinations of n things taken k at a time is the number of ways of picking a subset of k of the n things, without replacement, and without regard to the order in which the elements of the subset are picked. The number of such combinations is nCk = n!/(k!(n−k)!), where k! (pronounced “k factorial”) is k×(k−1)×(k−2)× … × 1. The numbers nCk are also called the Binomial coefficients. From a set that has n elements one can form a total of 2n subsets of all sizes. For example, from the set {a, b, c}, which has 3 elements, one can form the 23 = 8 subsets {}, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}. Because the number of subsets with k elements one can form from a set with n elements is nCk, and the total number of subsets of a set is the sum of the numbers of possible subsets of each size, it follows that nC0&#43;nC1&#43;nC2&#43; … &#43;nCn = 2n. The calculator has a button (nCm) that lets you compute the number of combinations of m things chosen from a set of n things. To use the button, first type the value of n, then push the nCm button, then type the value of m, then press the “=” button. 38 Complement. The complement of a subset of a given set is the collection of all elements of the set that are not elements of the subset. 39 Complement rule. The probability of the complement of an event is 100% minus the probability of the event: P(Ac) = 100% − P(A). 40 Compound proposition. A logical proposition formed from other propositions using logical operations such as !, /, XOR, &amp;, → and ↔. 41 Conditional Probability. Suppose we are interested in the probability that some event A occurs, and we learn that the event B occurred. How should we update the probability of A to reflect this new knowledge? This is what the conditional probability does: it says how the additional knowledge that B occurred should affect the probability that A occurred quantitatively. For example, suppose that A and B are mutually exclusive. Then if B occurred, A did not, so the conditional probability that A occurred given that B occurred is zero. At the other extreme, suppose that B is a subset of A, so that A must occur whenever B does. Then if we learn that B occurred, A must have occurred too, so the conditional probability that A occurred given that B occurred is 100%. For in-between cases, where A and B intersect, but B is not a subset of A, the conditional probability of A given B is a number between zero and 100%. Basically, one “restricts” the outcome space S to consider only the part of S that is in B, because we know that B occurred. For A to have happened given that B happened requires that AB happened, so we are interested in the event AB. To have a legitimate probability requires that P(S) = 100%, so if we are restricting the outcome space to B, we need to divide by the probability of B to make the probability of this new S be 100%. On this scale, the probability that AB happened is P(AB)/P(B). This is the definition of the conditional probability of A given B, provided P(B) is not zero (division by zero is undefined). Note that the special cases AB = {} (A and B are mutually exclusive) and AB = B (B is a subset of A) agree with our intuition as described at the top of this paragraph. Conditional probabilities satisfy the axioms of probability, just as ordinary probabilities do. 42 Confidence Interval. A confidence interval for a parameter is a random interval constructed from data in such a way that the probability that the interval contains the true value of the parameter can be specified before the data are collected. Confidence intervals are demonstrated in this page. 43 Confidence Level. The confidence level of a confidence interval is the chance that the interval that will result once data are collected will contain the corresponding parameter. If one computes confidence intervals again and again from independent data, the long-term limit of the fraction of intervals that contain the parameter is the confidence level. 44 Confounding. When the differences between the treatment and control groups other than the treatment produce differences in response that are not distinguishable from the effect of the treatment, those differences between the groups are said to be confounded with the effect of the treatment (if any). For example, prominent statisticians questioned whether differences between individuals that led some to smoke and others not to (rather than the act of smoking itself) were responsible for the observed difference in the frequencies with which smokers and non-smokers contract various illnesses. If that were the case, those factors would be confounded with the effect of smoking. Confounding is quite likely to affect observational studies and experiments that are not randomized. Confounding tends to be decreased by randomization. See also Simpson’s Paradox. 45 Continuity Correction. In using the normal approximation to the binomial probability histogram, one can get more accurate answers by finding the area under the normal curve corresponding to half-integers, transformed to standard units. This is clearest if we are seeking the chance of a particular number of successes. For example, suppose we seek to approximate the chance of 10 successes in 25 independent trials, each with probability p = 40% of success. The number of successes in this scenario has a binomial distribution with parameters n = 25 and p = 40%. The expected number of successes is np = 10, and the standard error is (np(1−p))½ = 6½ = 2.45. If we consider the area under the normal curve at the point 10 successes, transformed to standard units, we get zero: the area under a point is always zero. We get a better approximation by considering 10 successes to be the range from 9 1/2 to 10 1/2 successes. The only possible number of successes between 9 1/2 and 10 1/2 is 10, so this is exactly right for the binomial distribution. Because the normal curve is continuous and a binomial random variable is discrete, we need to “smear out” the binomial probability over an appropriate range. The lower endpoint of the range, 9 1/2 successes, is (9.5 − 10)/2.45 = −0.20 standard units. The upper endpoint of the range, 10 1/2 successes, is (10.5 − 10)/2.45 = &#43;0.20 standard units. The area under the normal curve between −0.20 and &#43;0.20 is about 15.8%. The true binomial probability is 25C10×(0.4)10×(0.6)15 = 16%. In a similar way, if we seek the normal approximation to the probability that a binomial random variable is in the range from i successes to k successes, inclusive, we should find the area under the normal curve from i−1/2 to k&#43;1/2 successes, transformed to standard units. If we seek the probability of more than i successes and fewer than k successes, we should find the area under the normal curve corresponding to the range i&#43;1/2 to k−1/2 successes, transformed to standard units. If we seek the probability of more than i but no more than k successes, we should find the area under the normal curve corresponding to the range i&#43;1/2 to k&#43;1/2 successes, transformed to standard units. If we seek the probability of at least i but fewer than k successes, we should find the area under the normal curve corresponding to the range i−1/2 to k−1/2 successes, transformed to standard units. Including or excluding the half-integer ranges at the ends of the interval in this manner is called the continuity correction. 46 Consequent. In a conditional p → q, the consequent is q. 47 Continuous Variable. A quantitative variable is continuous if its set of possible values is uncountable. Examples include temperature, exact height, exact age (including parts of a second). In practice, one can never measure a continuous variable to infinite precision, so continuous variables are sometimes approximated by discrete variables. A random variable X is also called continuous if its set of possible values is uncountable, and the chance that it takes any particular value is zero (in symbols, if P(X = x) = 0 for every real number x). A random variable is continuous if and only if its cumulative probability distribution function is a continuous function (a function with no jumps). 48 Contrapositive. If p and q are two logical propositions, then the contrapositive of the proposition (p → q) is the proposition ((! q) → (!p) ). The contrapositive is logically equivalent to the original proposition. 49 Control. There are at least three senses of “control” in statistics: a member of the control group, to whom no treatment is given; a controlled experiment, and to control for a possible confounding variable. 50 Controlled experiment. An experiment that uses the method of comparison to evaluate the effect of a treatment by comparing treated subjects with a control group, who do not receive the treatment. 51 Controlled, randomized experiment. A controlled experiment in which the assignment of subjects to the treatment group or control group is done at random, for example, by tossing a coin. 52 Control for a variable. To control for a variable is to try to separate its effect from the treatment effect, so it will not confound with the treatment. There are many methods that try to control for variables. Some are based on matching individuals between treatment and control; others use assumptions about the nature of the effects of the variables to try to model the effect mathematically, for example, using regression. 53 Control group. The subjects in a controlled experiment who do not receive the treatment. 54 Convenience Sample. A sample drawn because of its convenience; it is not a probability sample. For example, I might take a sample of opinions in Berkeley (where I live) by just asking my 10 nearest neighbors. That would be a sample of convenience, and would be unlikely to be representative of all of Berkeley. Samples of convenience are not typically representative, and it is not possible to quantify how unrepresentative results based on samples of convenience are likely to be. Convenience samples are to be avoided, and results based on convenience samples are to be viewed with suspicion. See also quota sample. 55 Converge, convergence. A sequence of numbers x1, x2, x3 … converges if there is a number x such that for any number E&gt;0, there is a number k (which can depend on E) such that /xj − x/ &lt; E whenever j &gt; k. If such a number x exists, it is called the limit of the sequence x1, x2, x3 … . 56 Convergence in probability. A sequence of random variables X1, X2, X3 … converges in probability if there is a random variable X such that for any number E&gt;0, the sequence of numbers P(/X1 − X/ &lt; e), P(/X2 − X/ &lt; e), P(/X3 − X/ &lt; e), … converges to 100%. 57 Converse. If p and q are two logical propositions, then the converse of the proposition (p → q) is the proposition (q → p). 58 Correlation. A measure of linear association between two (ordered) lists. Two variables can be strongly correlated without having any causal relationship, and two variables can have a causal relationship and yet be uncorrelated. 59 Correlation coefficient. The correlation coefficient r is a measure of how nearly a scatterplot falls on a straight line. The correlation coefficient is always between −1 and &#43;1. To compute the correlation coefficient of a list of pairs of measurements (X,Y), first transform X and Y individually into standard units. Multiply corresponding elements of the transformed pairs to get a single list of numbers. The correlation coefficient is the mean of that list of products. This page contains a tool that lets you generate bivariate data with any correlation coefficient you want. 60 Counting. To count a set of things is to put it in one to one correspondence with a consecutive subset of the positive integers (counting numbers). 61 Counting numbers, natural numbers. The counting numbers are the strictly positive integers ({1, 2, 3, … }). (Some authorities include (0) among the counting numbers.) 62 Countable Set. A set is countable if its elements can be put in one-to-one correspondence with a subset of the counting numbers. For example, the sets {0, 1, 7, −3}, {red, green, blue}, {…,−2, −1, 0, 1, 2, …}, {straight, curly}, and the set of all fractions, are countable. If a set is not countable, it is uncountable. The set of all real numbers is uncountable. 63 Cover. A confidence interval is said to cover if the interval contains the true value of the parameter. Before the data are collected, the chance that the confidence interval will contain the parameter value is the coverage probability, which equals the confidence level after the data are collected and the confidence interval is actually computed. 64 Coverage probability. The coverage probability of a procedure for making confidence intervals is the chance that the procedure produces an interval that covers the truth. 65 Critical value The critical value in an hypothesis test is the value of the test statistic beyond which we would reject the null hypothesis. The critical value is set so that the probability that the test statistic is beyond the critical value is at most equal to the significance level if the null hypothesis be true. 66 Cross-sectional study. A cross-sectional study compares different individuals to each other at the same time—it looks at a cross-section of a population. The differences between those individuals can confound with the effect being explored. For example, in trying to determine the effect of age on sexual promiscuity, a cross-sectional study would be likely to confound the effect of age with the effect of the mores the subjects were taught as children: the older individuals were probably raised with a very different attitude towards promiscuity than the younger subjects. Thus it would be imprudent to attribute differences in promiscuity to the aging process. C.f. longitudinal study. 67 Cumulative Probability Distribution Function (cdf). The cumulative distribution function of a random variable is the chance that the random variable is less than or equal to x, as a function of x. In symbols, if F is the cdf of the random variable X, then F(x) = P( X ≤ x). The cumulative distribution function must tend to zero as x approaches minus infinity, and must tend to unity as x approaches infinity. It is a positive function, and increases monotonically: if y &gt; x, then F(y) ≥ F(x). The cumulative distribution function completely characterizes the probability distribution of a random variable. 68 de Morgan’s Laws de Morgan’s Laws are identities involving logical operations: the negation of a conjunction is logically equivalent to the disjunction of the negations, and the negation of a disjunction is logically equivalent to the conjunction of the negations. In symbols, !(p &amp; q) = !p / !q and !(p / q) = !p &amp; !q. 69 Deck of Cards. A standard deck of playing cards contains 52 cards, 13 each of four suits: spades, hearts, diamonds, and clubs. The thirteen cards of each suit are {ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, jack, queen, king}. The face cards are {jack, queen, king}. It is typically assumed that if a deck of cards is shuffled well, it is equally likely to be in each possible ordering. There are 52! (52 factorial) possible orderings. 70 Dependent Events, Dependent Random Variables. Two events or random variables are dependent if they are not independent. 71 Dependent Variable. In regression, the variable whose values are supposed to be explained by changes in the other variable (the the independent or explanatory variable). Usually one regresses the dependent variable on the independent variable. 72 Density, Density Scale. The vertical axis of a histogram has units of percent per unit of the horizontal axis. This is called a density scale; it measures how “dense” the observations are in each bin. See also probability density. 73 Denying the antecedent. A logical fallacy that argues from the premise A → B and the premise !A that therefore, !B. The name comes from the fact that the operation denies (i.e., asserts the negation of) the antecedent (A) in the conditional. 74 Denying the consequent. A valid logical argument that concludes from the premise A → B and the premise !B that therefore, !A. The name comes from the fact that the operation denies (i.e., asserts the logical negation) the consequent (B) in the conditional. 75 Deviation. A deviation is the difference between a datum and some reference value, typically the mean of the data. In computing the SD, one finds the rms of the deviations from the mean, the differences between the individual data and the mean of the data. 76 Discrete Variable. A quantitative variable whose set of possible values is countable. Typical examples of discrete variables are variables whose possible values are a subset of the integers, such as Social Security numbers, the number of people in a family, ages rounded to the nearest year, etc. Discrete variables are “chunky.” C.f. continuous variable. A discrete random variable is one whose set of possible values is countable. A random variable is discrete if and only if its cumulative probability distribution function is a stair-step function; i.e., if it is piecewise constant and only increases by jumps. 77 Disjoint or Mutually Exclusive Events. Two events are disjoint or mutually exclusive if the occurrence of one is incompatible with the occurrence of the other; that is, if they can’t both happen at once (if they have no outcome in common). Equivalently, two events are disjoint if their intersection is the empty set. 78 Disjoint or Mutually Exclusive Sets. Two sets are disjoint or mutually exclusive if they have no element in common. Equivalently, two sets are disjoint if their intersection is the empty set. 79 Distribution. The distribution of a set of numerical data is how their values are distributed over the real numbers. It is completely characterized by the empirical distribution function. Similarly, the probability distribution of a random variable is completely characterized by its probability distribution function. Sometimes the word “distribution” is used as a synonym for the empirical distribution function or the probability distribution function. If two or more random variables are defined for the same experiment, they have a joint probability distribution. 80 Distribution Function, Empirical. The empirical (cumulative) distribution function of a set of numerical data is, for each real value of x, the fraction of observations that are less than or equal to x. A plot of the empirical distribution function is an uneven set of stairs. The width of the stairs is the spacing between adjacent data; the height of the stairs depends on how many data have exactly the same value. The distribution function is zero for small enough (negative) values of x, and is unity for large enough values of x. It increases monotonically: if y &gt; x, the empirical distribution function evaluated at y is at least as large as the empirical distribution function evaluated at x. 81 Double-Blind, Double-Blind Experiment. In a double-blind experiment, neither the subjects nor the people evaluating the subjects knows who is in the treatment group and who is in the control group. This mitigates the placebo effect and guards against conscious and unconscious prejudice for or against the treatment on the part of the evaluators. 82 Ecological Correlation. The correlation between averages of groups of individuals, instead of individuals. Ecological correlation can be misleading about the association of individuals. 83 Element of a Set. See member. 84 Empirical Law of Averages. The Empirical Law of Averages lies at the base of the frequency theory of probability. This law, which is, in fact, an assumption about how the world works, rather than a mathematical or physical law, states that if one repeats a random experiment over and over, independently and under “identical” conditions, the fraction of trials that result in a given outcome converges to a limit as the number of trials grows without bound. 85 Empty Set. The empty set, denoted {} or ∅, is the set that has no members. 86 Endpoint Convention. In plotting a histogram, one must decide whether to include a datum that lies at a class boundary with the class interval to the left or the right of the boundary. The rule for making this assignment is called an endpoint convention. The two standard endpoint conventions are (1) to include the left endpoint of all class intervals and exclude the right, except for the rightmost class interval, which includes both of its endpoints, and (2) to include the right endpoint of all class intervals and exclude the left, except for the leftmost interval, which includes both of its endpoints. 87 Equally Likely Outcomes. According to the equally likely outcome Theory of Probability, if an experiment has a finite number possible outcomes and there is no reason Nature should prefer any of those outcomes over any other (e.g., because the outcome is the result of rolling a symmetric die or tossing a perfectly balanced coin or thoroughly shuffling a deck of cards), then each of those possible outcomes has the same probability. See also Laplace’s Principle of Insufficient Reason. 88 Estimator. An estimator is a rule for “guessing” the value of a population parameter based on a random sample from the population. An estimator is a random variable, because its value depends on which particular sample is obtained, which is random. A canonical example of an estimator is the sample mean, which is an estimator of the population mean. 89 Event. An event is a subset of outcome space. An event determined by a random variable is an event of the form A=(X is in A). When the random variable X is observed, that determines whether or not A occurs: if the value of X happens to be in A, A occurs; if not, A does not occur. 90 Exhaustive. A collection of events {A1, A2, A3, … } exhausts the set A if, for the event A to occur, at least one of those sets must also occur; that is, if S ⊂ A1 ∪ A2 ∪ A3 ∪ … If the event A is not specified, it is assumed to be the entire outcome space S. 91 Expectation, Expected Value. The expected value of a random variable is the long-term limiting average of its values in independent repeated experiments. The expected value of the random variable X is denoted EX or E(X). For a discrete random variable (one that has a countable number of possible values) the expected value is the weighted average of its possible values, where the weight assigned to each possible value is the chance that the random variable takes that value. One can think of the expected value of a random variable as the point at which its probability histogram would balance, if it were cut out of a uniform material. Taking the expected value is a linear operation: if X and Y are two random variables, the expected value of their sum is the sum of their expected values (E(X&#43;Y) = E(X) &#43; E(Y)), and the expected value of a constant a times a random variable X is the constant times the expected value of X (E(a×X ) = a× E(X)). 92 Experiment. What distinguishes an experiment from an observational study is that in an experiment, the experimenter chooses who receives the treatment. 93 Explanatory Variable. In regression, the explanatory or independent variable is the one that is supposed to “explain” the other. For example, in examining crop yield versus quantity of fertilizer applied, the quantity of fertilizer would be the explanatory or independent variable, and the crop yield would be the dependent variable. In experiments, the explanatory variable is the one that is manipulated; the one that is observed is the dependent variable. 94 Extrapolation. See interpolation. 95 Factorial. For an integer k that is greater than or equal to 1, k! (pronounced “k factorial”) is k×(k−1)×(k−2)× …×1. By convention, 0! = 1. There are k! ways of ordering k distinct objects. For example, 9! is the number of batting orders of 9 baseball players, and 52! is the number of different ways a standard deck of playing cards can be ordered. The calculator above has a button to compute the factorial of a number. To compute k!, first type the value of k, then press the button labeled “!”. 96 Fair Bet. A fair bet is one for which the expected value of the payoff is zero, after accounting for the cost of the bet. For example, suppose I offer to pay you $2 if a fair coin lands heads, but you must ante up $1 to play. Your expected payoff is −$1&#43; $0×P(tails) &#43; $2×P(heads) = −$1 &#43; $2×50% = $0. This is a fair bet—in the long run, if you made this bet over and over again, you would expect to break even. 97 False Discovery Rate. In testing a collection of hypotheses, the false discovery rate is the fraction of rejected null hypotheses that are rejected erroneously (the number of Type I errors divided by the number of rejected null hypotheses), with the convention that if no hypothesis is rejected, the false discovery rate is zero. 98 Finite, finite set. A set is finite if it has a finite number of elements, that is, if for some natural number n, the elements can be put in one-to-one correspondence with the set {1, 2, … n}. 99 Finite Population Correction. When sampling without replacement, as in a simple random sample, the SE of sample sums and sample means depends on the fraction of the population that is in the sample: the greater the fraction, the smaller the SE. Sampling with replacement is like sampling from an infinitely large population. The adjustment to the SE for sampling without replacement is called the finite population correction. The SE for sampling without replacement is smaller than the SE for sampling with replacement by the finite population correction factor ((N −n)/(N − 1))½. Note that for sample size n=1, there is no difference between sampling with and without replacement; the finite population correction is then unity. If the sample size is the entire population of N units, there is no variability in the result of sampling without replacement (every member of the population is in the sample exactly once), and the SE should be zero. This is indeed what the finite population correction gives (the numerator vanishes). 100 Fisher’s exact test (for the equality of two percentages) Consider two populations of zeros and ones. Let p1 be the proportion of ones in the first population, and let p2 be the proportion of ones in the second population. We would like to test the null hypothesis that p1 = p2 on the basis of a simple random sample from each population. Let n1 be the size of the sample from population 1, and let n2 be the size of the sample from population 2. Let G be the total number of ones in both samples. If the null hypothesis be true, the two samples are like one larger sample from a single population of zeros and ones. The allocation of ones between the two samples would be expected to be proportional to the relative sizes of the samples, but would have some chance variability. Conditional on G and the two sample sizes, under the null hypothesis, the tickets in the first sample are like a random sample of size n1 without replacement from a collection of N = n1 &#43; n2 units of which G are labeled with ones. Thus, under the null hypothesis, the number of tickets labeled with ones in the first sample has (conditional on G) an hypergeometric distribution with parameters N, G, and n1. Fisher’s exact test uses this distribution to set the ranges of observed values of the number of ones in the first sample for which we would reject the null hypothesis. 101 Football-Shaped Scatterplot. In a football-shaped scatterplot, most of the points lie within a tilted oval, shaped more-or-less like a football. A football-shaped scatterplot is one in which the data are homoscedastically scattered about a straight line. 102 Frame, sampling frame. A sampling frame is a collection of units from which a sample will be drawn. Ideally, the frame is identical to the population we want to learn about; more typically, the frame is only a subset of the population of interest. The difference between the frame and the population can be a source of bias in sampling design, if the parameter of interest has a different value for the frame than it does for the population. For example, one might desire to estimate the current annual average income of 1998 graduates of the University of California at Berkeley. I propose to use the sample mean income of a sample of graduates drawn at random. To facilitate taking the sample and contacting the graduates to obtain income information from them, I might draw names at random from the list of 1998 graduates for whom the alumni association has an accurate current address. The population is the collection of 1998 graduates; the frame is those graduates who have current addresses on file with the alumni association. If there is a tendency for graduates with higher incomes to have up-to-date addresses on file with the alumni association, that would introduce a positive bias into the annual average income estimated from the sample by the sample mean. 103 FPP. Statistics, third edition, by Freedman, Pisani, and Purves, published by W.W. Norton, 1997. 104 Frequency theory of probability. See Probability, Theories of. 105 Frequency table. A table listing the frequency (number) or relative frequency (fraction or percentage) of observations in different ranges, called class intervals. 106 Fundamental Rule of Counting. If a sequence of experiments or trials T1, T2, T3, …, Tk could result, respectively, in n1, n2 n3, …, nk possible outcomes, and the numbers n1, n2 n3, …, nk do not depend on which outcomes actually occurred, the entire sequence of k experiments has n1× n2 × n3× …× nk possible outcomes. 107 Game Theory. A field of study that bridges mathematics, statistics, economics, and psychology. It is used to study economic behavior, and to model conflict between nations, for example, “nuclear stalemate” during the Cold War. 108 Geometric Distribution. The geometric distribution describes the number of trials up to and including the first success, in independent trials with the same probability of success. The geometric distribution depends only on the single parameter p, the probability of success in each trial. For example, the number of times one must toss a fair coin until the first time the coin lands heads has a geometric distribution with parameter p = 50%. The geometric distribution assigns probability p×(1 − p)k−1to the event that it takes k trials to the first success. The expected value of the geometric distribution is 1/p, and its SE is (1−p)½/p. 109 Geometric Mean. The geometric mean of n numbers {x1, x2, x3, …, xn} is the nth root of their product: (x1×x2×x3× … ×xn)1/n. 110 Graph of Averages. For bivariate data, a graph of averages is a plot of the average values of one variable (say y) for small ranges of values of the other variable (say x), against the value of the second variable (x) at the midpoints of the ranges. 111 Heteroscedasticity. “Mixed scatter.” A scatterplot or residual plot shows heteroscedasticity if the scatter in vertical slices through the plot depends on where you take the slice. Linear regression is not usually a good idea if the data are heteroscedastic. 112 Histogram. A histogram is a kind of plot that summarizes how data are distributed. Starting with a set of class intervals, the histogram is a set of rectangles (“bins”) sitting on the horizontal axis. The bases of the rectangles are the class intervals, and their heights are such that their areas are proportional to the fraction of observations in the corresponding class intervals. That is, the height of a given rectangle is the fraction of observations in the corresponding class interval, divided by the length of the corresponding class interval. A histogram does not need a vertical scale, because the total area of the histogram must equal 100%. The units of the vertical axis are percent per unit of the horizontal axis. This is called the density scale. The horizontal axis of a histogram needs a scale. If any observations coincide with the endpoints of class intervals, the endpoint convention is important. This page contains a histogram tool, with controls to highlight ranges of values and read their areas. 113 Historical Controls. Sometimes, the a treatment group is compared with individuals from another epoch who did not receive the treatment; for example, in studying the possible effect of fluoridated water on childhood cancer, we might compare cancer rates in a community before and after fluorine was added to the water supply. Those individuals who were children before fluoridation started would comprise an historical control group. Experiments and studies with historical controls tend to be more susceptible to confounding than those with contemporary controls, because many factors that might affect the outcome other than the treatment tend to change over time as well. (In this example, the level of other potential carcinogens in the environment also could have changed.) 114 Homoscedasticity. “Same scatter.” A scatterplot or residual plot shows homoscedasticity if the scatter in vertical slices through the plot does not depend much on where you take the slice. C.f. heteroscedasticity. 115 House Edge. In casino games, the expected payoff to the bettor is negative: the house (casino) tends to win money in the long run. The amount of money the house would expect to win for each $1 wagered on a particular bet (such as a bet on “red” in roulette) is called the house edge for that bet. 116 HTLWS. The book How to lie with Statistics by D. Huff. 117 Hypergeometric Distribution. The hypergeometric distribution with parameters N, G and n is the distribution of the number of “good” objects in a simple random sample of size n (i.e., a random sample without replacement in which every subset of size n has the same chance of occurring) from a population of N objects of which G are “good.” The chance of getting exactly g good objects in such a sample is GCg × N−GCn−g/NCn, provided g ≤ n, g ≤ G, and n − g ≤ N − G. (The probability is zero otherwise.) The expected value of the hypergeometric distribution is n×G/N, and its standard error is ((N−n)/(N−1))½ × (n × G/N × (1−G/N) )½. 118 Hypothesis testing. Statistical hypothesis testing is formalized as making a decision between rejecting or not rejecting a null hypothesis, on the basis of a set of observations. Two types of errors can result from any decision rule (test): rejecting the null hypothesis when it is true (a Type I error), and failing to reject the null hypothesis when it is false (a Type II error). For any hypothesis, it is possible to develop many different decision rules (tests). Typically, one specifies ahead of time the chance of a Type I error one is willing to allow. That chance is called the significance level of the test or decision rule. For a given significance level, one way of deciding which decision rule is best is to pick the one that has the smallest chance of a Type II error when a given alternative hypothesis is true. The chance of correctly rejecting the null hypothesis when a given alternative hypothesis is true is called the power of the test against that alternative. 119 iff, if and only if, ↔ If p and q are two logical propositions, then(p ↔ q) is a proposition that is true when both p and q are true, and when both p and q are false. It is logically equivalent to the proposition ( (p → q) &amp; (q → p) ) and to the proposition ( (p &amp; q) 120 Implies, logical implication, → , conditional, if-then Logical implication is an operation on two logical propositions. If p and q are two logical propositions, (p → q), pronounced “p implies q” or “if p then q” is a logical proposition that is true if p is false, or if both p and q are true. The proposition (p → q) is logically equivalent to the proposition ((!p) / q). In the conditional p → q, the antecedent is p and the consequent is q. 121 Independent, independence. Two events A and B are (statistically) independent if the chance that they both happen simultaneously is the product of the chances that each occurs individually; i.e., if P(AB) = P(A)P(B). This is essentially equivalent to saying that learning that one event occurs does not give any information about whether the other event occurred too: the conditional probability of A given B is the same as the unconditional probability of A, i.e., P(A/B) = P(A). Two random variables X and Y are independent if all events they determine are independent, for example, if the event {a &lt; X ≤ b} is independent of the event {c &lt; Y ≤ d} for all choices of a, b, c, and d. A collection of more than two random variables is independent if for every proper subset of the variables, every event determined by that subset of the variables is independent of every event determined by the variables in the complement of the subset. For example, the three random variables X, Y, and Z are independent if every event determined by X is independent of every event determined by Y and every event determined by X is independent of every event determined by Y and Z and every event determined by Y is independent of every event determined by X and Z and every event determined by Z is independent of every event determined by X and Y. 122 Independent and identically distributed (iid). A collection of two or more random variables {X1, X2, … , } is independent and identically distributed if the variables have the same probability distribution, and are independent. 123 Independent Variable. In regression, the independent variable is the one that is supposed to explain the other; the term is a synonym for “explanatory variable.” Usually, one regresses the “dependent variable” on the “independent variable.” There is not always a clear choice of the independent variable. The independent variable is usually plotted on the horizontal axis. Independent in this context does not mean the same thing as statistically independent. 124 Indicator Random Variable. The indicator [random variable] of the event A, often written 1A, is the random variable that equals unity if A occurs, and zero if A does not occur. The expected value of the indicator of A is the probability of A, P(A), and the standard error of the indicator of A is (P(A)×(1−P(A))½. The sum 1A &#43; 1B &#43; 1C &#43; … of the indicators of a collection of events {A, B, C, …} counts how many of the events {A, B, C, …} occur in a given trial. The product of the indicators of a collection of events is the indicator of the intersection of the events (the product equals one if and only if all of indicators equal one). The maximum of the indicators of a collection of events is the indicator of the union of the events (the maximum equals one if any of the indicators equals one). 125 Inter-quartile Range (IQR). The inter-quartile range of a list of numbers is the upper quartile minus the lower quartile. 126 Interpolation. Given a set of bivariate data (x, y), to impute a value of y corresponding to some value of x at which there is no measurement of y is called interpolation, if the value of x is within the range of the measured values of x. If the value of x is outside the range of measured values, imputing a corresponding value of y is called extrapolation. 127 Intersection. The intersection of two or more sets is the set of elements that all the sets have in common; the elements contained in every one of the sets. The intersection of the events A and B is written “A∩B,” “A and B,” and “AB.” C.f. union. See also Venn diagrams. 128 Invalid (logical) argument. An invalid logical argument is one in which the truth of the premises does not guarantee the truth of the conclusion. For example, the following logical argument is invaldraft: false id: If the forecast calls for rain, I will not wear sandals. The forecast does not call for rain. Therefore, I will wear sandals. See also valid argument. 129 Joint Probability Distribution. If X1, X2, … , Xk are random variables defined for the same experiment, their joint probability distribution gives the probability of events determined by the collection of random variables: for any collection of sets of numbers {A1, … , Ak}, the joint probability distribution determines P( (X1 is in A1) and (X2 is in A2) and … and (Xk is in Ak) ). For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. If a collection of random variables is independent, their joint probability distribution is the product of their marginal probability distributions, their individual probability distributions without regard for the value of the other variables. In this example, the marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6, and the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Note that P(X1 = 1, X2 = 10) = 0, while P(X1 = 1)×P(X2 = 10) = (1/6)(3/36) = 1/72. The joint probability is not equal to the product of the marginal probabilities: X1 and X2 are dependent random variables. 130 Law of Averages. The Law of Averages says that the average of independent observations of random variables that have the same probability distribution is increasingly likely to be close to the expected value of the random variables as the number of observations grows. More precisely, if X1, X2, X3, …, are independent random variables with the same probability distribution, and E(X) is their common expected value, then for every number ε &gt; 0, P{/(X1 &#43; X2 &#43; … &#43; Xn)/n − E(X) / &lt; ε} converges to 100% as n grows. This is equivalent to saying that the sequence of sample means X1, (X1&#43;X2)/2, (X1&#43;X2&#43;X3)/3, … converges in probability to E(X). 131 Law of Large Numbers. The Law of Large Numbers says that in repeated, independent trials with the same probability p of success in each trial, the percentage of successes is increasingly likely to be close to the chance of success as the number of trials increases. More precisely, the chance that the percentage of successes differs from the probability p by more than a fixed positive amount, e &gt; 0, converges to zero as the number of trials n goes to infinity, for every number e &gt; 0. Note that in contrast to the difference between the percentage of successes and the probability of success, the difference between the number of successes and the expected number of successes, n×p, tends to grow as n grows. The following tool illustrates the law of large numbers; the button toggles between displaying the difference between the number of successes and the expected number of successes, and the difference between the percentage of successes and the expected percentage of successes. The tool on this page illustrates the law of large numbers. 132 Limit. See converge. 133 Linear Operation. Suppose f is a function or operation that acts on things we shall denote generically by the lower-case Roman letters x and y. Suppose it makes sense to multiply x and y by numbers (which we denote by a), and that it makes sense to add things like x and y together. We say that f is linear if for every number a and every value of x and y for which f(x) and f(y) are defined, (i) f( a×x ) is defined and equals a×f(x), and (ii) f( x &#43; y ) is defined and equals f(x) &#43; f(y). C.f. affine. 134 Linear association. Two variables are linearly associated if a change in one is associated with a proportional change in the other, with the same constant of proportionality throughout the range of measurement. The correlation coefficient measures the degree of linear association on a scale of −1 to 1. 135 List. I use the term list to mean two things: either a multiset or (more often) an tuple. Lists are countable collections (multisets) in some order (like a tuple). That is, it makes sense to talk about the 1st (or 7th, or nth) element of a list, and the nth and mth elements of a list can be equal, even if n ≠ m (the elements of a list need not be distinct). 136 Location, Measure of. A measure of location is a way of summarizing what a “typical” element of a list is—it is a one-number summary of a distribution. See also arithmetic mean, median, and mode. 137 Logical argument. A logical argument consists of one or more premises, propositions that are assumed to be true, and a conclusion, a proposition that is supposed to be guaranteed to be true (as a matter of pure logic) if the premises are true. For example, the following is a logical argument: p → q Therefore, q. This argument has two premises: p → q, and p. The conclusion of the argument is q. If a logical argument is valid if the truth of the premises guarantees the truth of the conclusion; otherwise, the argument is invalid. That is, an argument with premises p1, p1, … pn and conclusion q is valid if the compound proposition (p1 &amp; p2 &amp; … &amp; pn) → q is logically equivalent to TRUE. The argument given above is valid because if it is true that p → q and that p is true (the two premises), then q (the conclusion of the argument) must also be true. 138 Logically equivalent, logical equivalence. Two propositions are logically equivalent if they always have the same truth value. That is, the propositions p and q are logically equivalent if p is true whenever q is true and p is false whenever q is false. The proposition (p ↔ q) is always true if and only if p and q are logically equivalent. For example, p is logically equivalent to p, to (p &amp; p), and to (p / p); (p / (!p)) is logically equivalent to TRUE; (p &amp; !p) is logically equivalent to FALSE; (p ↔ p) is logically equivalent to TRUE; and (p → q) is logically equivalent to (!p / q). 139 Longitudinal study. A study in which individuals are followed over time, and compared with themselves at different times, to determine, for example, the effect of aging on some measured variable. Longitudinal studies provide much more persuasive evidence about the effect of aging than do cross-sectional studies. 140 Lower Quartile (LQ). See quartiles. 141 Margin of error. A measure of the uncertainty in an estimate of a parameter; unfortunately, not everyone agrees what it should mean. The margin of error of an estimate is typically one or two times the estimated standard error of the estimate. 142 Marginal probability distribution. The marginal probability distribution of a random variable that has a joint probability distribution with some other random variables is the probability distribution of that random variable without regard for the values that the other random variables take. The marginal distribution of a discrete random variable X1 that has a joint distribution with other discrete random variables can be found from the joint distribution by summing over all possible values of the other variables. For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. The marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6. We can verify that the marginal probability that X1 = 1 is indeed the sum of the joint probability distribution over all possible values of X2 for which X1 = 1: P(X1 = 1) = P(X1 = 1, X2 = 2) &#43; P(X1 = 1, X2 = 3) &#43; P(X1 = 1, X2 = 4) &#43; P(X1 = 1, X2 = 5) &#43; P(X1 = 1, X2 = 6) &#43; P(X1 = 1, X2 = 7) = 6/36 = 1/6. Similarly, the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Again, we can verify that the marginal probability that X2 = 4 is 3/36 by adding the joint probabilities for all possible values of X1 for which X2 = 4: P(X2 = 4) = P(X1 = 1, X2 = 4) &#43; P(X1 = 2, X2 = 4) &#43; P(X1 = 3, X2 = 4) = 3/36. 143 Markov’s Inequality. For lists: If a list contains no negative numbers, the fraction of numbers in the list at least as large as any given constant a&gt;0 is no larger than the arithmetic mean of the list, divided by a. For random variables: if a random variable X must be nonnegative, the chance that X exceeds any given constant a&gt;0 is no larger than the expected value of X, divided by a. 144 Maximum Likelihood Estimate (MLE). The maximum likelihood estimate of a parameter from data is the possible value of the parameter for which the chance of observing the data largest. That is, suppose that the parameter is p, and that we observe data x. Then the maximum likelihood estimate of p is estimate p by the value q that makes P(observing x when the value of p is q) as large as possible. For example, suppose we are trying to estimate the chance that a (possibly biased) coin lands heads when it is tossed. Our data will be the number of times x the coin lands heads in n independent tosses of the coin. The distribution of the number of times the coin lands heads is binomial with parameters n (known) and p (unknown). The chance of observing x heads in n trials if the chance of heads in a given trial is q is nCx qx(1−q)n−x. The maximum likelihood estimate of p would be the value of q that makes that chance largest. We can find that value of q explicitly using calculus; it turns out to be q = x/n, the fraction of times the coin is observed to land heads in the n tosses. Thus the maximum likelihood estimate of the chance of heads from the number of heads in n independent tosses of the coin is the observed fraction of tosses in which the coin lands heads. 145 Mean, Arithmetic mean. The sum of a list of numbers, divided by the number of elements in the list. See also average. 146 Mean Squared Error (MSE). The mean squared error of an estimator of a parameter is the expected value of the square of the difference between the estimator and the parameter. In symbols, if X is an estimator of the parameter t, then MSE(X) = E( (X−t)2 ). The MSE measures how far the estimator is off from what it is trying to estimate, on the average in repeated experiments. It is a summary measure of the accuracy of the estimator. It combines any tendency of the estimator to overshoot or undershoot the truth (bias), and the variability of the estimator (SE). The MSE can be written in terms of the bias and SE of the estimator: MSE(X) = (bias(X))2 &#43; (SE(X))2. 147 Median. “Middle value” of a list. The smallest number such that at least half the numbers in the list are no greater than it. If the list has an odd number of entries, the median is the middle entry in the list after sorting the list into increasing order. If the list has an even number of entries, the median is the smaller of the two middle numbers after sorting. The median can be estimated from a histogram by finding the smallest number such that the area under the histogram to the left of that number is 50%. 148 Member of a set. Something is a member (or element) of a set if it is one of the things in the set. 149 Method of Comparison. The most basic and important method of determining whether a treatment has an effect: compare what happens to individuals who are treated (the treatment group) with what happens to individuals who are not treated (the control group). 150 Minimax Strategy. In game theory, a minimax strategy is one that minimizes one’s maximum loss, whatever the opponent might do (whatever strategy the opponent might choose). 151 Mode. For lists, the mode is a most common (frequent) value. A list can have more than one mode. For histograms, a mode is a relative maximum (“bump”). 152 Moment. The kth moment of a list is the average value of the elements raised to the kth power; that is, if the list consists of the N elements x1, x2, … , xN, the kth moment of the list is ( x1k &#43; x2k &#43; xNk )/N. The kth moment of a random variable X is the expected value of Xk, E(Xk). 153 Monotone, monotonic function. A function is monotone if it only increases or only decreases: f increases monotonically (is monotonic increasing) if x &gt; y, implies thatf(x) ≥ f(y). A function f decreases monotonically (is monotonic decreasing) if x &gt; y, implies thatf(x) ≤ f(y). A function f is strictly monotonically increasing if x &gt; y, implies thatf(x) &gt; f(y), and strictly monotonically decreasing if if x &gt; y, implies thatf(x) &lt; f(y). 154 Multimodal Distribution. A distribution with more than one mode. The histogram of a multimodal distribution has more than one “bump.” 155 Multinomial Distribution Consider a sequence of n independent trials, each of which can result in an outcome in any of k categories. Let pj be the probability that each trial results in an outcome in category j, j = 1, 2, … , k, so p1 &#43; p2 &#43; … &#43; pk = 100%. The number of outcomes of each type has a multinomial distribution. In particular, the probability that the n trials result in n1 outcomes of type 1, n2 outcomes of type 2, … , and nk outcomes of type k is n!/(n1! × n2! × … × nk!) × p1n1 × p2n2 × … × pknk, if n1, … , nk are nonnegative integers that sum to n; the chance is zero otherwise. 156 Multiplication rule. The chance that events A and B both occur (i.e., that event AB occurs), is the conditional probability that A occurs given that B occurs, times the unconditional probability that B occurs. 157 Multiplicity in hypothesis tests. In hypothesis testing, if more than one hypothesis is tested, the actual significance level of the combined tests is not equal to the nominal significance level of the individual tests. See also false discovery rate. 158 Multivariate Data. A set of measurements of two or more variables per individual. See bivariate. 159 Multiset. A multiset, also known as a bag is a collection of things, but—unlike a set, which is also a collection of things—the same object can occur in a multiset more than once. For instance, the sets {1, 2}, {1, 2, 2}, and {1, 1, 1, 1, 1, 2, 2} are all equal, while the multisets [1, 2], [1, 2, 2], and [1, 1, 1, 1, 1, 2, 2] are all different. However, order does not matter for sets or for multisets, so, for instance {1, 2} = {2, 1} and [1, 1, 1, 1, 1, 2, 2] = [2, 1, 1, 2, 1, 1, 1]. 160 Mutually Exclusive. See disjoint events or disjoint sets. 161 Nearly normal distribution. A population of numbers (a list of numbers) is said to have a nearly normal distribution if the histogram of its values in standard units nearly follows a normal curve. More precisely, suppose that the mean of the list is μ and the standard deviation of the list is SD. Then the list is nearly normally distributed if, for every two numbers a &lt; b, the fraction of numbers in the list that are between a and b is approximately equal to the area under the normal curve between (a − μ)/SD and (a − μ)/SD. 162 Negative Binomial Distribution. Consider a sequence of independent trials with the same probability p of success in each trial. The number of trials up to and including the rth success has the negative Binomial distribution with parameters n and r. If the random variable N has the negative binomial distribution with parameters n and r, then P(N=k) = k−1Cr−1 × pr × (1−p)k−r, for k = r, r&#43;1, r&#43;2, …, and zero for k &lt; r, because there must be at least r trials to have r successes. The negative binomial distribution is derived as follows: for the rth success to occur on the kth trial, there must have been r−1 successes in the first k−1 trials, and the kth trial must result in success. The chance of the former is the chance of r−1 successes in k−1 independent trials with the same probability of success in each trial, which, according to the Binomial distribution with parameters n=k−1 and p, has probability k−1Cr−1 × pr−1 × (1−p)k−r. The chance of the latter event is p, by assumption. Because the trials are independent, we can find the chance that both events occur by multiplying their chances together, which gives the expression for P(N=k) above. 163 No causation without manipulation. A slogan attributed to Paul Holland. If the conditions were not deliberately manipulated (for example, if the situation is an observational study rather than an experiment), it is unwise to conclude that there is any causal relationship between the outcome and the conditions. See post hoc ergo propter hoc and cum hoc ergo propter hoc. 164 Nonlinear Association. The relationship between two variables is nonlinear if a change in one is associated with a change in the other that is depends on the value of the first; that is, if the change in the second is not simply proportional to the change in the first, independent of the value of the first variable. 165 Nonresponse. In surveys, it is rare that everyone who is “invited” to participate (everyone whose phone number is called, everyone who is mailed a questionnaire, everyone an interviewer tries to stop on the street…) in fact responds. The difference between the “invited” sample sought, and that obtained, is the nonresponse. 166 Nonresponse bias. In a survey, those who respond may differ from those who do not, in ways that are related to the effect one is trying to measure. For example, a telephone survey of how many hours people work is likely to miss people who are working late, and are therefore not at home to answer the phone. When that happens, the survey may suffer from nonresponse bias. Nonresponse bias makes the result of a survey differ systematically from the truth. 167 Nonresponse rate. The fraction of nonresponders in a survey: the number of nonresponders divided by the number of people invited to participate (the number sent questionnaires, the number of interview attempts, etc.) If the nonresponse rate is appreciable, the survey suffer from large nonresponse bias. 168 Normal approximation. The normal approximation to data is to approximate areas under the histogram of data, transformed into standard units, by the corresponding areas under the normal curve. Many probability distributions can be approximated by a normal distribution, in the sense that the area under the probability histogram is close to the area under a corresponding part of the normal curve. To find the corresponding part of the normal curve, the range must be converted to standard units, by subtracting the expected value and dividing by the standard error. For example, the area under the binomial probability histogram for n = 50 and p = 30% between 9.5 and 17.5 is 74.2%. To use the normal approximation, we transform the endpoints to standard units, by subtracting the expected value (for the Binomial random variable, n×p = 15 for these values of n and p) and dividing the result by the standard error (for a Binomial, (n × p × (1−p))1/2 = 3.24 for these values of n and p). The area normal approximation is the area under the normal curve between (9.5 − 15)/3.24 = −1.697 and (17.5 − 15)/3.24 = 0.772; that area is 73.5%, slightly smaller than the corresponding area under the binomial histogram. See also the continuity correction. The tool on this page illustrates the normal approximation to the binomial probability histogram. Note that the approximation gets worse when p gets close to 0 or 1, and that the approximation improves as n increases. 169 Normal curve. The normal curve is the familiar “bell curve:,” illustrated on this page. The mathematical expression for the normal curve is y = (2×pi)−½e−x2/2, where pi is the ratio of the circumference of a circle to its diameter (3.14159265…), and e is the base of the natural logarithm (2.71828…). The normal curve is symmetric around the point x=0, and positive for every value of x. The area under the normal curve is unity, and the SD of the normal curve, suitably defined, is also unity. Many (but not most) histograms, converted into standard units, approximately follow the normal curve. 170 Normal distribution. A random variable X has a normal distribution with mean m and standard error s if for every pair of numbers a ≤ b, the chance that a &lt; (X−m)/s &lt; b is P(a &lt; (X−m)/s &lt; b) = area under the normal curve between a and b. If there are numbers m and s such that X has a normal distribution with mean m and standard error s, then X is said to have a normal distribution or to be normally distributed. If X has a normal distribution with mean m=0 and standard error s=1, then X is said to have a standard normal distribution. The notation XN(m,s2) means that X has a normal distribution with mean m and standard error s; for example, XN(0,1), means X has a standard normal distribution. 171 NOT, !, Negation, Logical Negation. The negation of a logical proposition p, !p, is a proposition that is the logical opposite of p. That is, if p is true, !p is false, and if p is false, !p is true. Negation takes precedence over other logical operations. Other common symbols for the negation operator include ¬, − and ˜. 172 Null hypothesis. In hypothesis testing, the hypothesis we wish to falsify on the basis of the data. The null hypothesis is typically that something is not present, that there is no effect, or that there is no difference between treatment and control. 173 Observational Study. Controlled experiment. 174 Odds. The odds in favor of an event is the ratio of the probability that the event occurs to the probability that the event does not occur. For example, suppose an experiment can result in any of n possible outcomes, all equally likely, and that k of the outcomes result in a “win” and n−k result in a “loss.” Then the chance of winning is k/n; the chance of not winning is (n−k)/n; and the odds in favor of winning are (k/n)/((n−k)/n) = k/(n−k), which is the number of favorable outcomes divided by the number of unfavorable outcomes. Note that odds are not synonymous with probability, but the two can be converted back and forth. If the odds in favor of an event are q, then the probability of the event is q/(1&#43;q). If the probability of an event is p, the odds in favor of the event are p/(1−p) and the odds against the event are (1−p)/p. 175 One-sided Test. C.f. two-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against either the alternative hypothesis that μ &lt; μ0 or the alternative μ &gt; μ0 (but not both). For example, a significance level 5%, one-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero, would reject the null hypothesis for values of 176 or, /, Disjunction, Logical Disjunction, ∨ An operation on two logical propositions. If p and q are two propositions, (p / q) is a proposition that is true if p is true or if q is true (or both); otherwise, it is false. That is, (p / q) is true unless both p and q are false. The operation / is sometimes represented by the symbol ∨ and sometimes by the word or. C.f. exclusive disjunction, XOR. 177 Ordinal Variable. A variable whose possible values have a natural order, such as {short, medium, long}, {cold, warm, hot}, or {0, 1, 2, 3, …}. In contrast, a variable whose possible values are {straight, curly} or {Arizona, California, Montana, New York} would not naturally be ordinal. Arithmetic with the possible values of an ordinal variable does not necessarily make sense, but it does make sense to say that one possible value is larger than another. 178 Outcome Space. The outcome space is the set of all possible outcomes of a given random experiment. The outcome space is often denoted by the capital letter S. 179 Outlier. An outlier is an observation that is many SD’s from the mean. It is sometimes tempting to discard outliers, but this is imprudent unless the cause of the outlier can be identified, and the outlier is determined to be spurious. Otherwise, discarding outliers can cause one to underestimate the true variability of the measurement process. 180 P-value. Suppose we have a family of hypothesis tests of a null hypothesis that let us test the hypothesis at any significance level p between 0 and 100% we choose. The P value of the null hypothesis given the data is the smallest significance level p for which any of the tests would have rejected the null hypothesis. For example, let X be a test statistic, and for p between 0 and 100%, let xp be the smallest number such that, under the null hypothesis, P( X ≤ x ) ≥ p. Then for any p between 0 and 100%, the rule reject the null hypothesis if X &lt; xp tests the null hypothesis at significance level p. If we observed X = x, the P-value of the null hypothesis given the data would be the smallest p such that x &lt; xp. 181 Parameter. A numerical property of a population, such as its mean. 182 Partition. A partition of an event A is a collection of events {A1, A2, A3, … } such that the events in the collection are disjoint, and their union is A. That is, AjAk = {} unless j = k, and A = A1 ∪ A2 ∪ A3 ∪ … . If the event A is not specified, it is assumed to be the entire outcome space S. 183 Payoff Matrix. A way of representing what each player in a game wins or loses, as a function of his and his opponent’s strategies. 184 Percentile. The pth percentile of a list is the smallest number such that at least p% of the numbers in the list are no larger than it. The pth percentile of a random variable is the smallest number such that the chance that the random variable is no larger than it is at least p%. C.f. quantile. 185 Permutation. A permutation of a set is an arrangement of the elements of the set in some order. If the set has n things in it, there are n! different orderings of its elements. For the first element in an ordering, there are n possible choices, for the second, there remain n−1 possible choices, for the third, there are n−2, etc., and for the nth element of the ordering, there is a single choice remaining. By the fundamental rule of counting, the total number of sequences is thus n×(n−1)×(n−2)×…×1. Similarly, the number of orderings of length k one can form from n≥k things is n×(n−1)×(n−2)×…×(n−k&#43;1) = n!/(n−k)!. This is denoted nPk, the number of permutations of n things taken k at a time. C.f. combinations. 186 Placebo. A “dummy” treatment that has no pharmacological effect; e.g., a sugar pill. 187 Placebo effect. The belief or knowledge that one is being treated can itself have an effect that confounds with the real effect of the treatment. Subjects given a placebo as a pain-killer report statistically significant reductions in pain in randomized experiments that compare them with subjects who receive no treatment at all. This very real psychological effect of a placebo, which has no direct biochemical effect, is called the placebo effect. Administering a placebo to the control group is thus important in experiments with human subjects; this is the essence of a blind experiment. 188 Point of Averages. In a scatterplot, the point whose coordinates are the arithmetic means of the corresponding variables. For example, if the variable X is plotted on the horizontal axis and the variable Y is plotted on the vertical axis, the point of averages has coordinates (mean of X, mean of Y). 189 Poisson Distribution. The Poisson distribution is a discrete probability distribution that depends on one parameter, m. If X is a random variable with the Poisson distribution with parameter m, then the probability that X = k is E−m × mk/k!, k = 0, 1, 2, … , where E is the base of the natural logarithm and ! is the factorial function. For all other values of k, the probability is zero. The expected value the Poisson distribution with parameter m is m, and the standard error of the Poisson distribution with parameter m is m½. 190 Population. A collection of units being studied. Units can be people, places, objects, epochs, drugs, procedures, or many other things. Much of statistics is concerned with estimating numerical properties (parameters) of an entire population from a random sample of units from the population. 191 Population Mean. The mean of the numbers in a numerical population. For example, the population mean of a box of numbered tickets is the mean of the list comprised of all the numbers on all the tickets. The population mean is a parameter. C.f. sample mean. 192 Population Percentage. The percentage of units in a population that possess a specified property. For example, the percentage of a given collection of registered voters who are registered as Republicans. If each unit that possesses the property is labeled with “1,” and each unit that does not possess the property is labeled with “0,” the population percentage is the same as the mean of that list of zeros and ones; that is, the population percentage is the population mean for a population of zeros and ones. The population percentage is a parameter. C.f. sample percentage. 193 Population Standard Deviation. The standard deviation of the values of a variable for a population. This is a parameter, not a statistic. C.f. sample standard deviation. 194 Post hoc ergo propter hoc. “After this, therefore because of this.” A fallacy of logic known since classical times: inferring a causal relation from correlation. Don’t do this at home! 195 Power. Refers to an hypothesis test. The power of a test against a specific alternative hypothesis is the chance that the test correctly rejects the null hypothesis when the alternative hypothesis is true. 196 Premise, logical premise. A premise is a proposition that is assumed to be true as part of a logical argument. 197 Prima facie. Latin for “at first glance.” “On the face of it.” Prima facie evidence for something is information that at first glance supports the conclusion. On closer examination, that might not be true; there could be another explanation for the evidence. 198 Principle of insufficient reason (Laplace) Laplace’s principle of insufficient reason says that if there is no reason to believe that the possible outcomes of an experiment are not equally likely, one should assume that the outcomes are equally likely. This is an example of a fallacy called appeal to ignorance. 199 Probability. The probability of an event is a number between zero and 100%. The meaning (interpretation) of probability is the subject of theories of probability, which differ in their interpretations. However, any rule for assigning probabilities to events has to satisfy the axioms of probability. 200 Probability density function. The chance that a continuous random variable is in any range of values can be calculated as the area under a curve over that range of values. The curve is the probability density function of the random variable. That is, if X is a continuous random variable, there is a function f(x) such that for every pair of numbers a≤b, P(a≤ X ≤b) = (area under f between a and b); f is the probability density function of X. For example, the probability density function of a random variable with a standard normal distribution is the normal curve. Only continuous random variables have probability density functions. 201 Probability Distribution. The probability distribution of a random variable specifies the chance that the variable takes a value in any subset of the real numbers. (The subsets have to satisfy some technical conditions that are not important for this course.) The probability distribution of a random variable is completely characterized by the cumulative probability distribution function; the terms sometimes are used synonymously. The probability distribution of a discrete random variable can be characterized by the chance that the random variable takes each of its possible values. For example, the probability distribution of the total number of spots S showing on the roll of two fair dice can be written as a table: The probability distribution of a continuous random variable can be characterized by its probability density function. 202 Probability Histogram. A probability histogram for a random variable is analogous to a histogram of data, but instead of plotting the area of the bins proportional to the relative frequency of observations in the class interval, one plots the area of the bins proportional to the probability that the random variable is in the class interval. 203 Probability Sample. A sample drawn from a population using a random mechanism so that every element of the population has a known chance of ending up in the sample. 204 Probability, Theories of. A theory of probability is a way of assigning meaning to probability statements such as “the chance that a thumbtack lands point-up is 2/3.” That is, a theory of probability connects the mathematics of probability, which is the set of consequences of the axioms of probability, with the real world of observation and experiment. There are several common theories of probability. According to the frequency theory of probability, the probability of an event is the limit of the percentage of times that the event occurs in repeated, independent trials under essentially the same circumstances. According to the subjective theory of probability, a probability is a number that measures how strongly we believe an event will occur. The number is on a scale of 0% to 100%, with 0% indicating that we are completely sure it won’t occur, and 100% indicating that we are completely sure that it will occur. According to the theory of equally likely outcomes, if an experiment has n possible outcomes, and (for example, by symmetry) there is no reason that any of the n possible outcomes should occur preferentially to any of the others, then the chance of each outcome is 100%/n. Each of these theories has its limitations, its proponents, and its detractors. 205 Proposition, logical proposition. A logical proposition is a statement that can be either true or false. For example, “the sun is shining in Berkeley right now” is a proposition. See also &amp;, ↔, →, /, XOR, converse, contrapositive and logical argument. 206 Prosecutor’s Fallacy. The prosecutor’s fallacy consists of confusing two conditional probabilities: P(A/B) and P(B/A). For instance, P(A/B) could be the chance of observing the evidence if the accused is guilty, while P(B/A) is the chance that the accused is guilty given the evidence. The latter might not make sense at all, but even when it does, the two numbers need not be equal. This fallacy is related to a common misinterpretation of P-values. 207 Qualitative Variable. A qualitative variable is one whose values are adjectives, such as colors, genders, nationalities, etc. C.f. quantitative variable and categorical variable. 208 Quantile. The qth quantile of a list (0 &lt; q ≤ 1) is the smallest number such that the fraction q or more of the elements of the list are less than or equal to it. I.e., if the list contains n numbers, the qth quantile, is the smallest number Q such that at least n×q elements of the list are less than or equal to Q. 209 Quantitative Variable. A variable that takes numerical values for which arithmetic makes sense, for example, counts, temperatures, weights, amounts of money, etc. For some variables that take numerical values, arithmetic with those values does not make sense; such variables are not quantitative. For example, adding and subtracting social security numbers does not make sense. Quantitative variables typically have units of measurement, such as inches, people, or pounds. 210 Quartiles. There are three quartiles. The first or lower quartile (LQ) of a list is a number (not necessarily a number in the list) such that at least 1/4 of the numbers in the list are no larger than it, and at least 3/4 of the numbers in the list are no smaller than it. The second quartile is the median. The third or upper quartile (UQ) is a number such that at least 3/4 of the entries in the list are no larger than it, and at least 1/4 of the numbers in the list are no smaller than it. To find the quartiles, first sort the list into increasing order. Find the smallest integer that is at least as big as the number of entries in the list divided by four. Call that integer k. The kth element of the sorted list is the lower quartile. Find the smallest integer that is at least as big as the number of entries in the list divided by two. Call that integer l. The lth element of the sorted list is the median. Find the smallest integer that is at least as large as the number of entries in the list times 3/4. Call that integer m. The mth element of the sorted list is the upper quartile. 211 Quota Sample. A quota sample is a sample picked to match the population with respect to some summary characteristics. It is not a random sample. For example, in an opinion poll, one might select a sample so that the proportions of various ethnicities in the sample match the proportions of ethnicities in the overall population from which the sample is drawn. Matching on summary statistics does not guarantee that the sample comes close to matching the population with respect to the quantity of interest. As a result, quota samples are typically biased, and the size of the bias is generally impossible to determine unless the result can be compared with a known result for the whole population or for a random sample. Moreover, with a quota sample, it is impossible to quantify how representative of the population a quota sample is likely to be—quota sampling does not allow one to quantify the likely size of sampling error. Quota samples are to be avoided, and results based on quota samples are to be viewed with suspicion. See also convenience sample. 212 Random Error. All measurements are subject to error, which can often be broken down into two components: a bias or systematic error, which affects all measurements the same way; and a random error, which is in general different each time a measurement is made, and behaves like a number drawn with replacement from a box of numbered tickets whose average is zero. 213 Random Event. See random experiment. 214 Random Experiment. An experiment or trial whose outcome is not perfectly predictable, but for which the long-run relative frequency of outcomes of different types in repeated trials is predictable. Note that “random” is different from “haphazard,” which does not necessarily imply long-term regularity. 215 Random Sample. A random sample is a sample whose members are chosen at random from a given population in such a way that the chance of obtaining any particular sample can be computed. The number of units in the sample is called the sample size, often denoted n. The number of units in the population often is denoted N. Random samples can be drawn with or without replacing objects between draws; that is, drawing all n objects in the sample at once (a random sample without replacement), or drawing the objects one at a time, replacing them in the population between draws (a random sample with replacement). In a random sample with replacement, any given member of the population can occur in the sample more than once. In a random sample without replacement, any given member of the population can be in the sample at most once. A random sample without replacement in which every subset of n of the N units in the population is equally likely is also called a simple random sample. The term random sample with replacement denotes a random sample drawn in such a way that every multiset of n units in the population is equally likely. See also probability sample. 216 Random Variable. A random variable is an assignment of numbers to possible outcomes of a random experiment. For example, consider tossing three coins. The number of heads showing when the coins land is a random variable: it assigns the number 0 to the outcome {T, T, T}, the number 1 to the outcome {T, T, H}, the number 2 to the outcome {T, H, H}, and the number 3 to the outcome {H, H, H}. 217 Randomized Controlled Experiment. An experiment in which chance is deliberately introduced in assigning subjects to the treatment and control groups. For example, we could write an identifying number for each subject on a slip of paper, stir up the slips of paper, and draw slips without replacement until we have drawn half of them. The subjects identified on the slips drawn could then be assigned to treatment, and the rest to control. Randomizing the assignment tends to decrease confounding of the treatment effect with other factors, by making the treatment and control groups roughly comparable in all respects but the treatment. 218 Range. The range of a set of numbers is the largest value in the set minus the smallest value in the set. Note that as a statistical term, the range is a single number, not a range of numbers. 219 Real number. Loosely speaking, the real numbers are all numbers that can be represented as fractions (rational numbers), whether proper or improper—and all numbers in between the rational numbers. That is, the real numbers comprise the rational numbers and all limits of Cauchy sequences of rational numbers, where the Cauchy sequence is with respect to the absolute value metric. (More formally, the real numbers are the completion of the set of rational numbers in the topology induced by the absolute value function.) The real numbers contain all integers, all fractions, and all irrational (and transcendental) numbers, such as π, e, and 2½. There are uncountably many real numbers between 0 and 1; in contrast, there are only countably many rational numbers between 0 and 1. 220 Regression, Linear Regression. Linear regression fits a line to a scatterplot in such a way as to minimize the sum of the squares of the residuals. The resulting regression line, together with the standard deviations of the two variables or their correlation coefficient, can be a reasonable summary of a scatterplot if the scatterplot is roughly football-shaped. In other cases, it is a poor summary. If we are regressing the variable Y on the variable X, and if Y is plotted on the vertical axis and X is plotted on the horizontal axis, the regression line passes through the point of averages, and has slope equal to the correlation coefficient times the SD of Y divided by the SD of X. This page shows a scatterplot, with a button to plot the regression line. 221 Regression Fallacy. The regression fallacy is to attribute the regression effect to an external cause. 222 Regression Toward the Mean, Regression Effect. Suppose one measures two variables for each member of a group of individuals, and that the correlation coefficient of the variables is positive (negative). If the value of the first variable for that individual is above average, the value of the second variable for that individual is likely to be above (below) average, but by fewer standard deviations than the first variable is. That is, the second observation is likely to be closer to the mean in standard units. For example, suppose one measures the heights of fathers and sons. Each individual is a (father, son) pair; the two variables measured are the height of the father and the height of the son. These two variables will tend to have a positive correlation coefficient: fathers who are taller than average tend to have sons who are taller than average. Consider a (father, son) pair chosen at random from this group. Suppose the father’s height is 3SD above the average of all the fathers’ heights. (The SD is the standard deviation of the fathers’ heights.) Then the son’s height is also likely to be above the average of the sons’ heights, but by fewer than 3SD (here the SD is the standard deviation of the sons’ heights). 223 Rejection region. In an hypothesis test using a test statistic, the rejection region is the set of values of the test statistic for which we reject the null hypothesis. 224 Residual. The difference between a datum and the value predicted for it by a model. In linear regression of a variable plotted on the vertical axis onto a variable plotted on the horizontal axis, a residual is the “vertical” distance from a datum to the line. Residuals can be positive (if the datum is above the line) or negative (if the datum is below the line). Plots of residuals can reveal computational errors in linear regression, as well as conditions under which linear regression is inappropriate, such as nonlinearity and heteroscedasticity. If linear regression is performed properly, the sum of the residuals from the regression line must be zero; otherwise, there is a computational error somewhere. 225 Residual Plot. A residual plot for a regression is a plot of the residuals from the regression against the explanatory variable. 226 Resistant. A statistic is said to be resistant if corrupting a datum cannot change the statistic much. The mean is not resistant; the median is. See also breakdown point. 227 Root-mean-square (RMS). The RMS of a list is the square-root of the mean of the squares of the elements in the list. It is a measure of the average “size” of the elements of the list. To compute the RMS of a list, you square all the entries, average the numbers you get, and take the square-root of that average. 228 Root-mean-square error (RMSE). The RMSE of an an estimator of a parameter is the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then RMSE(X) = ( E( (X−t)2 ) )½. The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator. See also mean squared error. 229 rms Error of Regression The rms error of regression is the rms of the vertical residuals from the regression line. For regressing Y on X, the rms error of regression is equal to (1 − r2)½×SDY, where r is the correlation coefficient between X and Y and SDY is the standard deviation of the values of Y. 230 Sample. A sample is a collection of units from a population. See also random sample. 231 Sample Mean. The arithmetic mean of a random sample from a population. It is a statistic commonly used to estimate the population mean. Suppose there are n data, {x1, x2, … , xn}. The sample mean is (x1 &#43; x2 &#43; … &#43; xn)/n. The expected value of the sample mean is the population mean. For sampling with replacement, the SE of the sample mean is the population standard deviation, divided by the square-root of the sample size. For sampling without replacement, the SE of the sample mean is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample mean for sampling with replacement, with N the size of the population and n the size of the sample. 232 Sample Percentage. The percentage of a random sample with a certain property, such as the percentage of voters registered as Democrats in a simple random sample of voters. The sample mean is a statistic commonly used to estimate the population percentage. The expected value of the sample percentage from a simple random sample or a random sample with replacement is the population percentage. The SE of the sample percentage for sampling with replacement is (p(1−p)/n )½, where p is the population percentage and n is the sample size. The SE of the sample percentage for sampling without replacement is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample percentage for sampling with replacement, with N the size of the population and n the size of the sample. The SE of the sample percentage is often estimated by the bootstrap. 233 Sample Size. The number of elements in a sample from a population. 234 Sound argument. A logical argument is sound if it is logically valid and its premises are in fact true. An argument can be logically valid and yet not sound—if its premises are false. 235 Sample Standard Deviation, S. The sample standard deviation S is an estimator of the standard deviation of a population based on a random sample from the population. The sample standard deviation is a statistic that measures how “spread out” the sample is around the sample mean. It is quite similar to the standard deviation of the sample, but instead of averaging the squared deviations (to get the rms of the deviations of the data from the sample mean) it divides the sum of the squared deviations by (number of data − 1) before taking the square-root. Suppose there are n data, {x1, x2, … , xn}, with mean M = (x1 &#43; x2 &#43; … &#43; xn)/n. Then s = ( ((x1 − M)2 &#43; (x2 − M)2 &#43; … &#43; (xn − M)2)/(n−1) )½ The square of the sample standard deviation, S2 (the sample variance) is an unbiased estimator of the square of the SD of the population (the variance of the population). 236 Sample Sum. The sum of a random sample from a population. The expected value of the sample sum is the sample size times the population mean. For sampling with replacement, the SE of the sample sum is the population standard deviation, times the square-root of the sample size. For sampling without replacement, the SE of the sample sum is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample sum for sampling with replacement, with N the size of the population and n the size of the sample. 237 Sample Survey. A survey based on the responses of a sample of individuals, rather than the entire population. 238 Sample Variance The sample variance is the square of the sample standard deviation S. It is an unbiased estimator of the square of the population standard deviation, which is also called the variance of the population. 239 Sampling distribution. The sampling distribution of an estimator is the probability distribution of the estimator when it is applied to random samples. The tool on this page allows you to explore empirically the sampling distribution of the sample mean and the sample percentage of random draws with or without replacement draws from a box of numbered tickets. 240 Sampling error. In estimating from a random sample, the difference between the estimator and the parameter can be written as the sum of two components: bias and sampling error. The bias is the average error of the estimator over all possible samples. The bias is not random. Sampling error is the component of error that varies from sample to sample. The sampling error is random: it comes from “the luck of the draw” in which units happen to be in the sample. It is the chance variation of the estimator. The average of the sampling error over all possible samples (the expected value of the sampling error) is zero. The standard error of the estimator is a measure of the typical size of the sampling error. 241 Sampling unit. A sample from a population can be drawn one unit at a time, or more than one unit at a time (one can sample clusters of units). The fundamental unit of the sample is called the sampling unit. It need not be a unit of the population. 242 Scatterplot. A scatterplot is a way to visualize bivariate data. A scatterplot is a plot of pairs of measurements on a collection of “individuals” (which need not be people). For example, suppose we record the heights and weights of a group of 100 people. The scatterplot of those data would be 100 points. Each point represents one person’s height and weight. In a scatterplot of weight against height, the x-coordinate of each point would be height of one person, the y-coordinate of that point would be the weight of the same person. In a scatterplot of height against weight, the x-coordinates would be the weights and the y-coordinates would be the heights. 243 Scientific Method. The scientific method…. 244 SD line. For a scatterplot, a line that goes through the point of averages, with slope equal to the ratio of the standard deviations of the two plotted variables. If the variable plotted on the horizontal axis is called X and the variable plotted on the vertical axis is called Y, the slope of the SD line is the SD of Y, divided by the SD of X. 245 Secular Trend. A linear association (trend) with time. 246 Selection Bias. A systematic tendency for a sampling procedure to include and/or exclude units of a certain type. For example, in a quota sample, unconscious prejudices or predilections on the part of the interviewer can result in selection bias. Selection bias is a potential problem whenever a human has latitude in selecting individual units for the sample; it tends to be eliminated by probability sampling schemes in which the interviewer is told exactly whom to contact (with no room for individual choice). 247 Self-Selection. Self-selection occurs when individuals decide for themselves whether they are in the control group or the treatment group. Self-selection is quite common in studies of human behavior. For example, studies of the effect of smoking on human health involve self-selection: individuals choose for themselves whether or not to smoke. Self-selection precludes an experiment; it results in an observational study. When there is self-selection, one must be wary of possible confounding from factors that influence individuals’ decisions to belong to the treatment group. 248 Set. A set is a collection of things (called elements), without regard to their order. An item is either in a set (it is an element of the set), or it is not. It cannot be in the set more than once. Two sets are equal if they contain electly the same elements. For instance, the set {1, 2, 3, 4} is equal to the set {1, 4, 3, 2}, but not to the set {0, 1, 2, 3}. As another example, the set {1, 2, 2} is equal to the set {1, 2}: they have the same two (distinct) elements, 1 and 2. 249 Significance, Significance level, Statistical significance. The significance level of an hypothesis test is the chance that the test erroneously rejects the null hypothesis when the null hypothesis is true. 250 Simple Random Sample. A simple random sample of n units from a population is a random sample drawn by a procedure that is equally likely to give every collection of n units from the population; that is, the probability that the sample will consist of any given subset of n of the N units in the population is 1/NCn. Simple random sampling is sampling at random without replacement (without replacing the units between draws). A simple random sample of size n from a population of N ≥ n units can be constructed by assigning a random number between zero and one to each unit in the population, then taking those units that were assigned the n largest random numbers to be the sample. 251 Simpson’s Paradox. What is true for the parts is not necessarily true for the whole. See also confounding. 252 Skewed Distribution. A distribution that is not symmetrical. 253 Spread, Measure of. See also inter-quartile range, range, and standard deviation. 254 Square-Root Law. The Square-Root Law says that the standard error (SE) of the sample sum of n random draws with replacement from a box of tickets with numbers on them is SE(sample sum) = n½×SD(box), and the standard error of the sample mean of n random draws with replacement from a box of tickets is SE(sample mean) = n−½×SD(box), where SD(box) is the standard deviation of the list of the numbers on all the tickets in the box (including repeated values). 255 Standard Deviation (SD). The standard deviation of a set of numbers is the rms of the set of deviations between each element of the set and the mean of the set. See also sample standard deviation. 256 Standard Error (SE). The Standard Error of a random variable is a measure of how far it is likely to be from its expected value; that is, its scatter in repeated experiments. The SE of a random variable X is defined to be SE(X) = [E( (X − E(X))2 )] ½. That is, the standard error is the square-root of the expected squared difference between the random variable and its expected value. The SE of a random variable is analogous to the SD of a list. 257 Standard Normal Curve. See normal curve. 258 Standard Units. A variable (a set of data) is said to be in standard units if its mean is zero and its standard deviation is one. You transform a set of data into standard units by subtracting the mean from each element of the list, and dividing the results by the standard deviation. A random variable is said to be in standard units if its expected value is zero and its standard error is one. You transform a random variable to standard units by subtracting its expected value then dividing by its standard error. 259 Standardize. To transform into standard units. 260 Statistic. A number that can be computed from data, involving no unknown parameters. As a function of a random sample, a statistic is a random variable. Statistics are used to estimate parameters, and to test hypotheses. 261 Stratified Sample. In a stratified sample, subsets of sampling units are selected separately from different strata, rather than from the frame as a whole. 262 Stratified sampling The act of drawing a stratified sample. 263 Stratum In random sampling, sometimes the sample is drawn separately from different disjoint subsets of the population. Each such subset is called a stratum. (The plural of stratum is strata.) Samples drawn in such a way are called stratified samples. Estimators based on stratified random samples can have smaller sampling errors than estimators computed from simple random samples of the same size, if the average variability of the variable of interest within strata is smaller than it is across the entire population; that is, if stratum membership is associated with the variable. For example, to determine average home prices in the U.S., it would be advantageous to stratify on geography, because average home prices vary enormously with location. We might divide the country into states, then divide each state into urban, suburban, and rural areas; then draw random samples separately from each such division. 264 Studentized score The observed value of a statistic, minus the expected value of the statistic, divided by the estimated standard error of the statistic. 265 Student’s t curve. Student’s t curve is a family of curves indexed by a parameter called the degrees of freedom, which can take the values 1, 2, … Student’s t curve is used to approximate some probability histograms. Consider a population of numbers that are nearly normally distributed and have population mean is μ. Consider drawing a random sample of size n with replacement from the population, and computing the sample mean M and the sample standard deviation S. Define the random variable T = (M − μ)/(S/n½). If the sample size n is large, the probability histogram of T can be approximated accurately by the normal curve. However, for small and intermediate values of n, Student’s t curve with n − 1 degrees of freedom gives a better approximation. That is, P(a &lt; T &lt; b) is approximately the area under Student’s T curve with n − 1 degrees of freedom, from a to b. Student’s t curve can be used to test hypotheses about the population mean and construct confidence intervals for the population mean, when the population distribution is known to be nearly normally distributed. This page contains a tool that shows Student’s t curve and lets you find the area under parts of the curve. 266 Subject, Experimental Subject. A member of the control group or the treatment group. 267 Subset. A subset of a given set is a collection of things that belong to the original set. Every element of the subset must belong to the original set, but not every element of the original set need be in a subset (otherwise, a subset would always be identical to the set it came from). 268 Survey. See sample survey. 269 Symmetric Distribution. The probability distribution of a random variable X is symmetric if there is a number a such that the chance that X≥a&#43;b is the same as the chance that X≤a−b for every value of b. A list of numbers has a symmetric distribution if there is a number a such that the fraction of numbers in the list that are greater than or equal to a&#43;b is the same as the fraction of numbers in the list that are less than or equal to a−b, for every value of b. In either case, the histogram or the probability histogram will be symmetrical about a vertical line drawn at x=a. 270 Systematic error. An error that affects all the measurements similarly. For example, if a ruler is too short, everything measured with it will appear to be longer than it really is (ignoring random error). If your watch runs fast, every time interval you measure with it will appear to be longer than it really is (again, ignoring random error). Systematic errors do not tend to average out. 271 Systematic sample. A systematic sample from a frame of units is one drawn by listing the units and selecting every kth element of the list. For example, if there are N units in the frame, and we want a sample of size N/10, we would take every tenth unit: the first unit, the eleventh unit, the 21st unit, etc. Systematic samples are not random samples, but they often behave essentially as if they were random, if the order in which the units appears in the list is haphazard. Systematic samples are a special case of cluster samples. 272 Systematic random sample. A systematic sample starting at a random point in the listing of units in the of frame, instead of starting at the first unit. Systematic random sampling is better than systematic sampling, but typically not as good as simple random sampling. 273 t test. An hypothesis test based on approximating the probability histogram of the test statistic by Student’s t curve. t tests usually are used to test hypotheses about the mean of a population when the sample size is intermediate and the distribution of the population is known to be nearly normal. 274 Test Statistic. A statistic used to test hypotheses. An hypothesis test can be constructed by deciding to reject the null hypothesis when the value of the test statistic is in some range or collection of ranges. To get a test with a specified significance level, the chance when the null hypothesis is true that the test statistic falls in the range where the hypothesis would be rejected must be at most the specified significance level. The Z statistic is a common test statistic. 275 Transformation. Transformations turn lists into other lists, or variables into other variables. For example, to transform a list of temperatures in degrees Celsius into the corresponding list of temperatures in degrees Fahrenheit, you multiply each element by 9/5, and add 32 to each product. This is an example of an affine transformation: multiply by something and add something (y = ax &#43; b is the general affine transformation of x; it’s the familiar equation of a straight line). In a linear transformation, you only multiply by something (y = ax). Affine transformations are used to put variables in standard units. In that case, you subtract the mean and divide the results by the SD. This is equivalent to multiplying by the reciprocal of the SD and adding the negative of the mean, divided by the SD, so it is an affine transformation. Affine transformations with positive multiplicative constants have a simple effect on the mean, median, mode, quartiles, and other percentiles: the new value of any of these is the old one, transformed using exactly the same formula. When the multiplicative constant is negative, the mean, median, mode, are still transformed by the same rule, but quartiles and percentiles are reversed: the qth quantile of the transformed distribution is the transformed value of the 1−qth quantile of the original distribution (ignoring the effect of data spacing). The effect of an affine transformation on the SD, range, and IQR, is to make the new value the old value times the absolute value of the number you multiplied the first list by: what you added does not affect them. 276 Treatment. The substance or procedure studied in an experiment or observational study. At issue is whether the treatment has an effect on the outcome or variable of interest. 277 Treatment Effect. The effect of the treatment on the variable of interest. Establishing whether the treatment has an effect is the point of an experiment. 278 Treatment group. The individuals who receive the treatment, as opposed to those in the control group, who do not. 279 Tuple, n-tuple. A tuple is an ordered collection of things. Two tuples are equal if they contain the same things, in the same order. For instance, the tuple (1, 2, 3) is equal to the tuple (1, 2, 3) but not equal to the tuple (1, 3, 2). Tuples can contain repeated elements. For instance, the tuple (1, 2, 2) is not equal to the tuple (1, 2), nor to the tuple (2, 2, 1). An n-tuple, where n is an integer, is a tuple with n positions. For example, (1, 2) is a 2-tuple (aka ordered pair) and (7, 3, 2, 2, 2, 1) is a 6-tuple. 280 Two-sided Hypothesis test. C.f. one-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against the alternative hypothesis that either μ &lt; μ0 or μ &gt; μ0 (the alternative hypothesis contains values on both sides of the null value). For example, a significance level 5%, two-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero would reject the null hypothesis for values of $$ /z/ = \left / \frac{\mbox{sample mean}}{\mbox{SE}} \right / &gt; 1.96.$$ 281 Type I and Type II errors. These refer to hypothesis testing. A Type I error occurs when the null hypothesis is rejected erroneously when it is in fact true. A Type II error occurs if the null hypothesis is not rejected when it is in fact false. See also significance level and power. 282 Unbiased. Not biased; having zero bias. 283 Uncontrolled Experiment. An experiment in which there is no control group; i.e., in which the method of comparison is not used: the experimenter decides who gets the treatment, but the outcome of the treated group is not compared with the outcome of a control group that does not receive treatment. 284 Uncorrelated. A set of bivariate data is uncorrelated if its correlation coefficient is zero. Two random variables are uncorrelated if the expected value of their product equals the product of their expected values. If two random variables are independent, they are uncorrelated. (The converse is not true in general.) 285 Uncountable. A set is uncountable if it is not countable, that is, if its elements cannot be put in one-to-one correspondence with the positive integers. 286 Unimodal. Having exactly one mode. 287 Union. The union of two or more sets is the set of objects contained by at least one of the sets. The union of the events A and B is denoted “A&#43;B”, “A or B”, and “A∪B”. C.f. intersection. 288 Unit. A member of a population. 289 Univariate. Having or having to do with a single variable. Some univariate techniques and statistics include the histogram, IQR, mean, median, percentiles, quantiles, and SD. C.f. bivariate. 290 Upper Quartile (UQ). See quartiles. 291 Valid (logical) argument. A valid logical argument is one in which the truth of the premises indeed guarantees the truth of the conclusion. For example, the following logical argument is valdraft: false id: If the forecast calls for rain, I will not wear sandals. The forecast calls for rain. Therefore, I will not wear sandals. This argument has two premises which, together, guarantee the truth of the conclusion. An argument can be logically valid even if its premises are false. See also invalid argument and sound argument. 292 Variable. A numerical value or a characteristic that can differ from individual to individual. See also categorical variable, qualitative variable, quantitative variable, discrete variable, continuous variable, and random variable. 293 Variance, population variance The variance of a list is the square of the standard deviation of the list, that is, the average of the squares of the deviations of the numbers in the list from their mean. The variance of a random variable X, Var(X), is the expected value of the squared difference between the variable and its expected value: Var(X) = E((X − E(X))2). The variance of a random variable is the square of the standard error (SE) of the variable. 294 Venn Diagram. A pictorial way of showing the relations among sets or events. The universal set or outcome space is usually drawn as a rectangle; sets are regions within the rectangle. The overlap of the regions corresponds to the intersection of the sets. If the regions do not overlap, the sets are disjoint. The part of the rectangle included in one or more of the regions corresponds to the union of the sets. This page contains a tool that illustrates Venn diagrams; the tool represents the probability of an event by the area of the event. 295 XOR, exclusive disjunction. XOR is an operation on two logical propositions. If p and q are two propositions, (p XOR q) is a proposition that is true if either p is true or if q is true, but not both. (p XOR q) is logically equivalent to ((p 296 z-score The observed value of the Z statistic. 297 Z statistic A Z statistic is a test statistic whose distribution under the null hypothesis has expected value zero and can be approximated well by the normal curve. Usually, Z statistics are constructed by standardizing some other statistic. The Z statistic is related to the original statistic by Z = (original − expected value of original)/SE(original). 298 z-test An hypothesis test based on approximating thehttps://dasarpai.com/300-important-statistical-terms/ probability histogram of the Z statistic under the null hypothesis by the normal curve. These definitions are taken from - https://www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
    <meta property="article:published_time" content="2021-09-29T15:50:00+05:30">
    <meta property="article:modified_time" content="2021-09-29T15:50:00+05:30">
    <meta property="article:tag" content="Statistical Terms">
    <meta property="article:tag" content="Statistics">
    <meta property="article:tag" content="Statistics for Data Science">

  <meta itemprop="name" content="300 Important Statistical Terms">
  <meta itemprop="description" content="Important Statistical Terms Sno Term Definition 1 Affine transformation. See transformation. 2 Affirming the antecedent. A valid logical argument that concludes from the premise A → B and the premise A that therefore, B is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the antecedent (A) in the conditional. 3 Affirming the consequent. A logical fallacy that argues from the premise A → B and the premise B that therefore, A is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the consequent (B) in the conditional. 4 Alternative Hypothesis. In hypothesis testing, a null hypothesis (typically that there is no effect) is compared with an alternative hypothesis (typically that there is an effect, or that there is an effect of a particular sign). For example, in evaluating whether a new cancer remedy works, the null hypothesis typically would be that the remedy does not work, while the alternative hypothesis would be that the remedy does work. When the data are sufficiently improbable under the assumption that the null hypothesis is true, the null hypothesis is rejected in favor of the alternative hypothesis. (This does not imply that the data are probable under the assumption that the alternative hypothesis is true, nor that the null hypothesis is false, nor that the alternative hypothesis is true. Confused? Take a course in Statistics!) 5 Ante. The up-front cost of a bet: the money you must pay to play the game. From Latin for “before.” 6 Antecedent. In a conditional p → q, the antecedent is p. 7 Appeal to Ignorance. A logical fallacy: taking the absence of evidence to be evidence of absence. If something is not known to be false, assume that it is true; or if something is not known to be true, assume that it is false. For example, if I have no reason to think that anyone in Tajikistan wish me well, that is not evidence that nobody in Tajikistan wishes me well. 8 Association. Two variables are associated if some of the variability of one can be accounted for by the other. In a scatterplot of the two variables, if the scatter in the values of the variable plotted on the vertical axis is smaller in narrow ranges of the variable plotted on the horizontal axis (i.e., in vertical “slices”) than it is overall, the two variables are associated. The correlation coefficient is a measure of linear association, which is a special case of association in which large values of one variable tend to occur with large values of the other, and small values of one tend to occur with small values of the other (positive association), or in which large values of one tend to occur with small values of the other, and vice versa (negative association). 9 Average. An ambiguous term. It often denotes the arithmetic mean, but it can also denote the median, the mode, the geometric mean, and weighted means, among other things. Beware if something reports “the average” without making it clear which average. 10 Axioms of Probability. There are three axioms of probability: (1) Chances are always at least zero. (2) The chance that something happens is 100%. (3) If two events cannot both occur at the same time (if they are disjoint or mutually exclusive), the chance that either one occurs is the sum of the chances that each occurs. For example, consider an experiment that consists of tossing a coin once. The first axiom says that the chance that the coin lands heads, for instance, must be at least zero. The second axiom says that the chance that the coin either lands heads or lands tails or lands on its edge or doesn’t land at all is 100%. The third axiom says that the chance that the coin either lands heads or lands tails is the sum of the chance that the coin lands heads and the chance that the coin lands tails, because both cannot occur in the same coin toss. All other mathematical facts about probability can be derived from these three axioms. For example, it is true that the chance that an event does not occur is (100% − the chance that the event occurs). This is a consequence of the second and third axioms. 11 Base rate fallacy. The base rate fallacy consists of failing to take into account prior probabilities (base rates) when computing conditional probabilities from other conditional probabilities. It is related to the Prosecutor’s Fallacy. For instance, suppose that a test for the presence of some condition has a 1% chance of a false positive result (the test says the condition is present when it is not) and a 1% chance of a false negative result (the test says the condition is absent when the condition is present), so the exam is 99% accurate. What is the chance that an item that tests positive really has the condition? The intuitive answer is 99%, but that is not necessarily true: the correct answer depends on the fraction f of items in the population that have the condition (and on whether the item tested is selected at random from the population). The chance that a randomly selected item tests positive is 0.99×f/(0.99×f &#43; 0.01×(1−f)), which could be much smaller than 99% if f is small. See Bayes’ Rule. 12 Bayes’ Rule. Bayes’ rule expresses the conditional probability of the event A given the event B in terms of the conditional probability of the event B given the event A and the unconditional probability of A: P(A/B) = P(B/A) ×P(A)/( P(B/A)×P(A) &#43; P(B/Ac) ×P(Ac) ). In this expression, the unconditional probability of A is also called the prior probability of A, because it is the probability assigned to A prior to observing any data. Similarly, in this context, P(A/B) is called the posterior probability of A given B, because it is the probability of A updated to reflect (i.e., to condition on) the fact that B was observed to occur. 13 Bernoulli’s Inequality. The Bernoulli Inequality says that if x ≥ −1 then (1&#43;x)n ≥ 1 &#43; nx for every integer n ≥ 0. If n is even, the inequality holds for all x. 14 Bias. A measurement procedure or estimator is said to be biased if, on the average, it gives an answer that differs from the truth. The bias is the average (expected) difference between the measurement and the truth. For example, if you get on the scale with clothes on, that biases the measurement to be larger than your true weight (this would be a positive bias). The design of an experiment or of a survey can also lead to bias. Bias can be deliberate, but it is not necessarily so. See also nonresponse bias. 15 Bimodal. Having two modes. 16 Bin. See class interval. 17 Binomial Coefficient. See combinations. 18 Binomial Distribution. A random variable has a binomial distribution (with parameters n and p) if it is the number of “successes” in a fixed number n of independent random trials, all of which have the same probability p of resulting in “success.” Under these assumptions, the probability of k successes (and n−k failures) is nCk pk(1−p)n−k, where nCk is the number of combinations of n objects taken k at a time: nCk = n!/(k!(n−k)!). The expected value of a random variable with the Binomial distribution is n×p, and the standard error of a random variable with the Binomial distribution is (n×p×(1 − p))½. This page shows the probability histogram of the binomial distribution. 19 Binomial Theorem. The Binomial theorem says that (x&#43;y)n = xn &#43; nxn−1y &#43; … &#43; nCkxn−kyk &#43; … &#43; yn. 20 Bivariate. Having or having to do with two variables. For example, bivariate data are data where we have two measurements of each “individual.” These measurements might be the heights and weights of a group of people (an “individual” is a person), the heights of fathers and sons (an “individual” is a father-son pair), the pressure and temperature of a fixed volume of gas (an “individual” is the volume of gas under a certain set of experimental conditions), etc. Scatterplots, the correlation coefficient, and regression make sense for bivariate data but not univariate data. C.f. univariate. 21 Blind, Blind Experiment. In a blind experiment, the subjects do not know whether they are in the treatment group or the control group. In order to have a blind experiment with human subjects, it is usually necessary to administer a placebo to the control group. 22 Bootstrap estimate of Standard Error. The name for this idea comes from the idiom “to pull oneself up by one’s bootstraps,” which connotes getting out of a hole without anything to stand on. The idea of the bootstrap is to assume, for the purposes of estimating uncertainties, that the sample is the population, then use the SE for sampling from the sample to estimate the SE of sampling from the population. For sampling from a box of numbers, the SD of the sample is the bootstrap estimate of the SD of the box from which the sample is drawn. For sample percentages, this takes a particularly simple form: the SE of the sample percentage of n draws from a box, with replacement, is SD(box)/n½, where for a box that contains only zeros and ones, SD(box) = ((fraction of ones in box)×(fraction of zeros in box) )½. The bootstrap estimate of the SE of the sample percentage consists of estimating SD(box) by ((fraction of ones in sample)×(fraction of zeros in sample))½. When the sample size is large, this approximation is likely to be good. 23 Box model. An analogy between an experiment and drawing numbered tickets “at random” from a box with replacement. For example, suppose we are trying to evaluate a cold remedy by giving it or a placebo to a group of n individuals, randomly choosing half the individuals to receive the remedy and half to receive the placebo. Consider the median time to recovery for all the individuals (we assume everyone recovers from the cold eventually; to simplify things, we also assume that no one recovered in exactly the median time, and that n is even). By definition, half the individuals got better in less than the median time, and half in more than the median time. The individuals who received the treatment are a random sample of size n/2 from the set of n subjects, half of whom got better in less than median time, and half in longer than median time. If the remedy is ineffective, the number of subjects who received the remedy and who recovered in less than median time is like the sum of n/2 draws with replacement from a box with two tickets in it: one with a “1” on it, and one with a “0” on it. This page illustrates the sampling distribution of random draws with or without from a box of numbered tickets. 24 Breakdown Point. The breakdown point of an estimator is the smallest fraction of observations one must corrupt to make the estimator take any value one wants. 25 Categorical Variable. A variable whose value ranges over categories, such as {red, green, blue}, {male, female}, {Arizona, California, Montana, New York}, {short, tall}, {Asian, African-American, Caucasian, Hispanic, Native American, Polynesian}, {straight, curly}, etc. Some categorical variables are ordinal. The distinction between categorical variables and qualitative variables is a bit blurry. C.f. quantitative variable. 26 Causation, causal relation. Two variables are causally related if changes in the value of one cause the other to change. For example, if one heats a rigid container filled with a gas, that causes the pressure of the gas in the container to increase. Two variables can be associated without having any causal relation, and even if two variables have a causal relation, their correlation can be small or zero. 27 Central Limit Theorem. The central limit theorem states that the probability histograms of the sample mean and sample sum of n draws with replacement from a box of labeled tickets converge to a normal curve as the sample size n grows, in the following sense: As n grows, the area of the probability histogram for any range of values approaches the area under the normal curve for the same range of values, converted to standard units. See also the normal approximation. 28 Certain Event. An event is certain if its probability is 100%. Even if an event is certain, it might not occur. However, by the complement rule, the chance that it does not occur is 0%. 29 Chance variation, chance error. A random variable can be decomposed into a sum of its expected value and chance variation around its expected value. The expected value of the chance variation is zero; the standard error of the chance variation is the same as the standard error of the random variable—the size of a “typical” difference between the random variable and its expected value. See also sampling error. 30 Change of Units or Variables. See also transformation. 31 Chebychev’s Inequality. For lists: For every number k&gt;0, the fraction of elements in a list that are k SD’s or further from the arithmetic mean of the list is at most 1/k2. For random variables: For every number k&gt;0, the probability that a random variable X is k SEs or further from its expected value is at most 1/k2. 32 Chi-square curve. The chi-square curve is a family of curves that depend on a parameter called degrees of freedom (d.f.). The chi-square curve is an approximation to the probability histogram of the chi-square statistic for multinomial model if the expected number of outcomes in each category is large. The chi-square curve is positive, and its total area is 100%, so we can think of it as the probability histogram of a random variable. The balance point of the curve is d.f., so the expected value of the corresponding random variable would equal d.f.. The standard error of the corresponding random variable would be (2×d.f.)½. As d.f. grows, the shape of the chi-square curve approaches the shape of the normal curve. This page shows the chi-square curve. 33 Chi-square Statistic. The chi-square statistic is used to measure the agreement between categorical data and a multinomial model that predicts the relative frequency of outcomes in each possible category. Suppose there are n independent trials, each of which can result in one of k possible outcomes. Suppose that in each trial, the probability that outcome i occurs is pi, for i = 1, 2, … , k, and that these probabilities are the same in every trial. The expected number of times outcome 1 occurs in the n trials is n×p1; more generally, the expected number of times outcome i occurs is expectedi = n×pi. If the model be correct, we would expect the n trials to result in outcome i about n×pi times, give or take a bit. Let observedi denote the number of times an outcome of type i occurs in the n trials, for i = 1, 2, … , k. The chi-squared statistic summarizes the discrepancies between the expected number of times each outcome occurs (assuming that the model is true) and the observed number of times each outcome occurs, by summing the squares of the discrepancies, normalized by the expected numbers, over all the categories: chi-squared = (observed1 − expected1)2/expected1 &#43; (observed2 − expected2)2/expected2 &#43; … &#43; (observedk − expectedk)2/expectedk. As the sample size n increases, if the model is correct, the sampling distribution of the chi-squared statistic is approximated increasingly well by the chi-squared curve with (#categories − 1) = k − 1 degrees of freedom (d.f.), in the sense that the chance that the chi-squared statistic is in any given range grows closer and closer to the area under the Chi-Squared curve over the same range. This page illustrates the sampling distribution of the chi-square statistic. 34 Class Boundary. A point that is the left endpoint of one class interval, and the right endpoint of another class interval. 35 Class Interval. In plotting a histogram, one starts by dividing the range of values into a set of non-overlapping intervals, called class intervals, in such a way that every datum is contained in some class interval. See the related entries class boundary and endpoint convention. 36 Cluster Sample. In a cluster sample, the sampling unit is a collection of population units, not single population units. For example, techniques for adjusting the U.S. census start with a sample of geographic blocks, then (try to) enumerate all inhabitants of the blocks in the sample to obtain a sample of people. This is an example of a cluster sample. (The blocks are chosen separately from different strata, so the overall design is a stratified cluster sample.) 37 Combinations. The number of combinations of n things taken k at a time is the number of ways of picking a subset of k of the n things, without replacement, and without regard to the order in which the elements of the subset are picked. The number of such combinations is nCk = n!/(k!(n−k)!), where k! (pronounced “k factorial”) is k×(k−1)×(k−2)× … × 1. The numbers nCk are also called the Binomial coefficients. From a set that has n elements one can form a total of 2n subsets of all sizes. For example, from the set {a, b, c}, which has 3 elements, one can form the 23 = 8 subsets {}, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}. Because the number of subsets with k elements one can form from a set with n elements is nCk, and the total number of subsets of a set is the sum of the numbers of possible subsets of each size, it follows that nC0&#43;nC1&#43;nC2&#43; … &#43;nCn = 2n. The calculator has a button (nCm) that lets you compute the number of combinations of m things chosen from a set of n things. To use the button, first type the value of n, then push the nCm button, then type the value of m, then press the “=” button. 38 Complement. The complement of a subset of a given set is the collection of all elements of the set that are not elements of the subset. 39 Complement rule. The probability of the complement of an event is 100% minus the probability of the event: P(Ac) = 100% − P(A). 40 Compound proposition. A logical proposition formed from other propositions using logical operations such as !, /, XOR, &amp;, → and ↔. 41 Conditional Probability. Suppose we are interested in the probability that some event A occurs, and we learn that the event B occurred. How should we update the probability of A to reflect this new knowledge? This is what the conditional probability does: it says how the additional knowledge that B occurred should affect the probability that A occurred quantitatively. For example, suppose that A and B are mutually exclusive. Then if B occurred, A did not, so the conditional probability that A occurred given that B occurred is zero. At the other extreme, suppose that B is a subset of A, so that A must occur whenever B does. Then if we learn that B occurred, A must have occurred too, so the conditional probability that A occurred given that B occurred is 100%. For in-between cases, where A and B intersect, but B is not a subset of A, the conditional probability of A given B is a number between zero and 100%. Basically, one “restricts” the outcome space S to consider only the part of S that is in B, because we know that B occurred. For A to have happened given that B happened requires that AB happened, so we are interested in the event AB. To have a legitimate probability requires that P(S) = 100%, so if we are restricting the outcome space to B, we need to divide by the probability of B to make the probability of this new S be 100%. On this scale, the probability that AB happened is P(AB)/P(B). This is the definition of the conditional probability of A given B, provided P(B) is not zero (division by zero is undefined). Note that the special cases AB = {} (A and B are mutually exclusive) and AB = B (B is a subset of A) agree with our intuition as described at the top of this paragraph. Conditional probabilities satisfy the axioms of probability, just as ordinary probabilities do. 42 Confidence Interval. A confidence interval for a parameter is a random interval constructed from data in such a way that the probability that the interval contains the true value of the parameter can be specified before the data are collected. Confidence intervals are demonstrated in this page. 43 Confidence Level. The confidence level of a confidence interval is the chance that the interval that will result once data are collected will contain the corresponding parameter. If one computes confidence intervals again and again from independent data, the long-term limit of the fraction of intervals that contain the parameter is the confidence level. 44 Confounding. When the differences between the treatment and control groups other than the treatment produce differences in response that are not distinguishable from the effect of the treatment, those differences between the groups are said to be confounded with the effect of the treatment (if any). For example, prominent statisticians questioned whether differences between individuals that led some to smoke and others not to (rather than the act of smoking itself) were responsible for the observed difference in the frequencies with which smokers and non-smokers contract various illnesses. If that were the case, those factors would be confounded with the effect of smoking. Confounding is quite likely to affect observational studies and experiments that are not randomized. Confounding tends to be decreased by randomization. See also Simpson’s Paradox. 45 Continuity Correction. In using the normal approximation to the binomial probability histogram, one can get more accurate answers by finding the area under the normal curve corresponding to half-integers, transformed to standard units. This is clearest if we are seeking the chance of a particular number of successes. For example, suppose we seek to approximate the chance of 10 successes in 25 independent trials, each with probability p = 40% of success. The number of successes in this scenario has a binomial distribution with parameters n = 25 and p = 40%. The expected number of successes is np = 10, and the standard error is (np(1−p))½ = 6½ = 2.45. If we consider the area under the normal curve at the point 10 successes, transformed to standard units, we get zero: the area under a point is always zero. We get a better approximation by considering 10 successes to be the range from 9 1/2 to 10 1/2 successes. The only possible number of successes between 9 1/2 and 10 1/2 is 10, so this is exactly right for the binomial distribution. Because the normal curve is continuous and a binomial random variable is discrete, we need to “smear out” the binomial probability over an appropriate range. The lower endpoint of the range, 9 1/2 successes, is (9.5 − 10)/2.45 = −0.20 standard units. The upper endpoint of the range, 10 1/2 successes, is (10.5 − 10)/2.45 = &#43;0.20 standard units. The area under the normal curve between −0.20 and &#43;0.20 is about 15.8%. The true binomial probability is 25C10×(0.4)10×(0.6)15 = 16%. In a similar way, if we seek the normal approximation to the probability that a binomial random variable is in the range from i successes to k successes, inclusive, we should find the area under the normal curve from i−1/2 to k&#43;1/2 successes, transformed to standard units. If we seek the probability of more than i successes and fewer than k successes, we should find the area under the normal curve corresponding to the range i&#43;1/2 to k−1/2 successes, transformed to standard units. If we seek the probability of more than i but no more than k successes, we should find the area under the normal curve corresponding to the range i&#43;1/2 to k&#43;1/2 successes, transformed to standard units. If we seek the probability of at least i but fewer than k successes, we should find the area under the normal curve corresponding to the range i−1/2 to k−1/2 successes, transformed to standard units. Including or excluding the half-integer ranges at the ends of the interval in this manner is called the continuity correction. 46 Consequent. In a conditional p → q, the consequent is q. 47 Continuous Variable. A quantitative variable is continuous if its set of possible values is uncountable. Examples include temperature, exact height, exact age (including parts of a second). In practice, one can never measure a continuous variable to infinite precision, so continuous variables are sometimes approximated by discrete variables. A random variable X is also called continuous if its set of possible values is uncountable, and the chance that it takes any particular value is zero (in symbols, if P(X = x) = 0 for every real number x). A random variable is continuous if and only if its cumulative probability distribution function is a continuous function (a function with no jumps). 48 Contrapositive. If p and q are two logical propositions, then the contrapositive of the proposition (p → q) is the proposition ((! q) → (!p) ). The contrapositive is logically equivalent to the original proposition. 49 Control. There are at least three senses of “control” in statistics: a member of the control group, to whom no treatment is given; a controlled experiment, and to control for a possible confounding variable. 50 Controlled experiment. An experiment that uses the method of comparison to evaluate the effect of a treatment by comparing treated subjects with a control group, who do not receive the treatment. 51 Controlled, randomized experiment. A controlled experiment in which the assignment of subjects to the treatment group or control group is done at random, for example, by tossing a coin. 52 Control for a variable. To control for a variable is to try to separate its effect from the treatment effect, so it will not confound with the treatment. There are many methods that try to control for variables. Some are based on matching individuals between treatment and control; others use assumptions about the nature of the effects of the variables to try to model the effect mathematically, for example, using regression. 53 Control group. The subjects in a controlled experiment who do not receive the treatment. 54 Convenience Sample. A sample drawn because of its convenience; it is not a probability sample. For example, I might take a sample of opinions in Berkeley (where I live) by just asking my 10 nearest neighbors. That would be a sample of convenience, and would be unlikely to be representative of all of Berkeley. Samples of convenience are not typically representative, and it is not possible to quantify how unrepresentative results based on samples of convenience are likely to be. Convenience samples are to be avoided, and results based on convenience samples are to be viewed with suspicion. See also quota sample. 55 Converge, convergence. A sequence of numbers x1, x2, x3 … converges if there is a number x such that for any number E&gt;0, there is a number k (which can depend on E) such that /xj − x/ &lt; E whenever j &gt; k. If such a number x exists, it is called the limit of the sequence x1, x2, x3 … . 56 Convergence in probability. A sequence of random variables X1, X2, X3 … converges in probability if there is a random variable X such that for any number E&gt;0, the sequence of numbers P(/X1 − X/ &lt; e), P(/X2 − X/ &lt; e), P(/X3 − X/ &lt; e), … converges to 100%. 57 Converse. If p and q are two logical propositions, then the converse of the proposition (p → q) is the proposition (q → p). 58 Correlation. A measure of linear association between two (ordered) lists. Two variables can be strongly correlated without having any causal relationship, and two variables can have a causal relationship and yet be uncorrelated. 59 Correlation coefficient. The correlation coefficient r is a measure of how nearly a scatterplot falls on a straight line. The correlation coefficient is always between −1 and &#43;1. To compute the correlation coefficient of a list of pairs of measurements (X,Y), first transform X and Y individually into standard units. Multiply corresponding elements of the transformed pairs to get a single list of numbers. The correlation coefficient is the mean of that list of products. This page contains a tool that lets you generate bivariate data with any correlation coefficient you want. 60 Counting. To count a set of things is to put it in one to one correspondence with a consecutive subset of the positive integers (counting numbers). 61 Counting numbers, natural numbers. The counting numbers are the strictly positive integers ({1, 2, 3, … }). (Some authorities include (0) among the counting numbers.) 62 Countable Set. A set is countable if its elements can be put in one-to-one correspondence with a subset of the counting numbers. For example, the sets {0, 1, 7, −3}, {red, green, blue}, {…,−2, −1, 0, 1, 2, …}, {straight, curly}, and the set of all fractions, are countable. If a set is not countable, it is uncountable. The set of all real numbers is uncountable. 63 Cover. A confidence interval is said to cover if the interval contains the true value of the parameter. Before the data are collected, the chance that the confidence interval will contain the parameter value is the coverage probability, which equals the confidence level after the data are collected and the confidence interval is actually computed. 64 Coverage probability. The coverage probability of a procedure for making confidence intervals is the chance that the procedure produces an interval that covers the truth. 65 Critical value The critical value in an hypothesis test is the value of the test statistic beyond which we would reject the null hypothesis. The critical value is set so that the probability that the test statistic is beyond the critical value is at most equal to the significance level if the null hypothesis be true. 66 Cross-sectional study. A cross-sectional study compares different individuals to each other at the same time—it looks at a cross-section of a population. The differences between those individuals can confound with the effect being explored. For example, in trying to determine the effect of age on sexual promiscuity, a cross-sectional study would be likely to confound the effect of age with the effect of the mores the subjects were taught as children: the older individuals were probably raised with a very different attitude towards promiscuity than the younger subjects. Thus it would be imprudent to attribute differences in promiscuity to the aging process. C.f. longitudinal study. 67 Cumulative Probability Distribution Function (cdf). The cumulative distribution function of a random variable is the chance that the random variable is less than or equal to x, as a function of x. In symbols, if F is the cdf of the random variable X, then F(x) = P( X ≤ x). The cumulative distribution function must tend to zero as x approaches minus infinity, and must tend to unity as x approaches infinity. It is a positive function, and increases monotonically: if y &gt; x, then F(y) ≥ F(x). The cumulative distribution function completely characterizes the probability distribution of a random variable. 68 de Morgan’s Laws de Morgan’s Laws are identities involving logical operations: the negation of a conjunction is logically equivalent to the disjunction of the negations, and the negation of a disjunction is logically equivalent to the conjunction of the negations. In symbols, !(p &amp; q) = !p / !q and !(p / q) = !p &amp; !q. 69 Deck of Cards. A standard deck of playing cards contains 52 cards, 13 each of four suits: spades, hearts, diamonds, and clubs. The thirteen cards of each suit are {ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, jack, queen, king}. The face cards are {jack, queen, king}. It is typically assumed that if a deck of cards is shuffled well, it is equally likely to be in each possible ordering. There are 52! (52 factorial) possible orderings. 70 Dependent Events, Dependent Random Variables. Two events or random variables are dependent if they are not independent. 71 Dependent Variable. In regression, the variable whose values are supposed to be explained by changes in the other variable (the the independent or explanatory variable). Usually one regresses the dependent variable on the independent variable. 72 Density, Density Scale. The vertical axis of a histogram has units of percent per unit of the horizontal axis. This is called a density scale; it measures how “dense” the observations are in each bin. See also probability density. 73 Denying the antecedent. A logical fallacy that argues from the premise A → B and the premise !A that therefore, !B. The name comes from the fact that the operation denies (i.e., asserts the negation of) the antecedent (A) in the conditional. 74 Denying the consequent. A valid logical argument that concludes from the premise A → B and the premise !B that therefore, !A. The name comes from the fact that the operation denies (i.e., asserts the logical negation) the consequent (B) in the conditional. 75 Deviation. A deviation is the difference between a datum and some reference value, typically the mean of the data. In computing the SD, one finds the rms of the deviations from the mean, the differences between the individual data and the mean of the data. 76 Discrete Variable. A quantitative variable whose set of possible values is countable. Typical examples of discrete variables are variables whose possible values are a subset of the integers, such as Social Security numbers, the number of people in a family, ages rounded to the nearest year, etc. Discrete variables are “chunky.” C.f. continuous variable. A discrete random variable is one whose set of possible values is countable. A random variable is discrete if and only if its cumulative probability distribution function is a stair-step function; i.e., if it is piecewise constant and only increases by jumps. 77 Disjoint or Mutually Exclusive Events. Two events are disjoint or mutually exclusive if the occurrence of one is incompatible with the occurrence of the other; that is, if they can’t both happen at once (if they have no outcome in common). Equivalently, two events are disjoint if their intersection is the empty set. 78 Disjoint or Mutually Exclusive Sets. Two sets are disjoint or mutually exclusive if they have no element in common. Equivalently, two sets are disjoint if their intersection is the empty set. 79 Distribution. The distribution of a set of numerical data is how their values are distributed over the real numbers. It is completely characterized by the empirical distribution function. Similarly, the probability distribution of a random variable is completely characterized by its probability distribution function. Sometimes the word “distribution” is used as a synonym for the empirical distribution function or the probability distribution function. If two or more random variables are defined for the same experiment, they have a joint probability distribution. 80 Distribution Function, Empirical. The empirical (cumulative) distribution function of a set of numerical data is, for each real value of x, the fraction of observations that are less than or equal to x. A plot of the empirical distribution function is an uneven set of stairs. The width of the stairs is the spacing between adjacent data; the height of the stairs depends on how many data have exactly the same value. The distribution function is zero for small enough (negative) values of x, and is unity for large enough values of x. It increases monotonically: if y &gt; x, the empirical distribution function evaluated at y is at least as large as the empirical distribution function evaluated at x. 81 Double-Blind, Double-Blind Experiment. In a double-blind experiment, neither the subjects nor the people evaluating the subjects knows who is in the treatment group and who is in the control group. This mitigates the placebo effect and guards against conscious and unconscious prejudice for or against the treatment on the part of the evaluators. 82 Ecological Correlation. The correlation between averages of groups of individuals, instead of individuals. Ecological correlation can be misleading about the association of individuals. 83 Element of a Set. See member. 84 Empirical Law of Averages. The Empirical Law of Averages lies at the base of the frequency theory of probability. This law, which is, in fact, an assumption about how the world works, rather than a mathematical or physical law, states that if one repeats a random experiment over and over, independently and under “identical” conditions, the fraction of trials that result in a given outcome converges to a limit as the number of trials grows without bound. 85 Empty Set. The empty set, denoted {} or ∅, is the set that has no members. 86 Endpoint Convention. In plotting a histogram, one must decide whether to include a datum that lies at a class boundary with the class interval to the left or the right of the boundary. The rule for making this assignment is called an endpoint convention. The two standard endpoint conventions are (1) to include the left endpoint of all class intervals and exclude the right, except for the rightmost class interval, which includes both of its endpoints, and (2) to include the right endpoint of all class intervals and exclude the left, except for the leftmost interval, which includes both of its endpoints. 87 Equally Likely Outcomes. According to the equally likely outcome Theory of Probability, if an experiment has a finite number possible outcomes and there is no reason Nature should prefer any of those outcomes over any other (e.g., because the outcome is the result of rolling a symmetric die or tossing a perfectly balanced coin or thoroughly shuffling a deck of cards), then each of those possible outcomes has the same probability. See also Laplace’s Principle of Insufficient Reason. 88 Estimator. An estimator is a rule for “guessing” the value of a population parameter based on a random sample from the population. An estimator is a random variable, because its value depends on which particular sample is obtained, which is random. A canonical example of an estimator is the sample mean, which is an estimator of the population mean. 89 Event. An event is a subset of outcome space. An event determined by a random variable is an event of the form A=(X is in A). When the random variable X is observed, that determines whether or not A occurs: if the value of X happens to be in A, A occurs; if not, A does not occur. 90 Exhaustive. A collection of events {A1, A2, A3, … } exhausts the set A if, for the event A to occur, at least one of those sets must also occur; that is, if S ⊂ A1 ∪ A2 ∪ A3 ∪ … If the event A is not specified, it is assumed to be the entire outcome space S. 91 Expectation, Expected Value. The expected value of a random variable is the long-term limiting average of its values in independent repeated experiments. The expected value of the random variable X is denoted EX or E(X). For a discrete random variable (one that has a countable number of possible values) the expected value is the weighted average of its possible values, where the weight assigned to each possible value is the chance that the random variable takes that value. One can think of the expected value of a random variable as the point at which its probability histogram would balance, if it were cut out of a uniform material. Taking the expected value is a linear operation: if X and Y are two random variables, the expected value of their sum is the sum of their expected values (E(X&#43;Y) = E(X) &#43; E(Y)), and the expected value of a constant a times a random variable X is the constant times the expected value of X (E(a×X ) = a× E(X)). 92 Experiment. What distinguishes an experiment from an observational study is that in an experiment, the experimenter chooses who receives the treatment. 93 Explanatory Variable. In regression, the explanatory or independent variable is the one that is supposed to “explain” the other. For example, in examining crop yield versus quantity of fertilizer applied, the quantity of fertilizer would be the explanatory or independent variable, and the crop yield would be the dependent variable. In experiments, the explanatory variable is the one that is manipulated; the one that is observed is the dependent variable. 94 Extrapolation. See interpolation. 95 Factorial. For an integer k that is greater than or equal to 1, k! (pronounced “k factorial”) is k×(k−1)×(k−2)× …×1. By convention, 0! = 1. There are k! ways of ordering k distinct objects. For example, 9! is the number of batting orders of 9 baseball players, and 52! is the number of different ways a standard deck of playing cards can be ordered. The calculator above has a button to compute the factorial of a number. To compute k!, first type the value of k, then press the button labeled “!”. 96 Fair Bet. A fair bet is one for which the expected value of the payoff is zero, after accounting for the cost of the bet. For example, suppose I offer to pay you $2 if a fair coin lands heads, but you must ante up $1 to play. Your expected payoff is −$1&#43; $0×P(tails) &#43; $2×P(heads) = −$1 &#43; $2×50% = $0. This is a fair bet—in the long run, if you made this bet over and over again, you would expect to break even. 97 False Discovery Rate. In testing a collection of hypotheses, the false discovery rate is the fraction of rejected null hypotheses that are rejected erroneously (the number of Type I errors divided by the number of rejected null hypotheses), with the convention that if no hypothesis is rejected, the false discovery rate is zero. 98 Finite, finite set. A set is finite if it has a finite number of elements, that is, if for some natural number n, the elements can be put in one-to-one correspondence with the set {1, 2, … n}. 99 Finite Population Correction. When sampling without replacement, as in a simple random sample, the SE of sample sums and sample means depends on the fraction of the population that is in the sample: the greater the fraction, the smaller the SE. Sampling with replacement is like sampling from an infinitely large population. The adjustment to the SE for sampling without replacement is called the finite population correction. The SE for sampling without replacement is smaller than the SE for sampling with replacement by the finite population correction factor ((N −n)/(N − 1))½. Note that for sample size n=1, there is no difference between sampling with and without replacement; the finite population correction is then unity. If the sample size is the entire population of N units, there is no variability in the result of sampling without replacement (every member of the population is in the sample exactly once), and the SE should be zero. This is indeed what the finite population correction gives (the numerator vanishes). 100 Fisher’s exact test (for the equality of two percentages) Consider two populations of zeros and ones. Let p1 be the proportion of ones in the first population, and let p2 be the proportion of ones in the second population. We would like to test the null hypothesis that p1 = p2 on the basis of a simple random sample from each population. Let n1 be the size of the sample from population 1, and let n2 be the size of the sample from population 2. Let G be the total number of ones in both samples. If the null hypothesis be true, the two samples are like one larger sample from a single population of zeros and ones. The allocation of ones between the two samples would be expected to be proportional to the relative sizes of the samples, but would have some chance variability. Conditional on G and the two sample sizes, under the null hypothesis, the tickets in the first sample are like a random sample of size n1 without replacement from a collection of N = n1 &#43; n2 units of which G are labeled with ones. Thus, under the null hypothesis, the number of tickets labeled with ones in the first sample has (conditional on G) an hypergeometric distribution with parameters N, G, and n1. Fisher’s exact test uses this distribution to set the ranges of observed values of the number of ones in the first sample for which we would reject the null hypothesis. 101 Football-Shaped Scatterplot. In a football-shaped scatterplot, most of the points lie within a tilted oval, shaped more-or-less like a football. A football-shaped scatterplot is one in which the data are homoscedastically scattered about a straight line. 102 Frame, sampling frame. A sampling frame is a collection of units from which a sample will be drawn. Ideally, the frame is identical to the population we want to learn about; more typically, the frame is only a subset of the population of interest. The difference between the frame and the population can be a source of bias in sampling design, if the parameter of interest has a different value for the frame than it does for the population. For example, one might desire to estimate the current annual average income of 1998 graduates of the University of California at Berkeley. I propose to use the sample mean income of a sample of graduates drawn at random. To facilitate taking the sample and contacting the graduates to obtain income information from them, I might draw names at random from the list of 1998 graduates for whom the alumni association has an accurate current address. The population is the collection of 1998 graduates; the frame is those graduates who have current addresses on file with the alumni association. If there is a tendency for graduates with higher incomes to have up-to-date addresses on file with the alumni association, that would introduce a positive bias into the annual average income estimated from the sample by the sample mean. 103 FPP. Statistics, third edition, by Freedman, Pisani, and Purves, published by W.W. Norton, 1997. 104 Frequency theory of probability. See Probability, Theories of. 105 Frequency table. A table listing the frequency (number) or relative frequency (fraction or percentage) of observations in different ranges, called class intervals. 106 Fundamental Rule of Counting. If a sequence of experiments or trials T1, T2, T3, …, Tk could result, respectively, in n1, n2 n3, …, nk possible outcomes, and the numbers n1, n2 n3, …, nk do not depend on which outcomes actually occurred, the entire sequence of k experiments has n1× n2 × n3× …× nk possible outcomes. 107 Game Theory. A field of study that bridges mathematics, statistics, economics, and psychology. It is used to study economic behavior, and to model conflict between nations, for example, “nuclear stalemate” during the Cold War. 108 Geometric Distribution. The geometric distribution describes the number of trials up to and including the first success, in independent trials with the same probability of success. The geometric distribution depends only on the single parameter p, the probability of success in each trial. For example, the number of times one must toss a fair coin until the first time the coin lands heads has a geometric distribution with parameter p = 50%. The geometric distribution assigns probability p×(1 − p)k−1to the event that it takes k trials to the first success. The expected value of the geometric distribution is 1/p, and its SE is (1−p)½/p. 109 Geometric Mean. The geometric mean of n numbers {x1, x2, x3, …, xn} is the nth root of their product: (x1×x2×x3× … ×xn)1/n. 110 Graph of Averages. For bivariate data, a graph of averages is a plot of the average values of one variable (say y) for small ranges of values of the other variable (say x), against the value of the second variable (x) at the midpoints of the ranges. 111 Heteroscedasticity. “Mixed scatter.” A scatterplot or residual plot shows heteroscedasticity if the scatter in vertical slices through the plot depends on where you take the slice. Linear regression is not usually a good idea if the data are heteroscedastic. 112 Histogram. A histogram is a kind of plot that summarizes how data are distributed. Starting with a set of class intervals, the histogram is a set of rectangles (“bins”) sitting on the horizontal axis. The bases of the rectangles are the class intervals, and their heights are such that their areas are proportional to the fraction of observations in the corresponding class intervals. That is, the height of a given rectangle is the fraction of observations in the corresponding class interval, divided by the length of the corresponding class interval. A histogram does not need a vertical scale, because the total area of the histogram must equal 100%. The units of the vertical axis are percent per unit of the horizontal axis. This is called the density scale. The horizontal axis of a histogram needs a scale. If any observations coincide with the endpoints of class intervals, the endpoint convention is important. This page contains a histogram tool, with controls to highlight ranges of values and read their areas. 113 Historical Controls. Sometimes, the a treatment group is compared with individuals from another epoch who did not receive the treatment; for example, in studying the possible effect of fluoridated water on childhood cancer, we might compare cancer rates in a community before and after fluorine was added to the water supply. Those individuals who were children before fluoridation started would comprise an historical control group. Experiments and studies with historical controls tend to be more susceptible to confounding than those with contemporary controls, because many factors that might affect the outcome other than the treatment tend to change over time as well. (In this example, the level of other potential carcinogens in the environment also could have changed.) 114 Homoscedasticity. “Same scatter.” A scatterplot or residual plot shows homoscedasticity if the scatter in vertical slices through the plot does not depend much on where you take the slice. C.f. heteroscedasticity. 115 House Edge. In casino games, the expected payoff to the bettor is negative: the house (casino) tends to win money in the long run. The amount of money the house would expect to win for each $1 wagered on a particular bet (such as a bet on “red” in roulette) is called the house edge for that bet. 116 HTLWS. The book How to lie with Statistics by D. Huff. 117 Hypergeometric Distribution. The hypergeometric distribution with parameters N, G and n is the distribution of the number of “good” objects in a simple random sample of size n (i.e., a random sample without replacement in which every subset of size n has the same chance of occurring) from a population of N objects of which G are “good.” The chance of getting exactly g good objects in such a sample is GCg × N−GCn−g/NCn, provided g ≤ n, g ≤ G, and n − g ≤ N − G. (The probability is zero otherwise.) The expected value of the hypergeometric distribution is n×G/N, and its standard error is ((N−n)/(N−1))½ × (n × G/N × (1−G/N) )½. 118 Hypothesis testing. Statistical hypothesis testing is formalized as making a decision between rejecting or not rejecting a null hypothesis, on the basis of a set of observations. Two types of errors can result from any decision rule (test): rejecting the null hypothesis when it is true (a Type I error), and failing to reject the null hypothesis when it is false (a Type II error). For any hypothesis, it is possible to develop many different decision rules (tests). Typically, one specifies ahead of time the chance of a Type I error one is willing to allow. That chance is called the significance level of the test or decision rule. For a given significance level, one way of deciding which decision rule is best is to pick the one that has the smallest chance of a Type II error when a given alternative hypothesis is true. The chance of correctly rejecting the null hypothesis when a given alternative hypothesis is true is called the power of the test against that alternative. 119 iff, if and only if, ↔ If p and q are two logical propositions, then(p ↔ q) is a proposition that is true when both p and q are true, and when both p and q are false. It is logically equivalent to the proposition ( (p → q) &amp; (q → p) ) and to the proposition ( (p &amp; q) 120 Implies, logical implication, → , conditional, if-then Logical implication is an operation on two logical propositions. If p and q are two logical propositions, (p → q), pronounced “p implies q” or “if p then q” is a logical proposition that is true if p is false, or if both p and q are true. The proposition (p → q) is logically equivalent to the proposition ((!p) / q). In the conditional p → q, the antecedent is p and the consequent is q. 121 Independent, independence. Two events A and B are (statistically) independent if the chance that they both happen simultaneously is the product of the chances that each occurs individually; i.e., if P(AB) = P(A)P(B). This is essentially equivalent to saying that learning that one event occurs does not give any information about whether the other event occurred too: the conditional probability of A given B is the same as the unconditional probability of A, i.e., P(A/B) = P(A). Two random variables X and Y are independent if all events they determine are independent, for example, if the event {a &lt; X ≤ b} is independent of the event {c &lt; Y ≤ d} for all choices of a, b, c, and d. A collection of more than two random variables is independent if for every proper subset of the variables, every event determined by that subset of the variables is independent of every event determined by the variables in the complement of the subset. For example, the three random variables X, Y, and Z are independent if every event determined by X is independent of every event determined by Y and every event determined by X is independent of every event determined by Y and Z and every event determined by Y is independent of every event determined by X and Z and every event determined by Z is independent of every event determined by X and Y. 122 Independent and identically distributed (iid). A collection of two or more random variables {X1, X2, … , } is independent and identically distributed if the variables have the same probability distribution, and are independent. 123 Independent Variable. In regression, the independent variable is the one that is supposed to explain the other; the term is a synonym for “explanatory variable.” Usually, one regresses the “dependent variable” on the “independent variable.” There is not always a clear choice of the independent variable. The independent variable is usually plotted on the horizontal axis. Independent in this context does not mean the same thing as statistically independent. 124 Indicator Random Variable. The indicator [random variable] of the event A, often written 1A, is the random variable that equals unity if A occurs, and zero if A does not occur. The expected value of the indicator of A is the probability of A, P(A), and the standard error of the indicator of A is (P(A)×(1−P(A))½. The sum 1A &#43; 1B &#43; 1C &#43; … of the indicators of a collection of events {A, B, C, …} counts how many of the events {A, B, C, …} occur in a given trial. The product of the indicators of a collection of events is the indicator of the intersection of the events (the product equals one if and only if all of indicators equal one). The maximum of the indicators of a collection of events is the indicator of the union of the events (the maximum equals one if any of the indicators equals one). 125 Inter-quartile Range (IQR). The inter-quartile range of a list of numbers is the upper quartile minus the lower quartile. 126 Interpolation. Given a set of bivariate data (x, y), to impute a value of y corresponding to some value of x at which there is no measurement of y is called interpolation, if the value of x is within the range of the measured values of x. If the value of x is outside the range of measured values, imputing a corresponding value of y is called extrapolation. 127 Intersection. The intersection of two or more sets is the set of elements that all the sets have in common; the elements contained in every one of the sets. The intersection of the events A and B is written “A∩B,” “A and B,” and “AB.” C.f. union. See also Venn diagrams. 128 Invalid (logical) argument. An invalid logical argument is one in which the truth of the premises does not guarantee the truth of the conclusion. For example, the following logical argument is invaldraft: false id: If the forecast calls for rain, I will not wear sandals. The forecast does not call for rain. Therefore, I will wear sandals. See also valid argument. 129 Joint Probability Distribution. If X1, X2, … , Xk are random variables defined for the same experiment, their joint probability distribution gives the probability of events determined by the collection of random variables: for any collection of sets of numbers {A1, … , Ak}, the joint probability distribution determines P( (X1 is in A1) and (X2 is in A2) and … and (Xk is in Ak) ). For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. If a collection of random variables is independent, their joint probability distribution is the product of their marginal probability distributions, their individual probability distributions without regard for the value of the other variables. In this example, the marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6, and the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Note that P(X1 = 1, X2 = 10) = 0, while P(X1 = 1)×P(X2 = 10) = (1/6)(3/36) = 1/72. The joint probability is not equal to the product of the marginal probabilities: X1 and X2 are dependent random variables. 130 Law of Averages. The Law of Averages says that the average of independent observations of random variables that have the same probability distribution is increasingly likely to be close to the expected value of the random variables as the number of observations grows. More precisely, if X1, X2, X3, …, are independent random variables with the same probability distribution, and E(X) is their common expected value, then for every number ε &gt; 0, P{/(X1 &#43; X2 &#43; … &#43; Xn)/n − E(X) / &lt; ε} converges to 100% as n grows. This is equivalent to saying that the sequence of sample means X1, (X1&#43;X2)/2, (X1&#43;X2&#43;X3)/3, … converges in probability to E(X). 131 Law of Large Numbers. The Law of Large Numbers says that in repeated, independent trials with the same probability p of success in each trial, the percentage of successes is increasingly likely to be close to the chance of success as the number of trials increases. More precisely, the chance that the percentage of successes differs from the probability p by more than a fixed positive amount, e &gt; 0, converges to zero as the number of trials n goes to infinity, for every number e &gt; 0. Note that in contrast to the difference between the percentage of successes and the probability of success, the difference between the number of successes and the expected number of successes, n×p, tends to grow as n grows. The following tool illustrates the law of large numbers; the button toggles between displaying the difference between the number of successes and the expected number of successes, and the difference between the percentage of successes and the expected percentage of successes. The tool on this page illustrates the law of large numbers. 132 Limit. See converge. 133 Linear Operation. Suppose f is a function or operation that acts on things we shall denote generically by the lower-case Roman letters x and y. Suppose it makes sense to multiply x and y by numbers (which we denote by a), and that it makes sense to add things like x and y together. We say that f is linear if for every number a and every value of x and y for which f(x) and f(y) are defined, (i) f( a×x ) is defined and equals a×f(x), and (ii) f( x &#43; y ) is defined and equals f(x) &#43; f(y). C.f. affine. 134 Linear association. Two variables are linearly associated if a change in one is associated with a proportional change in the other, with the same constant of proportionality throughout the range of measurement. The correlation coefficient measures the degree of linear association on a scale of −1 to 1. 135 List. I use the term list to mean two things: either a multiset or (more often) an tuple. Lists are countable collections (multisets) in some order (like a tuple). That is, it makes sense to talk about the 1st (or 7th, or nth) element of a list, and the nth and mth elements of a list can be equal, even if n ≠ m (the elements of a list need not be distinct). 136 Location, Measure of. A measure of location is a way of summarizing what a “typical” element of a list is—it is a one-number summary of a distribution. See also arithmetic mean, median, and mode. 137 Logical argument. A logical argument consists of one or more premises, propositions that are assumed to be true, and a conclusion, a proposition that is supposed to be guaranteed to be true (as a matter of pure logic) if the premises are true. For example, the following is a logical argument: p → q Therefore, q. This argument has two premises: p → q, and p. The conclusion of the argument is q. If a logical argument is valid if the truth of the premises guarantees the truth of the conclusion; otherwise, the argument is invalid. That is, an argument with premises p1, p1, … pn and conclusion q is valid if the compound proposition (p1 &amp; p2 &amp; … &amp; pn) → q is logically equivalent to TRUE. The argument given above is valid because if it is true that p → q and that p is true (the two premises), then q (the conclusion of the argument) must also be true. 138 Logically equivalent, logical equivalence. Two propositions are logically equivalent if they always have the same truth value. That is, the propositions p and q are logically equivalent if p is true whenever q is true and p is false whenever q is false. The proposition (p ↔ q) is always true if and only if p and q are logically equivalent. For example, p is logically equivalent to p, to (p &amp; p), and to (p / p); (p / (!p)) is logically equivalent to TRUE; (p &amp; !p) is logically equivalent to FALSE; (p ↔ p) is logically equivalent to TRUE; and (p → q) is logically equivalent to (!p / q). 139 Longitudinal study. A study in which individuals are followed over time, and compared with themselves at different times, to determine, for example, the effect of aging on some measured variable. Longitudinal studies provide much more persuasive evidence about the effect of aging than do cross-sectional studies. 140 Lower Quartile (LQ). See quartiles. 141 Margin of error. A measure of the uncertainty in an estimate of a parameter; unfortunately, not everyone agrees what it should mean. The margin of error of an estimate is typically one or two times the estimated standard error of the estimate. 142 Marginal probability distribution. The marginal probability distribution of a random variable that has a joint probability distribution with some other random variables is the probability distribution of that random variable without regard for the values that the other random variables take. The marginal distribution of a discrete random variable X1 that has a joint distribution with other discrete random variables can be found from the joint distribution by summing over all possible values of the other variables. For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. The marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6. We can verify that the marginal probability that X1 = 1 is indeed the sum of the joint probability distribution over all possible values of X2 for which X1 = 1: P(X1 = 1) = P(X1 = 1, X2 = 2) &#43; P(X1 = 1, X2 = 3) &#43; P(X1 = 1, X2 = 4) &#43; P(X1 = 1, X2 = 5) &#43; P(X1 = 1, X2 = 6) &#43; P(X1 = 1, X2 = 7) = 6/36 = 1/6. Similarly, the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Again, we can verify that the marginal probability that X2 = 4 is 3/36 by adding the joint probabilities for all possible values of X1 for which X2 = 4: P(X2 = 4) = P(X1 = 1, X2 = 4) &#43; P(X1 = 2, X2 = 4) &#43; P(X1 = 3, X2 = 4) = 3/36. 143 Markov’s Inequality. For lists: If a list contains no negative numbers, the fraction of numbers in the list at least as large as any given constant a&gt;0 is no larger than the arithmetic mean of the list, divided by a. For random variables: if a random variable X must be nonnegative, the chance that X exceeds any given constant a&gt;0 is no larger than the expected value of X, divided by a. 144 Maximum Likelihood Estimate (MLE). The maximum likelihood estimate of a parameter from data is the possible value of the parameter for which the chance of observing the data largest. That is, suppose that the parameter is p, and that we observe data x. Then the maximum likelihood estimate of p is estimate p by the value q that makes P(observing x when the value of p is q) as large as possible. For example, suppose we are trying to estimate the chance that a (possibly biased) coin lands heads when it is tossed. Our data will be the number of times x the coin lands heads in n independent tosses of the coin. The distribution of the number of times the coin lands heads is binomial with parameters n (known) and p (unknown). The chance of observing x heads in n trials if the chance of heads in a given trial is q is nCx qx(1−q)n−x. The maximum likelihood estimate of p would be the value of q that makes that chance largest. We can find that value of q explicitly using calculus; it turns out to be q = x/n, the fraction of times the coin is observed to land heads in the n tosses. Thus the maximum likelihood estimate of the chance of heads from the number of heads in n independent tosses of the coin is the observed fraction of tosses in which the coin lands heads. 145 Mean, Arithmetic mean. The sum of a list of numbers, divided by the number of elements in the list. See also average. 146 Mean Squared Error (MSE). The mean squared error of an estimator of a parameter is the expected value of the square of the difference between the estimator and the parameter. In symbols, if X is an estimator of the parameter t, then MSE(X) = E( (X−t)2 ). The MSE measures how far the estimator is off from what it is trying to estimate, on the average in repeated experiments. It is a summary measure of the accuracy of the estimator. It combines any tendency of the estimator to overshoot or undershoot the truth (bias), and the variability of the estimator (SE). The MSE can be written in terms of the bias and SE of the estimator: MSE(X) = (bias(X))2 &#43; (SE(X))2. 147 Median. “Middle value” of a list. The smallest number such that at least half the numbers in the list are no greater than it. If the list has an odd number of entries, the median is the middle entry in the list after sorting the list into increasing order. If the list has an even number of entries, the median is the smaller of the two middle numbers after sorting. The median can be estimated from a histogram by finding the smallest number such that the area under the histogram to the left of that number is 50%. 148 Member of a set. Something is a member (or element) of a set if it is one of the things in the set. 149 Method of Comparison. The most basic and important method of determining whether a treatment has an effect: compare what happens to individuals who are treated (the treatment group) with what happens to individuals who are not treated (the control group). 150 Minimax Strategy. In game theory, a minimax strategy is one that minimizes one’s maximum loss, whatever the opponent might do (whatever strategy the opponent might choose). 151 Mode. For lists, the mode is a most common (frequent) value. A list can have more than one mode. For histograms, a mode is a relative maximum (“bump”). 152 Moment. The kth moment of a list is the average value of the elements raised to the kth power; that is, if the list consists of the N elements x1, x2, … , xN, the kth moment of the list is ( x1k &#43; x2k &#43; xNk )/N. The kth moment of a random variable X is the expected value of Xk, E(Xk). 153 Monotone, monotonic function. A function is monotone if it only increases or only decreases: f increases monotonically (is monotonic increasing) if x &gt; y, implies thatf(x) ≥ f(y). A function f decreases monotonically (is monotonic decreasing) if x &gt; y, implies thatf(x) ≤ f(y). A function f is strictly monotonically increasing if x &gt; y, implies thatf(x) &gt; f(y), and strictly monotonically decreasing if if x &gt; y, implies thatf(x) &lt; f(y). 154 Multimodal Distribution. A distribution with more than one mode. The histogram of a multimodal distribution has more than one “bump.” 155 Multinomial Distribution Consider a sequence of n independent trials, each of which can result in an outcome in any of k categories. Let pj be the probability that each trial results in an outcome in category j, j = 1, 2, … , k, so p1 &#43; p2 &#43; … &#43; pk = 100%. The number of outcomes of each type has a multinomial distribution. In particular, the probability that the n trials result in n1 outcomes of type 1, n2 outcomes of type 2, … , and nk outcomes of type k is n!/(n1! × n2! × … × nk!) × p1n1 × p2n2 × … × pknk, if n1, … , nk are nonnegative integers that sum to n; the chance is zero otherwise. 156 Multiplication rule. The chance that events A and B both occur (i.e., that event AB occurs), is the conditional probability that A occurs given that B occurs, times the unconditional probability that B occurs. 157 Multiplicity in hypothesis tests. In hypothesis testing, if more than one hypothesis is tested, the actual significance level of the combined tests is not equal to the nominal significance level of the individual tests. See also false discovery rate. 158 Multivariate Data. A set of measurements of two or more variables per individual. See bivariate. 159 Multiset. A multiset, also known as a bag is a collection of things, but—unlike a set, which is also a collection of things—the same object can occur in a multiset more than once. For instance, the sets {1, 2}, {1, 2, 2}, and {1, 1, 1, 1, 1, 2, 2} are all equal, while the multisets [1, 2], [1, 2, 2], and [1, 1, 1, 1, 1, 2, 2] are all different. However, order does not matter for sets or for multisets, so, for instance {1, 2} = {2, 1} and [1, 1, 1, 1, 1, 2, 2] = [2, 1, 1, 2, 1, 1, 1]. 160 Mutually Exclusive. See disjoint events or disjoint sets. 161 Nearly normal distribution. A population of numbers (a list of numbers) is said to have a nearly normal distribution if the histogram of its values in standard units nearly follows a normal curve. More precisely, suppose that the mean of the list is μ and the standard deviation of the list is SD. Then the list is nearly normally distributed if, for every two numbers a &lt; b, the fraction of numbers in the list that are between a and b is approximately equal to the area under the normal curve between (a − μ)/SD and (a − μ)/SD. 162 Negative Binomial Distribution. Consider a sequence of independent trials with the same probability p of success in each trial. The number of trials up to and including the rth success has the negative Binomial distribution with parameters n and r. If the random variable N has the negative binomial distribution with parameters n and r, then P(N=k) = k−1Cr−1 × pr × (1−p)k−r, for k = r, r&#43;1, r&#43;2, …, and zero for k &lt; r, because there must be at least r trials to have r successes. The negative binomial distribution is derived as follows: for the rth success to occur on the kth trial, there must have been r−1 successes in the first k−1 trials, and the kth trial must result in success. The chance of the former is the chance of r−1 successes in k−1 independent trials with the same probability of success in each trial, which, according to the Binomial distribution with parameters n=k−1 and p, has probability k−1Cr−1 × pr−1 × (1−p)k−r. The chance of the latter event is p, by assumption. Because the trials are independent, we can find the chance that both events occur by multiplying their chances together, which gives the expression for P(N=k) above. 163 No causation without manipulation. A slogan attributed to Paul Holland. If the conditions were not deliberately manipulated (for example, if the situation is an observational study rather than an experiment), it is unwise to conclude that there is any causal relationship between the outcome and the conditions. See post hoc ergo propter hoc and cum hoc ergo propter hoc. 164 Nonlinear Association. The relationship between two variables is nonlinear if a change in one is associated with a change in the other that is depends on the value of the first; that is, if the change in the second is not simply proportional to the change in the first, independent of the value of the first variable. 165 Nonresponse. In surveys, it is rare that everyone who is “invited” to participate (everyone whose phone number is called, everyone who is mailed a questionnaire, everyone an interviewer tries to stop on the street…) in fact responds. The difference between the “invited” sample sought, and that obtained, is the nonresponse. 166 Nonresponse bias. In a survey, those who respond may differ from those who do not, in ways that are related to the effect one is trying to measure. For example, a telephone survey of how many hours people work is likely to miss people who are working late, and are therefore not at home to answer the phone. When that happens, the survey may suffer from nonresponse bias. Nonresponse bias makes the result of a survey differ systematically from the truth. 167 Nonresponse rate. The fraction of nonresponders in a survey: the number of nonresponders divided by the number of people invited to participate (the number sent questionnaires, the number of interview attempts, etc.) If the nonresponse rate is appreciable, the survey suffer from large nonresponse bias. 168 Normal approximation. The normal approximation to data is to approximate areas under the histogram of data, transformed into standard units, by the corresponding areas under the normal curve. Many probability distributions can be approximated by a normal distribution, in the sense that the area under the probability histogram is close to the area under a corresponding part of the normal curve. To find the corresponding part of the normal curve, the range must be converted to standard units, by subtracting the expected value and dividing by the standard error. For example, the area under the binomial probability histogram for n = 50 and p = 30% between 9.5 and 17.5 is 74.2%. To use the normal approximation, we transform the endpoints to standard units, by subtracting the expected value (for the Binomial random variable, n×p = 15 for these values of n and p) and dividing the result by the standard error (for a Binomial, (n × p × (1−p))1/2 = 3.24 for these values of n and p). The area normal approximation is the area under the normal curve between (9.5 − 15)/3.24 = −1.697 and (17.5 − 15)/3.24 = 0.772; that area is 73.5%, slightly smaller than the corresponding area under the binomial histogram. See also the continuity correction. The tool on this page illustrates the normal approximation to the binomial probability histogram. Note that the approximation gets worse when p gets close to 0 or 1, and that the approximation improves as n increases. 169 Normal curve. The normal curve is the familiar “bell curve:,” illustrated on this page. The mathematical expression for the normal curve is y = (2×pi)−½e−x2/2, where pi is the ratio of the circumference of a circle to its diameter (3.14159265…), and e is the base of the natural logarithm (2.71828…). The normal curve is symmetric around the point x=0, and positive for every value of x. The area under the normal curve is unity, and the SD of the normal curve, suitably defined, is also unity. Many (but not most) histograms, converted into standard units, approximately follow the normal curve. 170 Normal distribution. A random variable X has a normal distribution with mean m and standard error s if for every pair of numbers a ≤ b, the chance that a &lt; (X−m)/s &lt; b is P(a &lt; (X−m)/s &lt; b) = area under the normal curve between a and b. If there are numbers m and s such that X has a normal distribution with mean m and standard error s, then X is said to have a normal distribution or to be normally distributed. If X has a normal distribution with mean m=0 and standard error s=1, then X is said to have a standard normal distribution. The notation XN(m,s2) means that X has a normal distribution with mean m and standard error s; for example, XN(0,1), means X has a standard normal distribution. 171 NOT, !, Negation, Logical Negation. The negation of a logical proposition p, !p, is a proposition that is the logical opposite of p. That is, if p is true, !p is false, and if p is false, !p is true. Negation takes precedence over other logical operations. Other common symbols for the negation operator include ¬, − and ˜. 172 Null hypothesis. In hypothesis testing, the hypothesis we wish to falsify on the basis of the data. The null hypothesis is typically that something is not present, that there is no effect, or that there is no difference between treatment and control. 173 Observational Study. Controlled experiment. 174 Odds. The odds in favor of an event is the ratio of the probability that the event occurs to the probability that the event does not occur. For example, suppose an experiment can result in any of n possible outcomes, all equally likely, and that k of the outcomes result in a “win” and n−k result in a “loss.” Then the chance of winning is k/n; the chance of not winning is (n−k)/n; and the odds in favor of winning are (k/n)/((n−k)/n) = k/(n−k), which is the number of favorable outcomes divided by the number of unfavorable outcomes. Note that odds are not synonymous with probability, but the two can be converted back and forth. If the odds in favor of an event are q, then the probability of the event is q/(1&#43;q). If the probability of an event is p, the odds in favor of the event are p/(1−p) and the odds against the event are (1−p)/p. 175 One-sided Test. C.f. two-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against either the alternative hypothesis that μ &lt; μ0 or the alternative μ &gt; μ0 (but not both). For example, a significance level 5%, one-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero, would reject the null hypothesis for values of 176 or, /, Disjunction, Logical Disjunction, ∨ An operation on two logical propositions. If p and q are two propositions, (p / q) is a proposition that is true if p is true or if q is true (or both); otherwise, it is false. That is, (p / q) is true unless both p and q are false. The operation / is sometimes represented by the symbol ∨ and sometimes by the word or. C.f. exclusive disjunction, XOR. 177 Ordinal Variable. A variable whose possible values have a natural order, such as {short, medium, long}, {cold, warm, hot}, or {0, 1, 2, 3, …}. In contrast, a variable whose possible values are {straight, curly} or {Arizona, California, Montana, New York} would not naturally be ordinal. Arithmetic with the possible values of an ordinal variable does not necessarily make sense, but it does make sense to say that one possible value is larger than another. 178 Outcome Space. The outcome space is the set of all possible outcomes of a given random experiment. The outcome space is often denoted by the capital letter S. 179 Outlier. An outlier is an observation that is many SD’s from the mean. It is sometimes tempting to discard outliers, but this is imprudent unless the cause of the outlier can be identified, and the outlier is determined to be spurious. Otherwise, discarding outliers can cause one to underestimate the true variability of the measurement process. 180 P-value. Suppose we have a family of hypothesis tests of a null hypothesis that let us test the hypothesis at any significance level p between 0 and 100% we choose. The P value of the null hypothesis given the data is the smallest significance level p for which any of the tests would have rejected the null hypothesis. For example, let X be a test statistic, and for p between 0 and 100%, let xp be the smallest number such that, under the null hypothesis, P( X ≤ x ) ≥ p. Then for any p between 0 and 100%, the rule reject the null hypothesis if X &lt; xp tests the null hypothesis at significance level p. If we observed X = x, the P-value of the null hypothesis given the data would be the smallest p such that x &lt; xp. 181 Parameter. A numerical property of a population, such as its mean. 182 Partition. A partition of an event A is a collection of events {A1, A2, A3, … } such that the events in the collection are disjoint, and their union is A. That is, AjAk = {} unless j = k, and A = A1 ∪ A2 ∪ A3 ∪ … . If the event A is not specified, it is assumed to be the entire outcome space S. 183 Payoff Matrix. A way of representing what each player in a game wins or loses, as a function of his and his opponent’s strategies. 184 Percentile. The pth percentile of a list is the smallest number such that at least p% of the numbers in the list are no larger than it. The pth percentile of a random variable is the smallest number such that the chance that the random variable is no larger than it is at least p%. C.f. quantile. 185 Permutation. A permutation of a set is an arrangement of the elements of the set in some order. If the set has n things in it, there are n! different orderings of its elements. For the first element in an ordering, there are n possible choices, for the second, there remain n−1 possible choices, for the third, there are n−2, etc., and for the nth element of the ordering, there is a single choice remaining. By the fundamental rule of counting, the total number of sequences is thus n×(n−1)×(n−2)×…×1. Similarly, the number of orderings of length k one can form from n≥k things is n×(n−1)×(n−2)×…×(n−k&#43;1) = n!/(n−k)!. This is denoted nPk, the number of permutations of n things taken k at a time. C.f. combinations. 186 Placebo. A “dummy” treatment that has no pharmacological effect; e.g., a sugar pill. 187 Placebo effect. The belief or knowledge that one is being treated can itself have an effect that confounds with the real effect of the treatment. Subjects given a placebo as a pain-killer report statistically significant reductions in pain in randomized experiments that compare them with subjects who receive no treatment at all. This very real psychological effect of a placebo, which has no direct biochemical effect, is called the placebo effect. Administering a placebo to the control group is thus important in experiments with human subjects; this is the essence of a blind experiment. 188 Point of Averages. In a scatterplot, the point whose coordinates are the arithmetic means of the corresponding variables. For example, if the variable X is plotted on the horizontal axis and the variable Y is plotted on the vertical axis, the point of averages has coordinates (mean of X, mean of Y). 189 Poisson Distribution. The Poisson distribution is a discrete probability distribution that depends on one parameter, m. If X is a random variable with the Poisson distribution with parameter m, then the probability that X = k is E−m × mk/k!, k = 0, 1, 2, … , where E is the base of the natural logarithm and ! is the factorial function. For all other values of k, the probability is zero. The expected value the Poisson distribution with parameter m is m, and the standard error of the Poisson distribution with parameter m is m½. 190 Population. A collection of units being studied. Units can be people, places, objects, epochs, drugs, procedures, or many other things. Much of statistics is concerned with estimating numerical properties (parameters) of an entire population from a random sample of units from the population. 191 Population Mean. The mean of the numbers in a numerical population. For example, the population mean of a box of numbered tickets is the mean of the list comprised of all the numbers on all the tickets. The population mean is a parameter. C.f. sample mean. 192 Population Percentage. The percentage of units in a population that possess a specified property. For example, the percentage of a given collection of registered voters who are registered as Republicans. If each unit that possesses the property is labeled with “1,” and each unit that does not possess the property is labeled with “0,” the population percentage is the same as the mean of that list of zeros and ones; that is, the population percentage is the population mean for a population of zeros and ones. The population percentage is a parameter. C.f. sample percentage. 193 Population Standard Deviation. The standard deviation of the values of a variable for a population. This is a parameter, not a statistic. C.f. sample standard deviation. 194 Post hoc ergo propter hoc. “After this, therefore because of this.” A fallacy of logic known since classical times: inferring a causal relation from correlation. Don’t do this at home! 195 Power. Refers to an hypothesis test. The power of a test against a specific alternative hypothesis is the chance that the test correctly rejects the null hypothesis when the alternative hypothesis is true. 196 Premise, logical premise. A premise is a proposition that is assumed to be true as part of a logical argument. 197 Prima facie. Latin for “at first glance.” “On the face of it.” Prima facie evidence for something is information that at first glance supports the conclusion. On closer examination, that might not be true; there could be another explanation for the evidence. 198 Principle of insufficient reason (Laplace) Laplace’s principle of insufficient reason says that if there is no reason to believe that the possible outcomes of an experiment are not equally likely, one should assume that the outcomes are equally likely. This is an example of a fallacy called appeal to ignorance. 199 Probability. The probability of an event is a number between zero and 100%. The meaning (interpretation) of probability is the subject of theories of probability, which differ in their interpretations. However, any rule for assigning probabilities to events has to satisfy the axioms of probability. 200 Probability density function. The chance that a continuous random variable is in any range of values can be calculated as the area under a curve over that range of values. The curve is the probability density function of the random variable. That is, if X is a continuous random variable, there is a function f(x) such that for every pair of numbers a≤b, P(a≤ X ≤b) = (area under f between a and b); f is the probability density function of X. For example, the probability density function of a random variable with a standard normal distribution is the normal curve. Only continuous random variables have probability density functions. 201 Probability Distribution. The probability distribution of a random variable specifies the chance that the variable takes a value in any subset of the real numbers. (The subsets have to satisfy some technical conditions that are not important for this course.) The probability distribution of a random variable is completely characterized by the cumulative probability distribution function; the terms sometimes are used synonymously. The probability distribution of a discrete random variable can be characterized by the chance that the random variable takes each of its possible values. For example, the probability distribution of the total number of spots S showing on the roll of two fair dice can be written as a table: The probability distribution of a continuous random variable can be characterized by its probability density function. 202 Probability Histogram. A probability histogram for a random variable is analogous to a histogram of data, but instead of plotting the area of the bins proportional to the relative frequency of observations in the class interval, one plots the area of the bins proportional to the probability that the random variable is in the class interval. 203 Probability Sample. A sample drawn from a population using a random mechanism so that every element of the population has a known chance of ending up in the sample. 204 Probability, Theories of. A theory of probability is a way of assigning meaning to probability statements such as “the chance that a thumbtack lands point-up is 2/3.” That is, a theory of probability connects the mathematics of probability, which is the set of consequences of the axioms of probability, with the real world of observation and experiment. There are several common theories of probability. According to the frequency theory of probability, the probability of an event is the limit of the percentage of times that the event occurs in repeated, independent trials under essentially the same circumstances. According to the subjective theory of probability, a probability is a number that measures how strongly we believe an event will occur. The number is on a scale of 0% to 100%, with 0% indicating that we are completely sure it won’t occur, and 100% indicating that we are completely sure that it will occur. According to the theory of equally likely outcomes, if an experiment has n possible outcomes, and (for example, by symmetry) there is no reason that any of the n possible outcomes should occur preferentially to any of the others, then the chance of each outcome is 100%/n. Each of these theories has its limitations, its proponents, and its detractors. 205 Proposition, logical proposition. A logical proposition is a statement that can be either true or false. For example, “the sun is shining in Berkeley right now” is a proposition. See also &amp;, ↔, →, /, XOR, converse, contrapositive and logical argument. 206 Prosecutor’s Fallacy. The prosecutor’s fallacy consists of confusing two conditional probabilities: P(A/B) and P(B/A). For instance, P(A/B) could be the chance of observing the evidence if the accused is guilty, while P(B/A) is the chance that the accused is guilty given the evidence. The latter might not make sense at all, but even when it does, the two numbers need not be equal. This fallacy is related to a common misinterpretation of P-values. 207 Qualitative Variable. A qualitative variable is one whose values are adjectives, such as colors, genders, nationalities, etc. C.f. quantitative variable and categorical variable. 208 Quantile. The qth quantile of a list (0 &lt; q ≤ 1) is the smallest number such that the fraction q or more of the elements of the list are less than or equal to it. I.e., if the list contains n numbers, the qth quantile, is the smallest number Q such that at least n×q elements of the list are less than or equal to Q. 209 Quantitative Variable. A variable that takes numerical values for which arithmetic makes sense, for example, counts, temperatures, weights, amounts of money, etc. For some variables that take numerical values, arithmetic with those values does not make sense; such variables are not quantitative. For example, adding and subtracting social security numbers does not make sense. Quantitative variables typically have units of measurement, such as inches, people, or pounds. 210 Quartiles. There are three quartiles. The first or lower quartile (LQ) of a list is a number (not necessarily a number in the list) such that at least 1/4 of the numbers in the list are no larger than it, and at least 3/4 of the numbers in the list are no smaller than it. The second quartile is the median. The third or upper quartile (UQ) is a number such that at least 3/4 of the entries in the list are no larger than it, and at least 1/4 of the numbers in the list are no smaller than it. To find the quartiles, first sort the list into increasing order. Find the smallest integer that is at least as big as the number of entries in the list divided by four. Call that integer k. The kth element of the sorted list is the lower quartile. Find the smallest integer that is at least as big as the number of entries in the list divided by two. Call that integer l. The lth element of the sorted list is the median. Find the smallest integer that is at least as large as the number of entries in the list times 3/4. Call that integer m. The mth element of the sorted list is the upper quartile. 211 Quota Sample. A quota sample is a sample picked to match the population with respect to some summary characteristics. It is not a random sample. For example, in an opinion poll, one might select a sample so that the proportions of various ethnicities in the sample match the proportions of ethnicities in the overall population from which the sample is drawn. Matching on summary statistics does not guarantee that the sample comes close to matching the population with respect to the quantity of interest. As a result, quota samples are typically biased, and the size of the bias is generally impossible to determine unless the result can be compared with a known result for the whole population or for a random sample. Moreover, with a quota sample, it is impossible to quantify how representative of the population a quota sample is likely to be—quota sampling does not allow one to quantify the likely size of sampling error. Quota samples are to be avoided, and results based on quota samples are to be viewed with suspicion. See also convenience sample. 212 Random Error. All measurements are subject to error, which can often be broken down into two components: a bias or systematic error, which affects all measurements the same way; and a random error, which is in general different each time a measurement is made, and behaves like a number drawn with replacement from a box of numbered tickets whose average is zero. 213 Random Event. See random experiment. 214 Random Experiment. An experiment or trial whose outcome is not perfectly predictable, but for which the long-run relative frequency of outcomes of different types in repeated trials is predictable. Note that “random” is different from “haphazard,” which does not necessarily imply long-term regularity. 215 Random Sample. A random sample is a sample whose members are chosen at random from a given population in such a way that the chance of obtaining any particular sample can be computed. The number of units in the sample is called the sample size, often denoted n. The number of units in the population often is denoted N. Random samples can be drawn with or without replacing objects between draws; that is, drawing all n objects in the sample at once (a random sample without replacement), or drawing the objects one at a time, replacing them in the population between draws (a random sample with replacement). In a random sample with replacement, any given member of the population can occur in the sample more than once. In a random sample without replacement, any given member of the population can be in the sample at most once. A random sample without replacement in which every subset of n of the N units in the population is equally likely is also called a simple random sample. The term random sample with replacement denotes a random sample drawn in such a way that every multiset of n units in the population is equally likely. See also probability sample. 216 Random Variable. A random variable is an assignment of numbers to possible outcomes of a random experiment. For example, consider tossing three coins. The number of heads showing when the coins land is a random variable: it assigns the number 0 to the outcome {T, T, T}, the number 1 to the outcome {T, T, H}, the number 2 to the outcome {T, H, H}, and the number 3 to the outcome {H, H, H}. 217 Randomized Controlled Experiment. An experiment in which chance is deliberately introduced in assigning subjects to the treatment and control groups. For example, we could write an identifying number for each subject on a slip of paper, stir up the slips of paper, and draw slips without replacement until we have drawn half of them. The subjects identified on the slips drawn could then be assigned to treatment, and the rest to control. Randomizing the assignment tends to decrease confounding of the treatment effect with other factors, by making the treatment and control groups roughly comparable in all respects but the treatment. 218 Range. The range of a set of numbers is the largest value in the set minus the smallest value in the set. Note that as a statistical term, the range is a single number, not a range of numbers. 219 Real number. Loosely speaking, the real numbers are all numbers that can be represented as fractions (rational numbers), whether proper or improper—and all numbers in between the rational numbers. That is, the real numbers comprise the rational numbers and all limits of Cauchy sequences of rational numbers, where the Cauchy sequence is with respect to the absolute value metric. (More formally, the real numbers are the completion of the set of rational numbers in the topology induced by the absolute value function.) The real numbers contain all integers, all fractions, and all irrational (and transcendental) numbers, such as π, e, and 2½. There are uncountably many real numbers between 0 and 1; in contrast, there are only countably many rational numbers between 0 and 1. 220 Regression, Linear Regression. Linear regression fits a line to a scatterplot in such a way as to minimize the sum of the squares of the residuals. The resulting regression line, together with the standard deviations of the two variables or their correlation coefficient, can be a reasonable summary of a scatterplot if the scatterplot is roughly football-shaped. In other cases, it is a poor summary. If we are regressing the variable Y on the variable X, and if Y is plotted on the vertical axis and X is plotted on the horizontal axis, the regression line passes through the point of averages, and has slope equal to the correlation coefficient times the SD of Y divided by the SD of X. This page shows a scatterplot, with a button to plot the regression line. 221 Regression Fallacy. The regression fallacy is to attribute the regression effect to an external cause. 222 Regression Toward the Mean, Regression Effect. Suppose one measures two variables for each member of a group of individuals, and that the correlation coefficient of the variables is positive (negative). If the value of the first variable for that individual is above average, the value of the second variable for that individual is likely to be above (below) average, but by fewer standard deviations than the first variable is. That is, the second observation is likely to be closer to the mean in standard units. For example, suppose one measures the heights of fathers and sons. Each individual is a (father, son) pair; the two variables measured are the height of the father and the height of the son. These two variables will tend to have a positive correlation coefficient: fathers who are taller than average tend to have sons who are taller than average. Consider a (father, son) pair chosen at random from this group. Suppose the father’s height is 3SD above the average of all the fathers’ heights. (The SD is the standard deviation of the fathers’ heights.) Then the son’s height is also likely to be above the average of the sons’ heights, but by fewer than 3SD (here the SD is the standard deviation of the sons’ heights). 223 Rejection region. In an hypothesis test using a test statistic, the rejection region is the set of values of the test statistic for which we reject the null hypothesis. 224 Residual. The difference between a datum and the value predicted for it by a model. In linear regression of a variable plotted on the vertical axis onto a variable plotted on the horizontal axis, a residual is the “vertical” distance from a datum to the line. Residuals can be positive (if the datum is above the line) or negative (if the datum is below the line). Plots of residuals can reveal computational errors in linear regression, as well as conditions under which linear regression is inappropriate, such as nonlinearity and heteroscedasticity. If linear regression is performed properly, the sum of the residuals from the regression line must be zero; otherwise, there is a computational error somewhere. 225 Residual Plot. A residual plot for a regression is a plot of the residuals from the regression against the explanatory variable. 226 Resistant. A statistic is said to be resistant if corrupting a datum cannot change the statistic much. The mean is not resistant; the median is. See also breakdown point. 227 Root-mean-square (RMS). The RMS of a list is the square-root of the mean of the squares of the elements in the list. It is a measure of the average “size” of the elements of the list. To compute the RMS of a list, you square all the entries, average the numbers you get, and take the square-root of that average. 228 Root-mean-square error (RMSE). The RMSE of an an estimator of a parameter is the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then RMSE(X) = ( E( (X−t)2 ) )½. The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator. See also mean squared error. 229 rms Error of Regression The rms error of regression is the rms of the vertical residuals from the regression line. For regressing Y on X, the rms error of regression is equal to (1 − r2)½×SDY, where r is the correlation coefficient between X and Y and SDY is the standard deviation of the values of Y. 230 Sample. A sample is a collection of units from a population. See also random sample. 231 Sample Mean. The arithmetic mean of a random sample from a population. It is a statistic commonly used to estimate the population mean. Suppose there are n data, {x1, x2, … , xn}. The sample mean is (x1 &#43; x2 &#43; … &#43; xn)/n. The expected value of the sample mean is the population mean. For sampling with replacement, the SE of the sample mean is the population standard deviation, divided by the square-root of the sample size. For sampling without replacement, the SE of the sample mean is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample mean for sampling with replacement, with N the size of the population and n the size of the sample. 232 Sample Percentage. The percentage of a random sample with a certain property, such as the percentage of voters registered as Democrats in a simple random sample of voters. The sample mean is a statistic commonly used to estimate the population percentage. The expected value of the sample percentage from a simple random sample or a random sample with replacement is the population percentage. The SE of the sample percentage for sampling with replacement is (p(1−p)/n )½, where p is the population percentage and n is the sample size. The SE of the sample percentage for sampling without replacement is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample percentage for sampling with replacement, with N the size of the population and n the size of the sample. The SE of the sample percentage is often estimated by the bootstrap. 233 Sample Size. The number of elements in a sample from a population. 234 Sound argument. A logical argument is sound if it is logically valid and its premises are in fact true. An argument can be logically valid and yet not sound—if its premises are false. 235 Sample Standard Deviation, S. The sample standard deviation S is an estimator of the standard deviation of a population based on a random sample from the population. The sample standard deviation is a statistic that measures how “spread out” the sample is around the sample mean. It is quite similar to the standard deviation of the sample, but instead of averaging the squared deviations (to get the rms of the deviations of the data from the sample mean) it divides the sum of the squared deviations by (number of data − 1) before taking the square-root. Suppose there are n data, {x1, x2, … , xn}, with mean M = (x1 &#43; x2 &#43; … &#43; xn)/n. Then s = ( ((x1 − M)2 &#43; (x2 − M)2 &#43; … &#43; (xn − M)2)/(n−1) )½ The square of the sample standard deviation, S2 (the sample variance) is an unbiased estimator of the square of the SD of the population (the variance of the population). 236 Sample Sum. The sum of a random sample from a population. The expected value of the sample sum is the sample size times the population mean. For sampling with replacement, the SE of the sample sum is the population standard deviation, times the square-root of the sample size. For sampling without replacement, the SE of the sample sum is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample sum for sampling with replacement, with N the size of the population and n the size of the sample. 237 Sample Survey. A survey based on the responses of a sample of individuals, rather than the entire population. 238 Sample Variance The sample variance is the square of the sample standard deviation S. It is an unbiased estimator of the square of the population standard deviation, which is also called the variance of the population. 239 Sampling distribution. The sampling distribution of an estimator is the probability distribution of the estimator when it is applied to random samples. The tool on this page allows you to explore empirically the sampling distribution of the sample mean and the sample percentage of random draws with or without replacement draws from a box of numbered tickets. 240 Sampling error. In estimating from a random sample, the difference between the estimator and the parameter can be written as the sum of two components: bias and sampling error. The bias is the average error of the estimator over all possible samples. The bias is not random. Sampling error is the component of error that varies from sample to sample. The sampling error is random: it comes from “the luck of the draw” in which units happen to be in the sample. It is the chance variation of the estimator. The average of the sampling error over all possible samples (the expected value of the sampling error) is zero. The standard error of the estimator is a measure of the typical size of the sampling error. 241 Sampling unit. A sample from a population can be drawn one unit at a time, or more than one unit at a time (one can sample clusters of units). The fundamental unit of the sample is called the sampling unit. It need not be a unit of the population. 242 Scatterplot. A scatterplot is a way to visualize bivariate data. A scatterplot is a plot of pairs of measurements on a collection of “individuals” (which need not be people). For example, suppose we record the heights and weights of a group of 100 people. The scatterplot of those data would be 100 points. Each point represents one person’s height and weight. In a scatterplot of weight against height, the x-coordinate of each point would be height of one person, the y-coordinate of that point would be the weight of the same person. In a scatterplot of height against weight, the x-coordinates would be the weights and the y-coordinates would be the heights. 243 Scientific Method. The scientific method…. 244 SD line. For a scatterplot, a line that goes through the point of averages, with slope equal to the ratio of the standard deviations of the two plotted variables. If the variable plotted on the horizontal axis is called X and the variable plotted on the vertical axis is called Y, the slope of the SD line is the SD of Y, divided by the SD of X. 245 Secular Trend. A linear association (trend) with time. 246 Selection Bias. A systematic tendency for a sampling procedure to include and/or exclude units of a certain type. For example, in a quota sample, unconscious prejudices or predilections on the part of the interviewer can result in selection bias. Selection bias is a potential problem whenever a human has latitude in selecting individual units for the sample; it tends to be eliminated by probability sampling schemes in which the interviewer is told exactly whom to contact (with no room for individual choice). 247 Self-Selection. Self-selection occurs when individuals decide for themselves whether they are in the control group or the treatment group. Self-selection is quite common in studies of human behavior. For example, studies of the effect of smoking on human health involve self-selection: individuals choose for themselves whether or not to smoke. Self-selection precludes an experiment; it results in an observational study. When there is self-selection, one must be wary of possible confounding from factors that influence individuals’ decisions to belong to the treatment group. 248 Set. A set is a collection of things (called elements), without regard to their order. An item is either in a set (it is an element of the set), or it is not. It cannot be in the set more than once. Two sets are equal if they contain electly the same elements. For instance, the set {1, 2, 3, 4} is equal to the set {1, 4, 3, 2}, but not to the set {0, 1, 2, 3}. As another example, the set {1, 2, 2} is equal to the set {1, 2}: they have the same two (distinct) elements, 1 and 2. 249 Significance, Significance level, Statistical significance. The significance level of an hypothesis test is the chance that the test erroneously rejects the null hypothesis when the null hypothesis is true. 250 Simple Random Sample. A simple random sample of n units from a population is a random sample drawn by a procedure that is equally likely to give every collection of n units from the population; that is, the probability that the sample will consist of any given subset of n of the N units in the population is 1/NCn. Simple random sampling is sampling at random without replacement (without replacing the units between draws). A simple random sample of size n from a population of N ≥ n units can be constructed by assigning a random number between zero and one to each unit in the population, then taking those units that were assigned the n largest random numbers to be the sample. 251 Simpson’s Paradox. What is true for the parts is not necessarily true for the whole. See also confounding. 252 Skewed Distribution. A distribution that is not symmetrical. 253 Spread, Measure of. See also inter-quartile range, range, and standard deviation. 254 Square-Root Law. The Square-Root Law says that the standard error (SE) of the sample sum of n random draws with replacement from a box of tickets with numbers on them is SE(sample sum) = n½×SD(box), and the standard error of the sample mean of n random draws with replacement from a box of tickets is SE(sample mean) = n−½×SD(box), where SD(box) is the standard deviation of the list of the numbers on all the tickets in the box (including repeated values). 255 Standard Deviation (SD). The standard deviation of a set of numbers is the rms of the set of deviations between each element of the set and the mean of the set. See also sample standard deviation. 256 Standard Error (SE). The Standard Error of a random variable is a measure of how far it is likely to be from its expected value; that is, its scatter in repeated experiments. The SE of a random variable X is defined to be SE(X) = [E( (X − E(X))2 )] ½. That is, the standard error is the square-root of the expected squared difference between the random variable and its expected value. The SE of a random variable is analogous to the SD of a list. 257 Standard Normal Curve. See normal curve. 258 Standard Units. A variable (a set of data) is said to be in standard units if its mean is zero and its standard deviation is one. You transform a set of data into standard units by subtracting the mean from each element of the list, and dividing the results by the standard deviation. A random variable is said to be in standard units if its expected value is zero and its standard error is one. You transform a random variable to standard units by subtracting its expected value then dividing by its standard error. 259 Standardize. To transform into standard units. 260 Statistic. A number that can be computed from data, involving no unknown parameters. As a function of a random sample, a statistic is a random variable. Statistics are used to estimate parameters, and to test hypotheses. 261 Stratified Sample. In a stratified sample, subsets of sampling units are selected separately from different strata, rather than from the frame as a whole. 262 Stratified sampling The act of drawing a stratified sample. 263 Stratum In random sampling, sometimes the sample is drawn separately from different disjoint subsets of the population. Each such subset is called a stratum. (The plural of stratum is strata.) Samples drawn in such a way are called stratified samples. Estimators based on stratified random samples can have smaller sampling errors than estimators computed from simple random samples of the same size, if the average variability of the variable of interest within strata is smaller than it is across the entire population; that is, if stratum membership is associated with the variable. For example, to determine average home prices in the U.S., it would be advantageous to stratify on geography, because average home prices vary enormously with location. We might divide the country into states, then divide each state into urban, suburban, and rural areas; then draw random samples separately from each such division. 264 Studentized score The observed value of a statistic, minus the expected value of the statistic, divided by the estimated standard error of the statistic. 265 Student’s t curve. Student’s t curve is a family of curves indexed by a parameter called the degrees of freedom, which can take the values 1, 2, … Student’s t curve is used to approximate some probability histograms. Consider a population of numbers that are nearly normally distributed and have population mean is μ. Consider drawing a random sample of size n with replacement from the population, and computing the sample mean M and the sample standard deviation S. Define the random variable T = (M − μ)/(S/n½). If the sample size n is large, the probability histogram of T can be approximated accurately by the normal curve. However, for small and intermediate values of n, Student’s t curve with n − 1 degrees of freedom gives a better approximation. That is, P(a &lt; T &lt; b) is approximately the area under Student’s T curve with n − 1 degrees of freedom, from a to b. Student’s t curve can be used to test hypotheses about the population mean and construct confidence intervals for the population mean, when the population distribution is known to be nearly normally distributed. This page contains a tool that shows Student’s t curve and lets you find the area under parts of the curve. 266 Subject, Experimental Subject. A member of the control group or the treatment group. 267 Subset. A subset of a given set is a collection of things that belong to the original set. Every element of the subset must belong to the original set, but not every element of the original set need be in a subset (otherwise, a subset would always be identical to the set it came from). 268 Survey. See sample survey. 269 Symmetric Distribution. The probability distribution of a random variable X is symmetric if there is a number a such that the chance that X≥a&#43;b is the same as the chance that X≤a−b for every value of b. A list of numbers has a symmetric distribution if there is a number a such that the fraction of numbers in the list that are greater than or equal to a&#43;b is the same as the fraction of numbers in the list that are less than or equal to a−b, for every value of b. In either case, the histogram or the probability histogram will be symmetrical about a vertical line drawn at x=a. 270 Systematic error. An error that affects all the measurements similarly. For example, if a ruler is too short, everything measured with it will appear to be longer than it really is (ignoring random error). If your watch runs fast, every time interval you measure with it will appear to be longer than it really is (again, ignoring random error). Systematic errors do not tend to average out. 271 Systematic sample. A systematic sample from a frame of units is one drawn by listing the units and selecting every kth element of the list. For example, if there are N units in the frame, and we want a sample of size N/10, we would take every tenth unit: the first unit, the eleventh unit, the 21st unit, etc. Systematic samples are not random samples, but they often behave essentially as if they were random, if the order in which the units appears in the list is haphazard. Systematic samples are a special case of cluster samples. 272 Systematic random sample. A systematic sample starting at a random point in the listing of units in the of frame, instead of starting at the first unit. Systematic random sampling is better than systematic sampling, but typically not as good as simple random sampling. 273 t test. An hypothesis test based on approximating the probability histogram of the test statistic by Student’s t curve. t tests usually are used to test hypotheses about the mean of a population when the sample size is intermediate and the distribution of the population is known to be nearly normal. 274 Test Statistic. A statistic used to test hypotheses. An hypothesis test can be constructed by deciding to reject the null hypothesis when the value of the test statistic is in some range or collection of ranges. To get a test with a specified significance level, the chance when the null hypothesis is true that the test statistic falls in the range where the hypothesis would be rejected must be at most the specified significance level. The Z statistic is a common test statistic. 275 Transformation. Transformations turn lists into other lists, or variables into other variables. For example, to transform a list of temperatures in degrees Celsius into the corresponding list of temperatures in degrees Fahrenheit, you multiply each element by 9/5, and add 32 to each product. This is an example of an affine transformation: multiply by something and add something (y = ax &#43; b is the general affine transformation of x; it’s the familiar equation of a straight line). In a linear transformation, you only multiply by something (y = ax). Affine transformations are used to put variables in standard units. In that case, you subtract the mean and divide the results by the SD. This is equivalent to multiplying by the reciprocal of the SD and adding the negative of the mean, divided by the SD, so it is an affine transformation. Affine transformations with positive multiplicative constants have a simple effect on the mean, median, mode, quartiles, and other percentiles: the new value of any of these is the old one, transformed using exactly the same formula. When the multiplicative constant is negative, the mean, median, mode, are still transformed by the same rule, but quartiles and percentiles are reversed: the qth quantile of the transformed distribution is the transformed value of the 1−qth quantile of the original distribution (ignoring the effect of data spacing). The effect of an affine transformation on the SD, range, and IQR, is to make the new value the old value times the absolute value of the number you multiplied the first list by: what you added does not affect them. 276 Treatment. The substance or procedure studied in an experiment or observational study. At issue is whether the treatment has an effect on the outcome or variable of interest. 277 Treatment Effect. The effect of the treatment on the variable of interest. Establishing whether the treatment has an effect is the point of an experiment. 278 Treatment group. The individuals who receive the treatment, as opposed to those in the control group, who do not. 279 Tuple, n-tuple. A tuple is an ordered collection of things. Two tuples are equal if they contain the same things, in the same order. For instance, the tuple (1, 2, 3) is equal to the tuple (1, 2, 3) but not equal to the tuple (1, 3, 2). Tuples can contain repeated elements. For instance, the tuple (1, 2, 2) is not equal to the tuple (1, 2), nor to the tuple (2, 2, 1). An n-tuple, where n is an integer, is a tuple with n positions. For example, (1, 2) is a 2-tuple (aka ordered pair) and (7, 3, 2, 2, 2, 1) is a 6-tuple. 280 Two-sided Hypothesis test. C.f. one-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against the alternative hypothesis that either μ &lt; μ0 or μ &gt; μ0 (the alternative hypothesis contains values on both sides of the null value). For example, a significance level 5%, two-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero would reject the null hypothesis for values of $$ /z/ = \left / \frac{\mbox{sample mean}}{\mbox{SE}} \right / &gt; 1.96.$$ 281 Type I and Type II errors. These refer to hypothesis testing. A Type I error occurs when the null hypothesis is rejected erroneously when it is in fact true. A Type II error occurs if the null hypothesis is not rejected when it is in fact false. See also significance level and power. 282 Unbiased. Not biased; having zero bias. 283 Uncontrolled Experiment. An experiment in which there is no control group; i.e., in which the method of comparison is not used: the experimenter decides who gets the treatment, but the outcome of the treated group is not compared with the outcome of a control group that does not receive treatment. 284 Uncorrelated. A set of bivariate data is uncorrelated if its correlation coefficient is zero. Two random variables are uncorrelated if the expected value of their product equals the product of their expected values. If two random variables are independent, they are uncorrelated. (The converse is not true in general.) 285 Uncountable. A set is uncountable if it is not countable, that is, if its elements cannot be put in one-to-one correspondence with the positive integers. 286 Unimodal. Having exactly one mode. 287 Union. The union of two or more sets is the set of objects contained by at least one of the sets. The union of the events A and B is denoted “A&#43;B”, “A or B”, and “A∪B”. C.f. intersection. 288 Unit. A member of a population. 289 Univariate. Having or having to do with a single variable. Some univariate techniques and statistics include the histogram, IQR, mean, median, percentiles, quantiles, and SD. C.f. bivariate. 290 Upper Quartile (UQ). See quartiles. 291 Valid (logical) argument. A valid logical argument is one in which the truth of the premises indeed guarantees the truth of the conclusion. For example, the following logical argument is valdraft: false id: If the forecast calls for rain, I will not wear sandals. The forecast calls for rain. Therefore, I will not wear sandals. This argument has two premises which, together, guarantee the truth of the conclusion. An argument can be logically valid even if its premises are false. See also invalid argument and sound argument. 292 Variable. A numerical value or a characteristic that can differ from individual to individual. See also categorical variable, qualitative variable, quantitative variable, discrete variable, continuous variable, and random variable. 293 Variance, population variance The variance of a list is the square of the standard deviation of the list, that is, the average of the squares of the deviations of the numbers in the list from their mean. The variance of a random variable X, Var(X), is the expected value of the squared difference between the variable and its expected value: Var(X) = E((X − E(X))2). The variance of a random variable is the square of the standard error (SE) of the variable. 294 Venn Diagram. A pictorial way of showing the relations among sets or events. The universal set or outcome space is usually drawn as a rectangle; sets are regions within the rectangle. The overlap of the regions corresponds to the intersection of the sets. If the regions do not overlap, the sets are disjoint. The part of the rectangle included in one or more of the regions corresponds to the union of the sets. This page contains a tool that illustrates Venn diagrams; the tool represents the probability of an event by the area of the event. 295 XOR, exclusive disjunction. XOR is an operation on two logical propositions. If p and q are two propositions, (p XOR q) is a proposition that is true if either p is true or if q is true, but not both. (p XOR q) is logically equivalent to ((p 296 z-score The observed value of the Z statistic. 297 Z statistic A Z statistic is a test statistic whose distribution under the null hypothesis has expected value zero and can be approximated well by the normal curve. Usually, Z statistics are constructed by standardizing some other statistic. The Z statistic is related to the original statistic by Z = (original − expected value of original)/SE(original). 298 z-test An hypothesis test based on approximating thehttps://dasarpai.com/300-important-statistical-terms/ probability histogram of the Z statistic under the null hypothesis by the normal curve. These definitions are taken from - https://www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm">
  <meta itemprop="datePublished" content="2021-09-29T15:50:00+05:30">
  <meta itemprop="dateModified" content="2021-09-29T15:50:00+05:30">
  <meta itemprop="wordCount" content="22304">
  <meta itemprop="keywords" content="statistical,terminology,,data,science,statistics,,probability,concepts,,statistical,analysis,,statistical,measures,,data,distribution,,hypothesis,testing,,statistical,inference,,statistical,methods,,data,analysis,terms">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="300 Important Statistical Terms">
  <meta name="twitter:description" content="Important Statistical Terms Sno Term Definition 1 Affine transformation. See transformation. 2 Affirming the antecedent. A valid logical argument that concludes from the premise A → B and the premise A that therefore, B is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the antecedent (A) in the conditional. 3 Affirming the consequent. A logical fallacy that argues from the premise A → B and the premise B that therefore, A is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the consequent (B) in the conditional. 4 Alternative Hypothesis. In hypothesis testing, a null hypothesis (typically that there is no effect) is compared with an alternative hypothesis (typically that there is an effect, or that there is an effect of a particular sign). For example, in evaluating whether a new cancer remedy works, the null hypothesis typically would be that the remedy does not work, while the alternative hypothesis would be that the remedy does work. When the data are sufficiently improbable under the assumption that the null hypothesis is true, the null hypothesis is rejected in favor of the alternative hypothesis. (This does not imply that the data are probable under the assumption that the alternative hypothesis is true, nor that the null hypothesis is false, nor that the alternative hypothesis is true. Confused? Take a course in Statistics!) 5 Ante. The up-front cost of a bet: the money you must pay to play the game. From Latin for “before.” 6 Antecedent. In a conditional p → q, the antecedent is p. 7 Appeal to Ignorance. A logical fallacy: taking the absence of evidence to be evidence of absence. If something is not known to be false, assume that it is true; or if something is not known to be true, assume that it is false. For example, if I have no reason to think that anyone in Tajikistan wish me well, that is not evidence that nobody in Tajikistan wishes me well. 8 Association. Two variables are associated if some of the variability of one can be accounted for by the other. In a scatterplot of the two variables, if the scatter in the values of the variable plotted on the vertical axis is smaller in narrow ranges of the variable plotted on the horizontal axis (i.e., in vertical “slices”) than it is overall, the two variables are associated. The correlation coefficient is a measure of linear association, which is a special case of association in which large values of one variable tend to occur with large values of the other, and small values of one tend to occur with small values of the other (positive association), or in which large values of one tend to occur with small values of the other, and vice versa (negative association). 9 Average. An ambiguous term. It often denotes the arithmetic mean, but it can also denote the median, the mode, the geometric mean, and weighted means, among other things. Beware if something reports “the average” without making it clear which average. 10 Axioms of Probability. There are three axioms of probability: (1) Chances are always at least zero. (2) The chance that something happens is 100%. (3) If two events cannot both occur at the same time (if they are disjoint or mutually exclusive), the chance that either one occurs is the sum of the chances that each occurs. For example, consider an experiment that consists of tossing a coin once. The first axiom says that the chance that the coin lands heads, for instance, must be at least zero. The second axiom says that the chance that the coin either lands heads or lands tails or lands on its edge or doesn’t land at all is 100%. The third axiom says that the chance that the coin either lands heads or lands tails is the sum of the chance that the coin lands heads and the chance that the coin lands tails, because both cannot occur in the same coin toss. All other mathematical facts about probability can be derived from these three axioms. For example, it is true that the chance that an event does not occur is (100% − the chance that the event occurs). This is a consequence of the second and third axioms. 11 Base rate fallacy. The base rate fallacy consists of failing to take into account prior probabilities (base rates) when computing conditional probabilities from other conditional probabilities. It is related to the Prosecutor’s Fallacy. For instance, suppose that a test for the presence of some condition has a 1% chance of a false positive result (the test says the condition is present when it is not) and a 1% chance of a false negative result (the test says the condition is absent when the condition is present), so the exam is 99% accurate. What is the chance that an item that tests positive really has the condition? The intuitive answer is 99%, but that is not necessarily true: the correct answer depends on the fraction f of items in the population that have the condition (and on whether the item tested is selected at random from the population). The chance that a randomly selected item tests positive is 0.99×f/(0.99×f &#43; 0.01×(1−f)), which could be much smaller than 99% if f is small. See Bayes’ Rule. 12 Bayes’ Rule. Bayes’ rule expresses the conditional probability of the event A given the event B in terms of the conditional probability of the event B given the event A and the unconditional probability of A: P(A/B) = P(B/A) ×P(A)/( P(B/A)×P(A) &#43; P(B/Ac) ×P(Ac) ). In this expression, the unconditional probability of A is also called the prior probability of A, because it is the probability assigned to A prior to observing any data. Similarly, in this context, P(A/B) is called the posterior probability of A given B, because it is the probability of A updated to reflect (i.e., to condition on) the fact that B was observed to occur. 13 Bernoulli’s Inequality. The Bernoulli Inequality says that if x ≥ −1 then (1&#43;x)n ≥ 1 &#43; nx for every integer n ≥ 0. If n is even, the inequality holds for all x. 14 Bias. A measurement procedure or estimator is said to be biased if, on the average, it gives an answer that differs from the truth. The bias is the average (expected) difference between the measurement and the truth. For example, if you get on the scale with clothes on, that biases the measurement to be larger than your true weight (this would be a positive bias). The design of an experiment or of a survey can also lead to bias. Bias can be deliberate, but it is not necessarily so. See also nonresponse bias. 15 Bimodal. Having two modes. 16 Bin. See class interval. 17 Binomial Coefficient. See combinations. 18 Binomial Distribution. A random variable has a binomial distribution (with parameters n and p) if it is the number of “successes” in a fixed number n of independent random trials, all of which have the same probability p of resulting in “success.” Under these assumptions, the probability of k successes (and n−k failures) is nCk pk(1−p)n−k, where nCk is the number of combinations of n objects taken k at a time: nCk = n!/(k!(n−k)!). The expected value of a random variable with the Binomial distribution is n×p, and the standard error of a random variable with the Binomial distribution is (n×p×(1 − p))½. This page shows the probability histogram of the binomial distribution. 19 Binomial Theorem. The Binomial theorem says that (x&#43;y)n = xn &#43; nxn−1y &#43; … &#43; nCkxn−kyk &#43; … &#43; yn. 20 Bivariate. Having or having to do with two variables. For example, bivariate data are data where we have two measurements of each “individual.” These measurements might be the heights and weights of a group of people (an “individual” is a person), the heights of fathers and sons (an “individual” is a father-son pair), the pressure and temperature of a fixed volume of gas (an “individual” is the volume of gas under a certain set of experimental conditions), etc. Scatterplots, the correlation coefficient, and regression make sense for bivariate data but not univariate data. C.f. univariate. 21 Blind, Blind Experiment. In a blind experiment, the subjects do not know whether they are in the treatment group or the control group. In order to have a blind experiment with human subjects, it is usually necessary to administer a placebo to the control group. 22 Bootstrap estimate of Standard Error. The name for this idea comes from the idiom “to pull oneself up by one’s bootstraps,” which connotes getting out of a hole without anything to stand on. The idea of the bootstrap is to assume, for the purposes of estimating uncertainties, that the sample is the population, then use the SE for sampling from the sample to estimate the SE of sampling from the population. For sampling from a box of numbers, the SD of the sample is the bootstrap estimate of the SD of the box from which the sample is drawn. For sample percentages, this takes a particularly simple form: the SE of the sample percentage of n draws from a box, with replacement, is SD(box)/n½, where for a box that contains only zeros and ones, SD(box) = ((fraction of ones in box)×(fraction of zeros in box) )½. The bootstrap estimate of the SE of the sample percentage consists of estimating SD(box) by ((fraction of ones in sample)×(fraction of zeros in sample))½. When the sample size is large, this approximation is likely to be good. 23 Box model. An analogy between an experiment and drawing numbered tickets “at random” from a box with replacement. For example, suppose we are trying to evaluate a cold remedy by giving it or a placebo to a group of n individuals, randomly choosing half the individuals to receive the remedy and half to receive the placebo. Consider the median time to recovery for all the individuals (we assume everyone recovers from the cold eventually; to simplify things, we also assume that no one recovered in exactly the median time, and that n is even). By definition, half the individuals got better in less than the median time, and half in more than the median time. The individuals who received the treatment are a random sample of size n/2 from the set of n subjects, half of whom got better in less than median time, and half in longer than median time. If the remedy is ineffective, the number of subjects who received the remedy and who recovered in less than median time is like the sum of n/2 draws with replacement from a box with two tickets in it: one with a “1” on it, and one with a “0” on it. This page illustrates the sampling distribution of random draws with or without from a box of numbered tickets. 24 Breakdown Point. The breakdown point of an estimator is the smallest fraction of observations one must corrupt to make the estimator take any value one wants. 25 Categorical Variable. A variable whose value ranges over categories, such as {red, green, blue}, {male, female}, {Arizona, California, Montana, New York}, {short, tall}, {Asian, African-American, Caucasian, Hispanic, Native American, Polynesian}, {straight, curly}, etc. Some categorical variables are ordinal. The distinction between categorical variables and qualitative variables is a bit blurry. C.f. quantitative variable. 26 Causation, causal relation. Two variables are causally related if changes in the value of one cause the other to change. For example, if one heats a rigid container filled with a gas, that causes the pressure of the gas in the container to increase. Two variables can be associated without having any causal relation, and even if two variables have a causal relation, their correlation can be small or zero. 27 Central Limit Theorem. The central limit theorem states that the probability histograms of the sample mean and sample sum of n draws with replacement from a box of labeled tickets converge to a normal curve as the sample size n grows, in the following sense: As n grows, the area of the probability histogram for any range of values approaches the area under the normal curve for the same range of values, converted to standard units. See also the normal approximation. 28 Certain Event. An event is certain if its probability is 100%. Even if an event is certain, it might not occur. However, by the complement rule, the chance that it does not occur is 0%. 29 Chance variation, chance error. A random variable can be decomposed into a sum of its expected value and chance variation around its expected value. The expected value of the chance variation is zero; the standard error of the chance variation is the same as the standard error of the random variable—the size of a “typical” difference between the random variable and its expected value. See also sampling error. 30 Change of Units or Variables. See also transformation. 31 Chebychev’s Inequality. For lists: For every number k&gt;0, the fraction of elements in a list that are k SD’s or further from the arithmetic mean of the list is at most 1/k2. For random variables: For every number k&gt;0, the probability that a random variable X is k SEs or further from its expected value is at most 1/k2. 32 Chi-square curve. The chi-square curve is a family of curves that depend on a parameter called degrees of freedom (d.f.). The chi-square curve is an approximation to the probability histogram of the chi-square statistic for multinomial model if the expected number of outcomes in each category is large. The chi-square curve is positive, and its total area is 100%, so we can think of it as the probability histogram of a random variable. The balance point of the curve is d.f., so the expected value of the corresponding random variable would equal d.f.. The standard error of the corresponding random variable would be (2×d.f.)½. As d.f. grows, the shape of the chi-square curve approaches the shape of the normal curve. This page shows the chi-square curve. 33 Chi-square Statistic. The chi-square statistic is used to measure the agreement between categorical data and a multinomial model that predicts the relative frequency of outcomes in each possible category. Suppose there are n independent trials, each of which can result in one of k possible outcomes. Suppose that in each trial, the probability that outcome i occurs is pi, for i = 1, 2, … , k, and that these probabilities are the same in every trial. The expected number of times outcome 1 occurs in the n trials is n×p1; more generally, the expected number of times outcome i occurs is expectedi = n×pi. If the model be correct, we would expect the n trials to result in outcome i about n×pi times, give or take a bit. Let observedi denote the number of times an outcome of type i occurs in the n trials, for i = 1, 2, … , k. The chi-squared statistic summarizes the discrepancies between the expected number of times each outcome occurs (assuming that the model is true) and the observed number of times each outcome occurs, by summing the squares of the discrepancies, normalized by the expected numbers, over all the categories: chi-squared = (observed1 − expected1)2/expected1 &#43; (observed2 − expected2)2/expected2 &#43; … &#43; (observedk − expectedk)2/expectedk. As the sample size n increases, if the model is correct, the sampling distribution of the chi-squared statistic is approximated increasingly well by the chi-squared curve with (#categories − 1) = k − 1 degrees of freedom (d.f.), in the sense that the chance that the chi-squared statistic is in any given range grows closer and closer to the area under the Chi-Squared curve over the same range. This page illustrates the sampling distribution of the chi-square statistic. 34 Class Boundary. A point that is the left endpoint of one class interval, and the right endpoint of another class interval. 35 Class Interval. In plotting a histogram, one starts by dividing the range of values into a set of non-overlapping intervals, called class intervals, in such a way that every datum is contained in some class interval. See the related entries class boundary and endpoint convention. 36 Cluster Sample. In a cluster sample, the sampling unit is a collection of population units, not single population units. For example, techniques for adjusting the U.S. census start with a sample of geographic blocks, then (try to) enumerate all inhabitants of the blocks in the sample to obtain a sample of people. This is an example of a cluster sample. (The blocks are chosen separately from different strata, so the overall design is a stratified cluster sample.) 37 Combinations. The number of combinations of n things taken k at a time is the number of ways of picking a subset of k of the n things, without replacement, and without regard to the order in which the elements of the subset are picked. The number of such combinations is nCk = n!/(k!(n−k)!), where k! (pronounced “k factorial”) is k×(k−1)×(k−2)× … × 1. The numbers nCk are also called the Binomial coefficients. From a set that has n elements one can form a total of 2n subsets of all sizes. For example, from the set {a, b, c}, which has 3 elements, one can form the 23 = 8 subsets {}, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}. Because the number of subsets with k elements one can form from a set with n elements is nCk, and the total number of subsets of a set is the sum of the numbers of possible subsets of each size, it follows that nC0&#43;nC1&#43;nC2&#43; … &#43;nCn = 2n. The calculator has a button (nCm) that lets you compute the number of combinations of m things chosen from a set of n things. To use the button, first type the value of n, then push the nCm button, then type the value of m, then press the “=” button. 38 Complement. The complement of a subset of a given set is the collection of all elements of the set that are not elements of the subset. 39 Complement rule. The probability of the complement of an event is 100% minus the probability of the event: P(Ac) = 100% − P(A). 40 Compound proposition. A logical proposition formed from other propositions using logical operations such as !, /, XOR, &amp;, → and ↔. 41 Conditional Probability. Suppose we are interested in the probability that some event A occurs, and we learn that the event B occurred. How should we update the probability of A to reflect this new knowledge? This is what the conditional probability does: it says how the additional knowledge that B occurred should affect the probability that A occurred quantitatively. For example, suppose that A and B are mutually exclusive. Then if B occurred, A did not, so the conditional probability that A occurred given that B occurred is zero. At the other extreme, suppose that B is a subset of A, so that A must occur whenever B does. Then if we learn that B occurred, A must have occurred too, so the conditional probability that A occurred given that B occurred is 100%. For in-between cases, where A and B intersect, but B is not a subset of A, the conditional probability of A given B is a number between zero and 100%. Basically, one “restricts” the outcome space S to consider only the part of S that is in B, because we know that B occurred. For A to have happened given that B happened requires that AB happened, so we are interested in the event AB. To have a legitimate probability requires that P(S) = 100%, so if we are restricting the outcome space to B, we need to divide by the probability of B to make the probability of this new S be 100%. On this scale, the probability that AB happened is P(AB)/P(B). This is the definition of the conditional probability of A given B, provided P(B) is not zero (division by zero is undefined). Note that the special cases AB = {} (A and B are mutually exclusive) and AB = B (B is a subset of A) agree with our intuition as described at the top of this paragraph. Conditional probabilities satisfy the axioms of probability, just as ordinary probabilities do. 42 Confidence Interval. A confidence interval for a parameter is a random interval constructed from data in such a way that the probability that the interval contains the true value of the parameter can be specified before the data are collected. Confidence intervals are demonstrated in this page. 43 Confidence Level. The confidence level of a confidence interval is the chance that the interval that will result once data are collected will contain the corresponding parameter. If one computes confidence intervals again and again from independent data, the long-term limit of the fraction of intervals that contain the parameter is the confidence level. 44 Confounding. When the differences between the treatment and control groups other than the treatment produce differences in response that are not distinguishable from the effect of the treatment, those differences between the groups are said to be confounded with the effect of the treatment (if any). For example, prominent statisticians questioned whether differences between individuals that led some to smoke and others not to (rather than the act of smoking itself) were responsible for the observed difference in the frequencies with which smokers and non-smokers contract various illnesses. If that were the case, those factors would be confounded with the effect of smoking. Confounding is quite likely to affect observational studies and experiments that are not randomized. Confounding tends to be decreased by randomization. See also Simpson’s Paradox. 45 Continuity Correction. In using the normal approximation to the binomial probability histogram, one can get more accurate answers by finding the area under the normal curve corresponding to half-integers, transformed to standard units. This is clearest if we are seeking the chance of a particular number of successes. For example, suppose we seek to approximate the chance of 10 successes in 25 independent trials, each with probability p = 40% of success. The number of successes in this scenario has a binomial distribution with parameters n = 25 and p = 40%. The expected number of successes is np = 10, and the standard error is (np(1−p))½ = 6½ = 2.45. If we consider the area under the normal curve at the point 10 successes, transformed to standard units, we get zero: the area under a point is always zero. We get a better approximation by considering 10 successes to be the range from 9 1/2 to 10 1/2 successes. The only possible number of successes between 9 1/2 and 10 1/2 is 10, so this is exactly right for the binomial distribution. Because the normal curve is continuous and a binomial random variable is discrete, we need to “smear out” the binomial probability over an appropriate range. The lower endpoint of the range, 9 1/2 successes, is (9.5 − 10)/2.45 = −0.20 standard units. The upper endpoint of the range, 10 1/2 successes, is (10.5 − 10)/2.45 = &#43;0.20 standard units. The area under the normal curve between −0.20 and &#43;0.20 is about 15.8%. The true binomial probability is 25C10×(0.4)10×(0.6)15 = 16%. In a similar way, if we seek the normal approximation to the probability that a binomial random variable is in the range from i successes to k successes, inclusive, we should find the area under the normal curve from i−1/2 to k&#43;1/2 successes, transformed to standard units. If we seek the probability of more than i successes and fewer than k successes, we should find the area under the normal curve corresponding to the range i&#43;1/2 to k−1/2 successes, transformed to standard units. If we seek the probability of more than i but no more than k successes, we should find the area under the normal curve corresponding to the range i&#43;1/2 to k&#43;1/2 successes, transformed to standard units. If we seek the probability of at least i but fewer than k successes, we should find the area under the normal curve corresponding to the range i−1/2 to k−1/2 successes, transformed to standard units. Including or excluding the half-integer ranges at the ends of the interval in this manner is called the continuity correction. 46 Consequent. In a conditional p → q, the consequent is q. 47 Continuous Variable. A quantitative variable is continuous if its set of possible values is uncountable. Examples include temperature, exact height, exact age (including parts of a second). In practice, one can never measure a continuous variable to infinite precision, so continuous variables are sometimes approximated by discrete variables. A random variable X is also called continuous if its set of possible values is uncountable, and the chance that it takes any particular value is zero (in symbols, if P(X = x) = 0 for every real number x). A random variable is continuous if and only if its cumulative probability distribution function is a continuous function (a function with no jumps). 48 Contrapositive. If p and q are two logical propositions, then the contrapositive of the proposition (p → q) is the proposition ((! q) → (!p) ). The contrapositive is logically equivalent to the original proposition. 49 Control. There are at least three senses of “control” in statistics: a member of the control group, to whom no treatment is given; a controlled experiment, and to control for a possible confounding variable. 50 Controlled experiment. An experiment that uses the method of comparison to evaluate the effect of a treatment by comparing treated subjects with a control group, who do not receive the treatment. 51 Controlled, randomized experiment. A controlled experiment in which the assignment of subjects to the treatment group or control group is done at random, for example, by tossing a coin. 52 Control for a variable. To control for a variable is to try to separate its effect from the treatment effect, so it will not confound with the treatment. There are many methods that try to control for variables. Some are based on matching individuals between treatment and control; others use assumptions about the nature of the effects of the variables to try to model the effect mathematically, for example, using regression. 53 Control group. The subjects in a controlled experiment who do not receive the treatment. 54 Convenience Sample. A sample drawn because of its convenience; it is not a probability sample. For example, I might take a sample of opinions in Berkeley (where I live) by just asking my 10 nearest neighbors. That would be a sample of convenience, and would be unlikely to be representative of all of Berkeley. Samples of convenience are not typically representative, and it is not possible to quantify how unrepresentative results based on samples of convenience are likely to be. Convenience samples are to be avoided, and results based on convenience samples are to be viewed with suspicion. See also quota sample. 55 Converge, convergence. A sequence of numbers x1, x2, x3 … converges if there is a number x such that for any number E&gt;0, there is a number k (which can depend on E) such that /xj − x/ &lt; E whenever j &gt; k. If such a number x exists, it is called the limit of the sequence x1, x2, x3 … . 56 Convergence in probability. A sequence of random variables X1, X2, X3 … converges in probability if there is a random variable X such that for any number E&gt;0, the sequence of numbers P(/X1 − X/ &lt; e), P(/X2 − X/ &lt; e), P(/X3 − X/ &lt; e), … converges to 100%. 57 Converse. If p and q are two logical propositions, then the converse of the proposition (p → q) is the proposition (q → p). 58 Correlation. A measure of linear association between two (ordered) lists. Two variables can be strongly correlated without having any causal relationship, and two variables can have a causal relationship and yet be uncorrelated. 59 Correlation coefficient. The correlation coefficient r is a measure of how nearly a scatterplot falls on a straight line. The correlation coefficient is always between −1 and &#43;1. To compute the correlation coefficient of a list of pairs of measurements (X,Y), first transform X and Y individually into standard units. Multiply corresponding elements of the transformed pairs to get a single list of numbers. The correlation coefficient is the mean of that list of products. This page contains a tool that lets you generate bivariate data with any correlation coefficient you want. 60 Counting. To count a set of things is to put it in one to one correspondence with a consecutive subset of the positive integers (counting numbers). 61 Counting numbers, natural numbers. The counting numbers are the strictly positive integers ({1, 2, 3, … }). (Some authorities include (0) among the counting numbers.) 62 Countable Set. A set is countable if its elements can be put in one-to-one correspondence with a subset of the counting numbers. For example, the sets {0, 1, 7, −3}, {red, green, blue}, {…,−2, −1, 0, 1, 2, …}, {straight, curly}, and the set of all fractions, are countable. If a set is not countable, it is uncountable. The set of all real numbers is uncountable. 63 Cover. A confidence interval is said to cover if the interval contains the true value of the parameter. Before the data are collected, the chance that the confidence interval will contain the parameter value is the coverage probability, which equals the confidence level after the data are collected and the confidence interval is actually computed. 64 Coverage probability. The coverage probability of a procedure for making confidence intervals is the chance that the procedure produces an interval that covers the truth. 65 Critical value The critical value in an hypothesis test is the value of the test statistic beyond which we would reject the null hypothesis. The critical value is set so that the probability that the test statistic is beyond the critical value is at most equal to the significance level if the null hypothesis be true. 66 Cross-sectional study. A cross-sectional study compares different individuals to each other at the same time—it looks at a cross-section of a population. The differences between those individuals can confound with the effect being explored. For example, in trying to determine the effect of age on sexual promiscuity, a cross-sectional study would be likely to confound the effect of age with the effect of the mores the subjects were taught as children: the older individuals were probably raised with a very different attitude towards promiscuity than the younger subjects. Thus it would be imprudent to attribute differences in promiscuity to the aging process. C.f. longitudinal study. 67 Cumulative Probability Distribution Function (cdf). The cumulative distribution function of a random variable is the chance that the random variable is less than or equal to x, as a function of x. In symbols, if F is the cdf of the random variable X, then F(x) = P( X ≤ x). The cumulative distribution function must tend to zero as x approaches minus infinity, and must tend to unity as x approaches infinity. It is a positive function, and increases monotonically: if y &gt; x, then F(y) ≥ F(x). The cumulative distribution function completely characterizes the probability distribution of a random variable. 68 de Morgan’s Laws de Morgan’s Laws are identities involving logical operations: the negation of a conjunction is logically equivalent to the disjunction of the negations, and the negation of a disjunction is logically equivalent to the conjunction of the negations. In symbols, !(p &amp; q) = !p / !q and !(p / q) = !p &amp; !q. 69 Deck of Cards. A standard deck of playing cards contains 52 cards, 13 each of four suits: spades, hearts, diamonds, and clubs. The thirteen cards of each suit are {ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, jack, queen, king}. The face cards are {jack, queen, king}. It is typically assumed that if a deck of cards is shuffled well, it is equally likely to be in each possible ordering. There are 52! (52 factorial) possible orderings. 70 Dependent Events, Dependent Random Variables. Two events or random variables are dependent if they are not independent. 71 Dependent Variable. In regression, the variable whose values are supposed to be explained by changes in the other variable (the the independent or explanatory variable). Usually one regresses the dependent variable on the independent variable. 72 Density, Density Scale. The vertical axis of a histogram has units of percent per unit of the horizontal axis. This is called a density scale; it measures how “dense” the observations are in each bin. See also probability density. 73 Denying the antecedent. A logical fallacy that argues from the premise A → B and the premise !A that therefore, !B. The name comes from the fact that the operation denies (i.e., asserts the negation of) the antecedent (A) in the conditional. 74 Denying the consequent. A valid logical argument that concludes from the premise A → B and the premise !B that therefore, !A. The name comes from the fact that the operation denies (i.e., asserts the logical negation) the consequent (B) in the conditional. 75 Deviation. A deviation is the difference between a datum and some reference value, typically the mean of the data. In computing the SD, one finds the rms of the deviations from the mean, the differences between the individual data and the mean of the data. 76 Discrete Variable. A quantitative variable whose set of possible values is countable. Typical examples of discrete variables are variables whose possible values are a subset of the integers, such as Social Security numbers, the number of people in a family, ages rounded to the nearest year, etc. Discrete variables are “chunky.” C.f. continuous variable. A discrete random variable is one whose set of possible values is countable. A random variable is discrete if and only if its cumulative probability distribution function is a stair-step function; i.e., if it is piecewise constant and only increases by jumps. 77 Disjoint or Mutually Exclusive Events. Two events are disjoint or mutually exclusive if the occurrence of one is incompatible with the occurrence of the other; that is, if they can’t both happen at once (if they have no outcome in common). Equivalently, two events are disjoint if their intersection is the empty set. 78 Disjoint or Mutually Exclusive Sets. Two sets are disjoint or mutually exclusive if they have no element in common. Equivalently, two sets are disjoint if their intersection is the empty set. 79 Distribution. The distribution of a set of numerical data is how their values are distributed over the real numbers. It is completely characterized by the empirical distribution function. Similarly, the probability distribution of a random variable is completely characterized by its probability distribution function. Sometimes the word “distribution” is used as a synonym for the empirical distribution function or the probability distribution function. If two or more random variables are defined for the same experiment, they have a joint probability distribution. 80 Distribution Function, Empirical. The empirical (cumulative) distribution function of a set of numerical data is, for each real value of x, the fraction of observations that are less than or equal to x. A plot of the empirical distribution function is an uneven set of stairs. The width of the stairs is the spacing between adjacent data; the height of the stairs depends on how many data have exactly the same value. The distribution function is zero for small enough (negative) values of x, and is unity for large enough values of x. It increases monotonically: if y &gt; x, the empirical distribution function evaluated at y is at least as large as the empirical distribution function evaluated at x. 81 Double-Blind, Double-Blind Experiment. In a double-blind experiment, neither the subjects nor the people evaluating the subjects knows who is in the treatment group and who is in the control group. This mitigates the placebo effect and guards against conscious and unconscious prejudice for or against the treatment on the part of the evaluators. 82 Ecological Correlation. The correlation between averages of groups of individuals, instead of individuals. Ecological correlation can be misleading about the association of individuals. 83 Element of a Set. See member. 84 Empirical Law of Averages. The Empirical Law of Averages lies at the base of the frequency theory of probability. This law, which is, in fact, an assumption about how the world works, rather than a mathematical or physical law, states that if one repeats a random experiment over and over, independently and under “identical” conditions, the fraction of trials that result in a given outcome converges to a limit as the number of trials grows without bound. 85 Empty Set. The empty set, denoted {} or ∅, is the set that has no members. 86 Endpoint Convention. In plotting a histogram, one must decide whether to include a datum that lies at a class boundary with the class interval to the left or the right of the boundary. The rule for making this assignment is called an endpoint convention. The two standard endpoint conventions are (1) to include the left endpoint of all class intervals and exclude the right, except for the rightmost class interval, which includes both of its endpoints, and (2) to include the right endpoint of all class intervals and exclude the left, except for the leftmost interval, which includes both of its endpoints. 87 Equally Likely Outcomes. According to the equally likely outcome Theory of Probability, if an experiment has a finite number possible outcomes and there is no reason Nature should prefer any of those outcomes over any other (e.g., because the outcome is the result of rolling a symmetric die or tossing a perfectly balanced coin or thoroughly shuffling a deck of cards), then each of those possible outcomes has the same probability. See also Laplace’s Principle of Insufficient Reason. 88 Estimator. An estimator is a rule for “guessing” the value of a population parameter based on a random sample from the population. An estimator is a random variable, because its value depends on which particular sample is obtained, which is random. A canonical example of an estimator is the sample mean, which is an estimator of the population mean. 89 Event. An event is a subset of outcome space. An event determined by a random variable is an event of the form A=(X is in A). When the random variable X is observed, that determines whether or not A occurs: if the value of X happens to be in A, A occurs; if not, A does not occur. 90 Exhaustive. A collection of events {A1, A2, A3, … } exhausts the set A if, for the event A to occur, at least one of those sets must also occur; that is, if S ⊂ A1 ∪ A2 ∪ A3 ∪ … If the event A is not specified, it is assumed to be the entire outcome space S. 91 Expectation, Expected Value. The expected value of a random variable is the long-term limiting average of its values in independent repeated experiments. The expected value of the random variable X is denoted EX or E(X). For a discrete random variable (one that has a countable number of possible values) the expected value is the weighted average of its possible values, where the weight assigned to each possible value is the chance that the random variable takes that value. One can think of the expected value of a random variable as the point at which its probability histogram would balance, if it were cut out of a uniform material. Taking the expected value is a linear operation: if X and Y are two random variables, the expected value of their sum is the sum of their expected values (E(X&#43;Y) = E(X) &#43; E(Y)), and the expected value of a constant a times a random variable X is the constant times the expected value of X (E(a×X ) = a× E(X)). 92 Experiment. What distinguishes an experiment from an observational study is that in an experiment, the experimenter chooses who receives the treatment. 93 Explanatory Variable. In regression, the explanatory or independent variable is the one that is supposed to “explain” the other. For example, in examining crop yield versus quantity of fertilizer applied, the quantity of fertilizer would be the explanatory or independent variable, and the crop yield would be the dependent variable. In experiments, the explanatory variable is the one that is manipulated; the one that is observed is the dependent variable. 94 Extrapolation. See interpolation. 95 Factorial. For an integer k that is greater than or equal to 1, k! (pronounced “k factorial”) is k×(k−1)×(k−2)× …×1. By convention, 0! = 1. There are k! ways of ordering k distinct objects. For example, 9! is the number of batting orders of 9 baseball players, and 52! is the number of different ways a standard deck of playing cards can be ordered. The calculator above has a button to compute the factorial of a number. To compute k!, first type the value of k, then press the button labeled “!”. 96 Fair Bet. A fair bet is one for which the expected value of the payoff is zero, after accounting for the cost of the bet. For example, suppose I offer to pay you $2 if a fair coin lands heads, but you must ante up $1 to play. Your expected payoff is −$1&#43; $0×P(tails) &#43; $2×P(heads) = −$1 &#43; $2×50% = $0. This is a fair bet—in the long run, if you made this bet over and over again, you would expect to break even. 97 False Discovery Rate. In testing a collection of hypotheses, the false discovery rate is the fraction of rejected null hypotheses that are rejected erroneously (the number of Type I errors divided by the number of rejected null hypotheses), with the convention that if no hypothesis is rejected, the false discovery rate is zero. 98 Finite, finite set. A set is finite if it has a finite number of elements, that is, if for some natural number n, the elements can be put in one-to-one correspondence with the set {1, 2, … n}. 99 Finite Population Correction. When sampling without replacement, as in a simple random sample, the SE of sample sums and sample means depends on the fraction of the population that is in the sample: the greater the fraction, the smaller the SE. Sampling with replacement is like sampling from an infinitely large population. The adjustment to the SE for sampling without replacement is called the finite population correction. The SE for sampling without replacement is smaller than the SE for sampling with replacement by the finite population correction factor ((N −n)/(N − 1))½. Note that for sample size n=1, there is no difference between sampling with and without replacement; the finite population correction is then unity. If the sample size is the entire population of N units, there is no variability in the result of sampling without replacement (every member of the population is in the sample exactly once), and the SE should be zero. This is indeed what the finite population correction gives (the numerator vanishes). 100 Fisher’s exact test (for the equality of two percentages) Consider two populations of zeros and ones. Let p1 be the proportion of ones in the first population, and let p2 be the proportion of ones in the second population. We would like to test the null hypothesis that p1 = p2 on the basis of a simple random sample from each population. Let n1 be the size of the sample from population 1, and let n2 be the size of the sample from population 2. Let G be the total number of ones in both samples. If the null hypothesis be true, the two samples are like one larger sample from a single population of zeros and ones. The allocation of ones between the two samples would be expected to be proportional to the relative sizes of the samples, but would have some chance variability. Conditional on G and the two sample sizes, under the null hypothesis, the tickets in the first sample are like a random sample of size n1 without replacement from a collection of N = n1 &#43; n2 units of which G are labeled with ones. Thus, under the null hypothesis, the number of tickets labeled with ones in the first sample has (conditional on G) an hypergeometric distribution with parameters N, G, and n1. Fisher’s exact test uses this distribution to set the ranges of observed values of the number of ones in the first sample for which we would reject the null hypothesis. 101 Football-Shaped Scatterplot. In a football-shaped scatterplot, most of the points lie within a tilted oval, shaped more-or-less like a football. A football-shaped scatterplot is one in which the data are homoscedastically scattered about a straight line. 102 Frame, sampling frame. A sampling frame is a collection of units from which a sample will be drawn. Ideally, the frame is identical to the population we want to learn about; more typically, the frame is only a subset of the population of interest. The difference between the frame and the population can be a source of bias in sampling design, if the parameter of interest has a different value for the frame than it does for the population. For example, one might desire to estimate the current annual average income of 1998 graduates of the University of California at Berkeley. I propose to use the sample mean income of a sample of graduates drawn at random. To facilitate taking the sample and contacting the graduates to obtain income information from them, I might draw names at random from the list of 1998 graduates for whom the alumni association has an accurate current address. The population is the collection of 1998 graduates; the frame is those graduates who have current addresses on file with the alumni association. If there is a tendency for graduates with higher incomes to have up-to-date addresses on file with the alumni association, that would introduce a positive bias into the annual average income estimated from the sample by the sample mean. 103 FPP. Statistics, third edition, by Freedman, Pisani, and Purves, published by W.W. Norton, 1997. 104 Frequency theory of probability. See Probability, Theories of. 105 Frequency table. A table listing the frequency (number) or relative frequency (fraction or percentage) of observations in different ranges, called class intervals. 106 Fundamental Rule of Counting. If a sequence of experiments or trials T1, T2, T3, …, Tk could result, respectively, in n1, n2 n3, …, nk possible outcomes, and the numbers n1, n2 n3, …, nk do not depend on which outcomes actually occurred, the entire sequence of k experiments has n1× n2 × n3× …× nk possible outcomes. 107 Game Theory. A field of study that bridges mathematics, statistics, economics, and psychology. It is used to study economic behavior, and to model conflict between nations, for example, “nuclear stalemate” during the Cold War. 108 Geometric Distribution. The geometric distribution describes the number of trials up to and including the first success, in independent trials with the same probability of success. The geometric distribution depends only on the single parameter p, the probability of success in each trial. For example, the number of times one must toss a fair coin until the first time the coin lands heads has a geometric distribution with parameter p = 50%. The geometric distribution assigns probability p×(1 − p)k−1to the event that it takes k trials to the first success. The expected value of the geometric distribution is 1/p, and its SE is (1−p)½/p. 109 Geometric Mean. The geometric mean of n numbers {x1, x2, x3, …, xn} is the nth root of their product: (x1×x2×x3× … ×xn)1/n. 110 Graph of Averages. For bivariate data, a graph of averages is a plot of the average values of one variable (say y) for small ranges of values of the other variable (say x), against the value of the second variable (x) at the midpoints of the ranges. 111 Heteroscedasticity. “Mixed scatter.” A scatterplot or residual plot shows heteroscedasticity if the scatter in vertical slices through the plot depends on where you take the slice. Linear regression is not usually a good idea if the data are heteroscedastic. 112 Histogram. A histogram is a kind of plot that summarizes how data are distributed. Starting with a set of class intervals, the histogram is a set of rectangles (“bins”) sitting on the horizontal axis. The bases of the rectangles are the class intervals, and their heights are such that their areas are proportional to the fraction of observations in the corresponding class intervals. That is, the height of a given rectangle is the fraction of observations in the corresponding class interval, divided by the length of the corresponding class interval. A histogram does not need a vertical scale, because the total area of the histogram must equal 100%. The units of the vertical axis are percent per unit of the horizontal axis. This is called the density scale. The horizontal axis of a histogram needs a scale. If any observations coincide with the endpoints of class intervals, the endpoint convention is important. This page contains a histogram tool, with controls to highlight ranges of values and read their areas. 113 Historical Controls. Sometimes, the a treatment group is compared with individuals from another epoch who did not receive the treatment; for example, in studying the possible effect of fluoridated water on childhood cancer, we might compare cancer rates in a community before and after fluorine was added to the water supply. Those individuals who were children before fluoridation started would comprise an historical control group. Experiments and studies with historical controls tend to be more susceptible to confounding than those with contemporary controls, because many factors that might affect the outcome other than the treatment tend to change over time as well. (In this example, the level of other potential carcinogens in the environment also could have changed.) 114 Homoscedasticity. “Same scatter.” A scatterplot or residual plot shows homoscedasticity if the scatter in vertical slices through the plot does not depend much on where you take the slice. C.f. heteroscedasticity. 115 House Edge. In casino games, the expected payoff to the bettor is negative: the house (casino) tends to win money in the long run. The amount of money the house would expect to win for each $1 wagered on a particular bet (such as a bet on “red” in roulette) is called the house edge for that bet. 116 HTLWS. The book How to lie with Statistics by D. Huff. 117 Hypergeometric Distribution. The hypergeometric distribution with parameters N, G and n is the distribution of the number of “good” objects in a simple random sample of size n (i.e., a random sample without replacement in which every subset of size n has the same chance of occurring) from a population of N objects of which G are “good.” The chance of getting exactly g good objects in such a sample is GCg × N−GCn−g/NCn, provided g ≤ n, g ≤ G, and n − g ≤ N − G. (The probability is zero otherwise.) The expected value of the hypergeometric distribution is n×G/N, and its standard error is ((N−n)/(N−1))½ × (n × G/N × (1−G/N) )½. 118 Hypothesis testing. Statistical hypothesis testing is formalized as making a decision between rejecting or not rejecting a null hypothesis, on the basis of a set of observations. Two types of errors can result from any decision rule (test): rejecting the null hypothesis when it is true (a Type I error), and failing to reject the null hypothesis when it is false (a Type II error). For any hypothesis, it is possible to develop many different decision rules (tests). Typically, one specifies ahead of time the chance of a Type I error one is willing to allow. That chance is called the significance level of the test or decision rule. For a given significance level, one way of deciding which decision rule is best is to pick the one that has the smallest chance of a Type II error when a given alternative hypothesis is true. The chance of correctly rejecting the null hypothesis when a given alternative hypothesis is true is called the power of the test against that alternative. 119 iff, if and only if, ↔ If p and q are two logical propositions, then(p ↔ q) is a proposition that is true when both p and q are true, and when both p and q are false. It is logically equivalent to the proposition ( (p → q) &amp; (q → p) ) and to the proposition ( (p &amp; q) 120 Implies, logical implication, → , conditional, if-then Logical implication is an operation on two logical propositions. If p and q are two logical propositions, (p → q), pronounced “p implies q” or “if p then q” is a logical proposition that is true if p is false, or if both p and q are true. The proposition (p → q) is logically equivalent to the proposition ((!p) / q). In the conditional p → q, the antecedent is p and the consequent is q. 121 Independent, independence. Two events A and B are (statistically) independent if the chance that they both happen simultaneously is the product of the chances that each occurs individually; i.e., if P(AB) = P(A)P(B). This is essentially equivalent to saying that learning that one event occurs does not give any information about whether the other event occurred too: the conditional probability of A given B is the same as the unconditional probability of A, i.e., P(A/B) = P(A). Two random variables X and Y are independent if all events they determine are independent, for example, if the event {a &lt; X ≤ b} is independent of the event {c &lt; Y ≤ d} for all choices of a, b, c, and d. A collection of more than two random variables is independent if for every proper subset of the variables, every event determined by that subset of the variables is independent of every event determined by the variables in the complement of the subset. For example, the three random variables X, Y, and Z are independent if every event determined by X is independent of every event determined by Y and every event determined by X is independent of every event determined by Y and Z and every event determined by Y is independent of every event determined by X and Z and every event determined by Z is independent of every event determined by X and Y. 122 Independent and identically distributed (iid). A collection of two or more random variables {X1, X2, … , } is independent and identically distributed if the variables have the same probability distribution, and are independent. 123 Independent Variable. In regression, the independent variable is the one that is supposed to explain the other; the term is a synonym for “explanatory variable.” Usually, one regresses the “dependent variable” on the “independent variable.” There is not always a clear choice of the independent variable. The independent variable is usually plotted on the horizontal axis. Independent in this context does not mean the same thing as statistically independent. 124 Indicator Random Variable. The indicator [random variable] of the event A, often written 1A, is the random variable that equals unity if A occurs, and zero if A does not occur. The expected value of the indicator of A is the probability of A, P(A), and the standard error of the indicator of A is (P(A)×(1−P(A))½. The sum 1A &#43; 1B &#43; 1C &#43; … of the indicators of a collection of events {A, B, C, …} counts how many of the events {A, B, C, …} occur in a given trial. The product of the indicators of a collection of events is the indicator of the intersection of the events (the product equals one if and only if all of indicators equal one). The maximum of the indicators of a collection of events is the indicator of the union of the events (the maximum equals one if any of the indicators equals one). 125 Inter-quartile Range (IQR). The inter-quartile range of a list of numbers is the upper quartile minus the lower quartile. 126 Interpolation. Given a set of bivariate data (x, y), to impute a value of y corresponding to some value of x at which there is no measurement of y is called interpolation, if the value of x is within the range of the measured values of x. If the value of x is outside the range of measured values, imputing a corresponding value of y is called extrapolation. 127 Intersection. The intersection of two or more sets is the set of elements that all the sets have in common; the elements contained in every one of the sets. The intersection of the events A and B is written “A∩B,” “A and B,” and “AB.” C.f. union. See also Venn diagrams. 128 Invalid (logical) argument. An invalid logical argument is one in which the truth of the premises does not guarantee the truth of the conclusion. For example, the following logical argument is invaldraft: false id: If the forecast calls for rain, I will not wear sandals. The forecast does not call for rain. Therefore, I will wear sandals. See also valid argument. 129 Joint Probability Distribution. If X1, X2, … , Xk are random variables defined for the same experiment, their joint probability distribution gives the probability of events determined by the collection of random variables: for any collection of sets of numbers {A1, … , Ak}, the joint probability distribution determines P( (X1 is in A1) and (X2 is in A2) and … and (Xk is in Ak) ). For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. If a collection of random variables is independent, their joint probability distribution is the product of their marginal probability distributions, their individual probability distributions without regard for the value of the other variables. In this example, the marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6, and the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Note that P(X1 = 1, X2 = 10) = 0, while P(X1 = 1)×P(X2 = 10) = (1/6)(3/36) = 1/72. The joint probability is not equal to the product of the marginal probabilities: X1 and X2 are dependent random variables. 130 Law of Averages. The Law of Averages says that the average of independent observations of random variables that have the same probability distribution is increasingly likely to be close to the expected value of the random variables as the number of observations grows. More precisely, if X1, X2, X3, …, are independent random variables with the same probability distribution, and E(X) is their common expected value, then for every number ε &gt; 0, P{/(X1 &#43; X2 &#43; … &#43; Xn)/n − E(X) / &lt; ε} converges to 100% as n grows. This is equivalent to saying that the sequence of sample means X1, (X1&#43;X2)/2, (X1&#43;X2&#43;X3)/3, … converges in probability to E(X). 131 Law of Large Numbers. The Law of Large Numbers says that in repeated, independent trials with the same probability p of success in each trial, the percentage of successes is increasingly likely to be close to the chance of success as the number of trials increases. More precisely, the chance that the percentage of successes differs from the probability p by more than a fixed positive amount, e &gt; 0, converges to zero as the number of trials n goes to infinity, for every number e &gt; 0. Note that in contrast to the difference between the percentage of successes and the probability of success, the difference between the number of successes and the expected number of successes, n×p, tends to grow as n grows. The following tool illustrates the law of large numbers; the button toggles between displaying the difference between the number of successes and the expected number of successes, and the difference between the percentage of successes and the expected percentage of successes. The tool on this page illustrates the law of large numbers. 132 Limit. See converge. 133 Linear Operation. Suppose f is a function or operation that acts on things we shall denote generically by the lower-case Roman letters x and y. Suppose it makes sense to multiply x and y by numbers (which we denote by a), and that it makes sense to add things like x and y together. We say that f is linear if for every number a and every value of x and y for which f(x) and f(y) are defined, (i) f( a×x ) is defined and equals a×f(x), and (ii) f( x &#43; y ) is defined and equals f(x) &#43; f(y). C.f. affine. 134 Linear association. Two variables are linearly associated if a change in one is associated with a proportional change in the other, with the same constant of proportionality throughout the range of measurement. The correlation coefficient measures the degree of linear association on a scale of −1 to 1. 135 List. I use the term list to mean two things: either a multiset or (more often) an tuple. Lists are countable collections (multisets) in some order (like a tuple). That is, it makes sense to talk about the 1st (or 7th, or nth) element of a list, and the nth and mth elements of a list can be equal, even if n ≠ m (the elements of a list need not be distinct). 136 Location, Measure of. A measure of location is a way of summarizing what a “typical” element of a list is—it is a one-number summary of a distribution. See also arithmetic mean, median, and mode. 137 Logical argument. A logical argument consists of one or more premises, propositions that are assumed to be true, and a conclusion, a proposition that is supposed to be guaranteed to be true (as a matter of pure logic) if the premises are true. For example, the following is a logical argument: p → q Therefore, q. This argument has two premises: p → q, and p. The conclusion of the argument is q. If a logical argument is valid if the truth of the premises guarantees the truth of the conclusion; otherwise, the argument is invalid. That is, an argument with premises p1, p1, … pn and conclusion q is valid if the compound proposition (p1 &amp; p2 &amp; … &amp; pn) → q is logically equivalent to TRUE. The argument given above is valid because if it is true that p → q and that p is true (the two premises), then q (the conclusion of the argument) must also be true. 138 Logically equivalent, logical equivalence. Two propositions are logically equivalent if they always have the same truth value. That is, the propositions p and q are logically equivalent if p is true whenever q is true and p is false whenever q is false. The proposition (p ↔ q) is always true if and only if p and q are logically equivalent. For example, p is logically equivalent to p, to (p &amp; p), and to (p / p); (p / (!p)) is logically equivalent to TRUE; (p &amp; !p) is logically equivalent to FALSE; (p ↔ p) is logically equivalent to TRUE; and (p → q) is logically equivalent to (!p / q). 139 Longitudinal study. A study in which individuals are followed over time, and compared with themselves at different times, to determine, for example, the effect of aging on some measured variable. Longitudinal studies provide much more persuasive evidence about the effect of aging than do cross-sectional studies. 140 Lower Quartile (LQ). See quartiles. 141 Margin of error. A measure of the uncertainty in an estimate of a parameter; unfortunately, not everyone agrees what it should mean. The margin of error of an estimate is typically one or two times the estimated standard error of the estimate. 142 Marginal probability distribution. The marginal probability distribution of a random variable that has a joint probability distribution with some other random variables is the probability distribution of that random variable without regard for the values that the other random variables take. The marginal distribution of a discrete random variable X1 that has a joint distribution with other discrete random variables can be found from the joint distribution by summing over all possible values of the other variables. For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. The marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6. We can verify that the marginal probability that X1 = 1 is indeed the sum of the joint probability distribution over all possible values of X2 for which X1 = 1: P(X1 = 1) = P(X1 = 1, X2 = 2) &#43; P(X1 = 1, X2 = 3) &#43; P(X1 = 1, X2 = 4) &#43; P(X1 = 1, X2 = 5) &#43; P(X1 = 1, X2 = 6) &#43; P(X1 = 1, X2 = 7) = 6/36 = 1/6. Similarly, the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Again, we can verify that the marginal probability that X2 = 4 is 3/36 by adding the joint probabilities for all possible values of X1 for which X2 = 4: P(X2 = 4) = P(X1 = 1, X2 = 4) &#43; P(X1 = 2, X2 = 4) &#43; P(X1 = 3, X2 = 4) = 3/36. 143 Markov’s Inequality. For lists: If a list contains no negative numbers, the fraction of numbers in the list at least as large as any given constant a&gt;0 is no larger than the arithmetic mean of the list, divided by a. For random variables: if a random variable X must be nonnegative, the chance that X exceeds any given constant a&gt;0 is no larger than the expected value of X, divided by a. 144 Maximum Likelihood Estimate (MLE). The maximum likelihood estimate of a parameter from data is the possible value of the parameter for which the chance of observing the data largest. That is, suppose that the parameter is p, and that we observe data x. Then the maximum likelihood estimate of p is estimate p by the value q that makes P(observing x when the value of p is q) as large as possible. For example, suppose we are trying to estimate the chance that a (possibly biased) coin lands heads when it is tossed. Our data will be the number of times x the coin lands heads in n independent tosses of the coin. The distribution of the number of times the coin lands heads is binomial with parameters n (known) and p (unknown). The chance of observing x heads in n trials if the chance of heads in a given trial is q is nCx qx(1−q)n−x. The maximum likelihood estimate of p would be the value of q that makes that chance largest. We can find that value of q explicitly using calculus; it turns out to be q = x/n, the fraction of times the coin is observed to land heads in the n tosses. Thus the maximum likelihood estimate of the chance of heads from the number of heads in n independent tosses of the coin is the observed fraction of tosses in which the coin lands heads. 145 Mean, Arithmetic mean. The sum of a list of numbers, divided by the number of elements in the list. See also average. 146 Mean Squared Error (MSE). The mean squared error of an estimator of a parameter is the expected value of the square of the difference between the estimator and the parameter. In symbols, if X is an estimator of the parameter t, then MSE(X) = E( (X−t)2 ). The MSE measures how far the estimator is off from what it is trying to estimate, on the average in repeated experiments. It is a summary measure of the accuracy of the estimator. It combines any tendency of the estimator to overshoot or undershoot the truth (bias), and the variability of the estimator (SE). The MSE can be written in terms of the bias and SE of the estimator: MSE(X) = (bias(X))2 &#43; (SE(X))2. 147 Median. “Middle value” of a list. The smallest number such that at least half the numbers in the list are no greater than it. If the list has an odd number of entries, the median is the middle entry in the list after sorting the list into increasing order. If the list has an even number of entries, the median is the smaller of the two middle numbers after sorting. The median can be estimated from a histogram by finding the smallest number such that the area under the histogram to the left of that number is 50%. 148 Member of a set. Something is a member (or element) of a set if it is one of the things in the set. 149 Method of Comparison. The most basic and important method of determining whether a treatment has an effect: compare what happens to individuals who are treated (the treatment group) with what happens to individuals who are not treated (the control group). 150 Minimax Strategy. In game theory, a minimax strategy is one that minimizes one’s maximum loss, whatever the opponent might do (whatever strategy the opponent might choose). 151 Mode. For lists, the mode is a most common (frequent) value. A list can have more than one mode. For histograms, a mode is a relative maximum (“bump”). 152 Moment. The kth moment of a list is the average value of the elements raised to the kth power; that is, if the list consists of the N elements x1, x2, … , xN, the kth moment of the list is ( x1k &#43; x2k &#43; xNk )/N. The kth moment of a random variable X is the expected value of Xk, E(Xk). 153 Monotone, monotonic function. A function is monotone if it only increases or only decreases: f increases monotonically (is monotonic increasing) if x &gt; y, implies thatf(x) ≥ f(y). A function f decreases monotonically (is monotonic decreasing) if x &gt; y, implies thatf(x) ≤ f(y). A function f is strictly monotonically increasing if x &gt; y, implies thatf(x) &gt; f(y), and strictly monotonically decreasing if if x &gt; y, implies thatf(x) &lt; f(y). 154 Multimodal Distribution. A distribution with more than one mode. The histogram of a multimodal distribution has more than one “bump.” 155 Multinomial Distribution Consider a sequence of n independent trials, each of which can result in an outcome in any of k categories. Let pj be the probability that each trial results in an outcome in category j, j = 1, 2, … , k, so p1 &#43; p2 &#43; … &#43; pk = 100%. The number of outcomes of each type has a multinomial distribution. In particular, the probability that the n trials result in n1 outcomes of type 1, n2 outcomes of type 2, … , and nk outcomes of type k is n!/(n1! × n2! × … × nk!) × p1n1 × p2n2 × … × pknk, if n1, … , nk are nonnegative integers that sum to n; the chance is zero otherwise. 156 Multiplication rule. The chance that events A and B both occur (i.e., that event AB occurs), is the conditional probability that A occurs given that B occurs, times the unconditional probability that B occurs. 157 Multiplicity in hypothesis tests. In hypothesis testing, if more than one hypothesis is tested, the actual significance level of the combined tests is not equal to the nominal significance level of the individual tests. See also false discovery rate. 158 Multivariate Data. A set of measurements of two or more variables per individual. See bivariate. 159 Multiset. A multiset, also known as a bag is a collection of things, but—unlike a set, which is also a collection of things—the same object can occur in a multiset more than once. For instance, the sets {1, 2}, {1, 2, 2}, and {1, 1, 1, 1, 1, 2, 2} are all equal, while the multisets [1, 2], [1, 2, 2], and [1, 1, 1, 1, 1, 2, 2] are all different. However, order does not matter for sets or for multisets, so, for instance {1, 2} = {2, 1} and [1, 1, 1, 1, 1, 2, 2] = [2, 1, 1, 2, 1, 1, 1]. 160 Mutually Exclusive. See disjoint events or disjoint sets. 161 Nearly normal distribution. A population of numbers (a list of numbers) is said to have a nearly normal distribution if the histogram of its values in standard units nearly follows a normal curve. More precisely, suppose that the mean of the list is μ and the standard deviation of the list is SD. Then the list is nearly normally distributed if, for every two numbers a &lt; b, the fraction of numbers in the list that are between a and b is approximately equal to the area under the normal curve between (a − μ)/SD and (a − μ)/SD. 162 Negative Binomial Distribution. Consider a sequence of independent trials with the same probability p of success in each trial. The number of trials up to and including the rth success has the negative Binomial distribution with parameters n and r. If the random variable N has the negative binomial distribution with parameters n and r, then P(N=k) = k−1Cr−1 × pr × (1−p)k−r, for k = r, r&#43;1, r&#43;2, …, and zero for k &lt; r, because there must be at least r trials to have r successes. The negative binomial distribution is derived as follows: for the rth success to occur on the kth trial, there must have been r−1 successes in the first k−1 trials, and the kth trial must result in success. The chance of the former is the chance of r−1 successes in k−1 independent trials with the same probability of success in each trial, which, according to the Binomial distribution with parameters n=k−1 and p, has probability k−1Cr−1 × pr−1 × (1−p)k−r. The chance of the latter event is p, by assumption. Because the trials are independent, we can find the chance that both events occur by multiplying their chances together, which gives the expression for P(N=k) above. 163 No causation without manipulation. A slogan attributed to Paul Holland. If the conditions were not deliberately manipulated (for example, if the situation is an observational study rather than an experiment), it is unwise to conclude that there is any causal relationship between the outcome and the conditions. See post hoc ergo propter hoc and cum hoc ergo propter hoc. 164 Nonlinear Association. The relationship between two variables is nonlinear if a change in one is associated with a change in the other that is depends on the value of the first; that is, if the change in the second is not simply proportional to the change in the first, independent of the value of the first variable. 165 Nonresponse. In surveys, it is rare that everyone who is “invited” to participate (everyone whose phone number is called, everyone who is mailed a questionnaire, everyone an interviewer tries to stop on the street…) in fact responds. The difference between the “invited” sample sought, and that obtained, is the nonresponse. 166 Nonresponse bias. In a survey, those who respond may differ from those who do not, in ways that are related to the effect one is trying to measure. For example, a telephone survey of how many hours people work is likely to miss people who are working late, and are therefore not at home to answer the phone. When that happens, the survey may suffer from nonresponse bias. Nonresponse bias makes the result of a survey differ systematically from the truth. 167 Nonresponse rate. The fraction of nonresponders in a survey: the number of nonresponders divided by the number of people invited to participate (the number sent questionnaires, the number of interview attempts, etc.) If the nonresponse rate is appreciable, the survey suffer from large nonresponse bias. 168 Normal approximation. The normal approximation to data is to approximate areas under the histogram of data, transformed into standard units, by the corresponding areas under the normal curve. Many probability distributions can be approximated by a normal distribution, in the sense that the area under the probability histogram is close to the area under a corresponding part of the normal curve. To find the corresponding part of the normal curve, the range must be converted to standard units, by subtracting the expected value and dividing by the standard error. For example, the area under the binomial probability histogram for n = 50 and p = 30% between 9.5 and 17.5 is 74.2%. To use the normal approximation, we transform the endpoints to standard units, by subtracting the expected value (for the Binomial random variable, n×p = 15 for these values of n and p) and dividing the result by the standard error (for a Binomial, (n × p × (1−p))1/2 = 3.24 for these values of n and p). The area normal approximation is the area under the normal curve between (9.5 − 15)/3.24 = −1.697 and (17.5 − 15)/3.24 = 0.772; that area is 73.5%, slightly smaller than the corresponding area under the binomial histogram. See also the continuity correction. The tool on this page illustrates the normal approximation to the binomial probability histogram. Note that the approximation gets worse when p gets close to 0 or 1, and that the approximation improves as n increases. 169 Normal curve. The normal curve is the familiar “bell curve:,” illustrated on this page. The mathematical expression for the normal curve is y = (2×pi)−½e−x2/2, where pi is the ratio of the circumference of a circle to its diameter (3.14159265…), and e is the base of the natural logarithm (2.71828…). The normal curve is symmetric around the point x=0, and positive for every value of x. The area under the normal curve is unity, and the SD of the normal curve, suitably defined, is also unity. Many (but not most) histograms, converted into standard units, approximately follow the normal curve. 170 Normal distribution. A random variable X has a normal distribution with mean m and standard error s if for every pair of numbers a ≤ b, the chance that a &lt; (X−m)/s &lt; b is P(a &lt; (X−m)/s &lt; b) = area under the normal curve between a and b. If there are numbers m and s such that X has a normal distribution with mean m and standard error s, then X is said to have a normal distribution or to be normally distributed. If X has a normal distribution with mean m=0 and standard error s=1, then X is said to have a standard normal distribution. The notation XN(m,s2) means that X has a normal distribution with mean m and standard error s; for example, XN(0,1), means X has a standard normal distribution. 171 NOT, !, Negation, Logical Negation. The negation of a logical proposition p, !p, is a proposition that is the logical opposite of p. That is, if p is true, !p is false, and if p is false, !p is true. Negation takes precedence over other logical operations. Other common symbols for the negation operator include ¬, − and ˜. 172 Null hypothesis. In hypothesis testing, the hypothesis we wish to falsify on the basis of the data. The null hypothesis is typically that something is not present, that there is no effect, or that there is no difference between treatment and control. 173 Observational Study. Controlled experiment. 174 Odds. The odds in favor of an event is the ratio of the probability that the event occurs to the probability that the event does not occur. For example, suppose an experiment can result in any of n possible outcomes, all equally likely, and that k of the outcomes result in a “win” and n−k result in a “loss.” Then the chance of winning is k/n; the chance of not winning is (n−k)/n; and the odds in favor of winning are (k/n)/((n−k)/n) = k/(n−k), which is the number of favorable outcomes divided by the number of unfavorable outcomes. Note that odds are not synonymous with probability, but the two can be converted back and forth. If the odds in favor of an event are q, then the probability of the event is q/(1&#43;q). If the probability of an event is p, the odds in favor of the event are p/(1−p) and the odds against the event are (1−p)/p. 175 One-sided Test. C.f. two-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against either the alternative hypothesis that μ &lt; μ0 or the alternative μ &gt; μ0 (but not both). For example, a significance level 5%, one-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero, would reject the null hypothesis for values of 176 or, /, Disjunction, Logical Disjunction, ∨ An operation on two logical propositions. If p and q are two propositions, (p / q) is a proposition that is true if p is true or if q is true (or both); otherwise, it is false. That is, (p / q) is true unless both p and q are false. The operation / is sometimes represented by the symbol ∨ and sometimes by the word or. C.f. exclusive disjunction, XOR. 177 Ordinal Variable. A variable whose possible values have a natural order, such as {short, medium, long}, {cold, warm, hot}, or {0, 1, 2, 3, …}. In contrast, a variable whose possible values are {straight, curly} or {Arizona, California, Montana, New York} would not naturally be ordinal. Arithmetic with the possible values of an ordinal variable does not necessarily make sense, but it does make sense to say that one possible value is larger than another. 178 Outcome Space. The outcome space is the set of all possible outcomes of a given random experiment. The outcome space is often denoted by the capital letter S. 179 Outlier. An outlier is an observation that is many SD’s from the mean. It is sometimes tempting to discard outliers, but this is imprudent unless the cause of the outlier can be identified, and the outlier is determined to be spurious. Otherwise, discarding outliers can cause one to underestimate the true variability of the measurement process. 180 P-value. Suppose we have a family of hypothesis tests of a null hypothesis that let us test the hypothesis at any significance level p between 0 and 100% we choose. The P value of the null hypothesis given the data is the smallest significance level p for which any of the tests would have rejected the null hypothesis. For example, let X be a test statistic, and for p between 0 and 100%, let xp be the smallest number such that, under the null hypothesis, P( X ≤ x ) ≥ p. Then for any p between 0 and 100%, the rule reject the null hypothesis if X &lt; xp tests the null hypothesis at significance level p. If we observed X = x, the P-value of the null hypothesis given the data would be the smallest p such that x &lt; xp. 181 Parameter. A numerical property of a population, such as its mean. 182 Partition. A partition of an event A is a collection of events {A1, A2, A3, … } such that the events in the collection are disjoint, and their union is A. That is, AjAk = {} unless j = k, and A = A1 ∪ A2 ∪ A3 ∪ … . If the event A is not specified, it is assumed to be the entire outcome space S. 183 Payoff Matrix. A way of representing what each player in a game wins or loses, as a function of his and his opponent’s strategies. 184 Percentile. The pth percentile of a list is the smallest number such that at least p% of the numbers in the list are no larger than it. The pth percentile of a random variable is the smallest number such that the chance that the random variable is no larger than it is at least p%. C.f. quantile. 185 Permutation. A permutation of a set is an arrangement of the elements of the set in some order. If the set has n things in it, there are n! different orderings of its elements. For the first element in an ordering, there are n possible choices, for the second, there remain n−1 possible choices, for the third, there are n−2, etc., and for the nth element of the ordering, there is a single choice remaining. By the fundamental rule of counting, the total number of sequences is thus n×(n−1)×(n−2)×…×1. Similarly, the number of orderings of length k one can form from n≥k things is n×(n−1)×(n−2)×…×(n−k&#43;1) = n!/(n−k)!. This is denoted nPk, the number of permutations of n things taken k at a time. C.f. combinations. 186 Placebo. A “dummy” treatment that has no pharmacological effect; e.g., a sugar pill. 187 Placebo effect. The belief or knowledge that one is being treated can itself have an effect that confounds with the real effect of the treatment. Subjects given a placebo as a pain-killer report statistically significant reductions in pain in randomized experiments that compare them with subjects who receive no treatment at all. This very real psychological effect of a placebo, which has no direct biochemical effect, is called the placebo effect. Administering a placebo to the control group is thus important in experiments with human subjects; this is the essence of a blind experiment. 188 Point of Averages. In a scatterplot, the point whose coordinates are the arithmetic means of the corresponding variables. For example, if the variable X is plotted on the horizontal axis and the variable Y is plotted on the vertical axis, the point of averages has coordinates (mean of X, mean of Y). 189 Poisson Distribution. The Poisson distribution is a discrete probability distribution that depends on one parameter, m. If X is a random variable with the Poisson distribution with parameter m, then the probability that X = k is E−m × mk/k!, k = 0, 1, 2, … , where E is the base of the natural logarithm and ! is the factorial function. For all other values of k, the probability is zero. The expected value the Poisson distribution with parameter m is m, and the standard error of the Poisson distribution with parameter m is m½. 190 Population. A collection of units being studied. Units can be people, places, objects, epochs, drugs, procedures, or many other things. Much of statistics is concerned with estimating numerical properties (parameters) of an entire population from a random sample of units from the population. 191 Population Mean. The mean of the numbers in a numerical population. For example, the population mean of a box of numbered tickets is the mean of the list comprised of all the numbers on all the tickets. The population mean is a parameter. C.f. sample mean. 192 Population Percentage. The percentage of units in a population that possess a specified property. For example, the percentage of a given collection of registered voters who are registered as Republicans. If each unit that possesses the property is labeled with “1,” and each unit that does not possess the property is labeled with “0,” the population percentage is the same as the mean of that list of zeros and ones; that is, the population percentage is the population mean for a population of zeros and ones. The population percentage is a parameter. C.f. sample percentage. 193 Population Standard Deviation. The standard deviation of the values of a variable for a population. This is a parameter, not a statistic. C.f. sample standard deviation. 194 Post hoc ergo propter hoc. “After this, therefore because of this.” A fallacy of logic known since classical times: inferring a causal relation from correlation. Don’t do this at home! 195 Power. Refers to an hypothesis test. The power of a test against a specific alternative hypothesis is the chance that the test correctly rejects the null hypothesis when the alternative hypothesis is true. 196 Premise, logical premise. A premise is a proposition that is assumed to be true as part of a logical argument. 197 Prima facie. Latin for “at first glance.” “On the face of it.” Prima facie evidence for something is information that at first glance supports the conclusion. On closer examination, that might not be true; there could be another explanation for the evidence. 198 Principle of insufficient reason (Laplace) Laplace’s principle of insufficient reason says that if there is no reason to believe that the possible outcomes of an experiment are not equally likely, one should assume that the outcomes are equally likely. This is an example of a fallacy called appeal to ignorance. 199 Probability. The probability of an event is a number between zero and 100%. The meaning (interpretation) of probability is the subject of theories of probability, which differ in their interpretations. However, any rule for assigning probabilities to events has to satisfy the axioms of probability. 200 Probability density function. The chance that a continuous random variable is in any range of values can be calculated as the area under a curve over that range of values. The curve is the probability density function of the random variable. That is, if X is a continuous random variable, there is a function f(x) such that for every pair of numbers a≤b, P(a≤ X ≤b) = (area under f between a and b); f is the probability density function of X. For example, the probability density function of a random variable with a standard normal distribution is the normal curve. Only continuous random variables have probability density functions. 201 Probability Distribution. The probability distribution of a random variable specifies the chance that the variable takes a value in any subset of the real numbers. (The subsets have to satisfy some technical conditions that are not important for this course.) The probability distribution of a random variable is completely characterized by the cumulative probability distribution function; the terms sometimes are used synonymously. The probability distribution of a discrete random variable can be characterized by the chance that the random variable takes each of its possible values. For example, the probability distribution of the total number of spots S showing on the roll of two fair dice can be written as a table: The probability distribution of a continuous random variable can be characterized by its probability density function. 202 Probability Histogram. A probability histogram for a random variable is analogous to a histogram of data, but instead of plotting the area of the bins proportional to the relative frequency of observations in the class interval, one plots the area of the bins proportional to the probability that the random variable is in the class interval. 203 Probability Sample. A sample drawn from a population using a random mechanism so that every element of the population has a known chance of ending up in the sample. 204 Probability, Theories of. A theory of probability is a way of assigning meaning to probability statements such as “the chance that a thumbtack lands point-up is 2/3.” That is, a theory of probability connects the mathematics of probability, which is the set of consequences of the axioms of probability, with the real world of observation and experiment. There are several common theories of probability. According to the frequency theory of probability, the probability of an event is the limit of the percentage of times that the event occurs in repeated, independent trials under essentially the same circumstances. According to the subjective theory of probability, a probability is a number that measures how strongly we believe an event will occur. The number is on a scale of 0% to 100%, with 0% indicating that we are completely sure it won’t occur, and 100% indicating that we are completely sure that it will occur. According to the theory of equally likely outcomes, if an experiment has n possible outcomes, and (for example, by symmetry) there is no reason that any of the n possible outcomes should occur preferentially to any of the others, then the chance of each outcome is 100%/n. Each of these theories has its limitations, its proponents, and its detractors. 205 Proposition, logical proposition. A logical proposition is a statement that can be either true or false. For example, “the sun is shining in Berkeley right now” is a proposition. See also &amp;, ↔, →, /, XOR, converse, contrapositive and logical argument. 206 Prosecutor’s Fallacy. The prosecutor’s fallacy consists of confusing two conditional probabilities: P(A/B) and P(B/A). For instance, P(A/B) could be the chance of observing the evidence if the accused is guilty, while P(B/A) is the chance that the accused is guilty given the evidence. The latter might not make sense at all, but even when it does, the two numbers need not be equal. This fallacy is related to a common misinterpretation of P-values. 207 Qualitative Variable. A qualitative variable is one whose values are adjectives, such as colors, genders, nationalities, etc. C.f. quantitative variable and categorical variable. 208 Quantile. The qth quantile of a list (0 &lt; q ≤ 1) is the smallest number such that the fraction q or more of the elements of the list are less than or equal to it. I.e., if the list contains n numbers, the qth quantile, is the smallest number Q such that at least n×q elements of the list are less than or equal to Q. 209 Quantitative Variable. A variable that takes numerical values for which arithmetic makes sense, for example, counts, temperatures, weights, amounts of money, etc. For some variables that take numerical values, arithmetic with those values does not make sense; such variables are not quantitative. For example, adding and subtracting social security numbers does not make sense. Quantitative variables typically have units of measurement, such as inches, people, or pounds. 210 Quartiles. There are three quartiles. The first or lower quartile (LQ) of a list is a number (not necessarily a number in the list) such that at least 1/4 of the numbers in the list are no larger than it, and at least 3/4 of the numbers in the list are no smaller than it. The second quartile is the median. The third or upper quartile (UQ) is a number such that at least 3/4 of the entries in the list are no larger than it, and at least 1/4 of the numbers in the list are no smaller than it. To find the quartiles, first sort the list into increasing order. Find the smallest integer that is at least as big as the number of entries in the list divided by four. Call that integer k. The kth element of the sorted list is the lower quartile. Find the smallest integer that is at least as big as the number of entries in the list divided by two. Call that integer l. The lth element of the sorted list is the median. Find the smallest integer that is at least as large as the number of entries in the list times 3/4. Call that integer m. The mth element of the sorted list is the upper quartile. 211 Quota Sample. A quota sample is a sample picked to match the population with respect to some summary characteristics. It is not a random sample. For example, in an opinion poll, one might select a sample so that the proportions of various ethnicities in the sample match the proportions of ethnicities in the overall population from which the sample is drawn. Matching on summary statistics does not guarantee that the sample comes close to matching the population with respect to the quantity of interest. As a result, quota samples are typically biased, and the size of the bias is generally impossible to determine unless the result can be compared with a known result for the whole population or for a random sample. Moreover, with a quota sample, it is impossible to quantify how representative of the population a quota sample is likely to be—quota sampling does not allow one to quantify the likely size of sampling error. Quota samples are to be avoided, and results based on quota samples are to be viewed with suspicion. See also convenience sample. 212 Random Error. All measurements are subject to error, which can often be broken down into two components: a bias or systematic error, which affects all measurements the same way; and a random error, which is in general different each time a measurement is made, and behaves like a number drawn with replacement from a box of numbered tickets whose average is zero. 213 Random Event. See random experiment. 214 Random Experiment. An experiment or trial whose outcome is not perfectly predictable, but for which the long-run relative frequency of outcomes of different types in repeated trials is predictable. Note that “random” is different from “haphazard,” which does not necessarily imply long-term regularity. 215 Random Sample. A random sample is a sample whose members are chosen at random from a given population in such a way that the chance of obtaining any particular sample can be computed. The number of units in the sample is called the sample size, often denoted n. The number of units in the population often is denoted N. Random samples can be drawn with or without replacing objects between draws; that is, drawing all n objects in the sample at once (a random sample without replacement), or drawing the objects one at a time, replacing them in the population between draws (a random sample with replacement). In a random sample with replacement, any given member of the population can occur in the sample more than once. In a random sample without replacement, any given member of the population can be in the sample at most once. A random sample without replacement in which every subset of n of the N units in the population is equally likely is also called a simple random sample. The term random sample with replacement denotes a random sample drawn in such a way that every multiset of n units in the population is equally likely. See also probability sample. 216 Random Variable. A random variable is an assignment of numbers to possible outcomes of a random experiment. For example, consider tossing three coins. The number of heads showing when the coins land is a random variable: it assigns the number 0 to the outcome {T, T, T}, the number 1 to the outcome {T, T, H}, the number 2 to the outcome {T, H, H}, and the number 3 to the outcome {H, H, H}. 217 Randomized Controlled Experiment. An experiment in which chance is deliberately introduced in assigning subjects to the treatment and control groups. For example, we could write an identifying number for each subject on a slip of paper, stir up the slips of paper, and draw slips without replacement until we have drawn half of them. The subjects identified on the slips drawn could then be assigned to treatment, and the rest to control. Randomizing the assignment tends to decrease confounding of the treatment effect with other factors, by making the treatment and control groups roughly comparable in all respects but the treatment. 218 Range. The range of a set of numbers is the largest value in the set minus the smallest value in the set. Note that as a statistical term, the range is a single number, not a range of numbers. 219 Real number. Loosely speaking, the real numbers are all numbers that can be represented as fractions (rational numbers), whether proper or improper—and all numbers in between the rational numbers. That is, the real numbers comprise the rational numbers and all limits of Cauchy sequences of rational numbers, where the Cauchy sequence is with respect to the absolute value metric. (More formally, the real numbers are the completion of the set of rational numbers in the topology induced by the absolute value function.) The real numbers contain all integers, all fractions, and all irrational (and transcendental) numbers, such as π, e, and 2½. There are uncountably many real numbers between 0 and 1; in contrast, there are only countably many rational numbers between 0 and 1. 220 Regression, Linear Regression. Linear regression fits a line to a scatterplot in such a way as to minimize the sum of the squares of the residuals. The resulting regression line, together with the standard deviations of the two variables or their correlation coefficient, can be a reasonable summary of a scatterplot if the scatterplot is roughly football-shaped. In other cases, it is a poor summary. If we are regressing the variable Y on the variable X, and if Y is plotted on the vertical axis and X is plotted on the horizontal axis, the regression line passes through the point of averages, and has slope equal to the correlation coefficient times the SD of Y divided by the SD of X. This page shows a scatterplot, with a button to plot the regression line. 221 Regression Fallacy. The regression fallacy is to attribute the regression effect to an external cause. 222 Regression Toward the Mean, Regression Effect. Suppose one measures two variables for each member of a group of individuals, and that the correlation coefficient of the variables is positive (negative). If the value of the first variable for that individual is above average, the value of the second variable for that individual is likely to be above (below) average, but by fewer standard deviations than the first variable is. That is, the second observation is likely to be closer to the mean in standard units. For example, suppose one measures the heights of fathers and sons. Each individual is a (father, son) pair; the two variables measured are the height of the father and the height of the son. These two variables will tend to have a positive correlation coefficient: fathers who are taller than average tend to have sons who are taller than average. Consider a (father, son) pair chosen at random from this group. Suppose the father’s height is 3SD above the average of all the fathers’ heights. (The SD is the standard deviation of the fathers’ heights.) Then the son’s height is also likely to be above the average of the sons’ heights, but by fewer than 3SD (here the SD is the standard deviation of the sons’ heights). 223 Rejection region. In an hypothesis test using a test statistic, the rejection region is the set of values of the test statistic for which we reject the null hypothesis. 224 Residual. The difference between a datum and the value predicted for it by a model. In linear regression of a variable plotted on the vertical axis onto a variable plotted on the horizontal axis, a residual is the “vertical” distance from a datum to the line. Residuals can be positive (if the datum is above the line) or negative (if the datum is below the line). Plots of residuals can reveal computational errors in linear regression, as well as conditions under which linear regression is inappropriate, such as nonlinearity and heteroscedasticity. If linear regression is performed properly, the sum of the residuals from the regression line must be zero; otherwise, there is a computational error somewhere. 225 Residual Plot. A residual plot for a regression is a plot of the residuals from the regression against the explanatory variable. 226 Resistant. A statistic is said to be resistant if corrupting a datum cannot change the statistic much. The mean is not resistant; the median is. See also breakdown point. 227 Root-mean-square (RMS). The RMS of a list is the square-root of the mean of the squares of the elements in the list. It is a measure of the average “size” of the elements of the list. To compute the RMS of a list, you square all the entries, average the numbers you get, and take the square-root of that average. 228 Root-mean-square error (RMSE). The RMSE of an an estimator of a parameter is the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then RMSE(X) = ( E( (X−t)2 ) )½. The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator. See also mean squared error. 229 rms Error of Regression The rms error of regression is the rms of the vertical residuals from the regression line. For regressing Y on X, the rms error of regression is equal to (1 − r2)½×SDY, where r is the correlation coefficient between X and Y and SDY is the standard deviation of the values of Y. 230 Sample. A sample is a collection of units from a population. See also random sample. 231 Sample Mean. The arithmetic mean of a random sample from a population. It is a statistic commonly used to estimate the population mean. Suppose there are n data, {x1, x2, … , xn}. The sample mean is (x1 &#43; x2 &#43; … &#43; xn)/n. The expected value of the sample mean is the population mean. For sampling with replacement, the SE of the sample mean is the population standard deviation, divided by the square-root of the sample size. For sampling without replacement, the SE of the sample mean is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample mean for sampling with replacement, with N the size of the population and n the size of the sample. 232 Sample Percentage. The percentage of a random sample with a certain property, such as the percentage of voters registered as Democrats in a simple random sample of voters. The sample mean is a statistic commonly used to estimate the population percentage. The expected value of the sample percentage from a simple random sample or a random sample with replacement is the population percentage. The SE of the sample percentage for sampling with replacement is (p(1−p)/n )½, where p is the population percentage and n is the sample size. The SE of the sample percentage for sampling without replacement is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample percentage for sampling with replacement, with N the size of the population and n the size of the sample. The SE of the sample percentage is often estimated by the bootstrap. 233 Sample Size. The number of elements in a sample from a population. 234 Sound argument. A logical argument is sound if it is logically valid and its premises are in fact true. An argument can be logically valid and yet not sound—if its premises are false. 235 Sample Standard Deviation, S. The sample standard deviation S is an estimator of the standard deviation of a population based on a random sample from the population. The sample standard deviation is a statistic that measures how “spread out” the sample is around the sample mean. It is quite similar to the standard deviation of the sample, but instead of averaging the squared deviations (to get the rms of the deviations of the data from the sample mean) it divides the sum of the squared deviations by (number of data − 1) before taking the square-root. Suppose there are n data, {x1, x2, … , xn}, with mean M = (x1 &#43; x2 &#43; … &#43; xn)/n. Then s = ( ((x1 − M)2 &#43; (x2 − M)2 &#43; … &#43; (xn − M)2)/(n−1) )½ The square of the sample standard deviation, S2 (the sample variance) is an unbiased estimator of the square of the SD of the population (the variance of the population). 236 Sample Sum. The sum of a random sample from a population. The expected value of the sample sum is the sample size times the population mean. For sampling with replacement, the SE of the sample sum is the population standard deviation, times the square-root of the sample size. For sampling without replacement, the SE of the sample sum is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample sum for sampling with replacement, with N the size of the population and n the size of the sample. 237 Sample Survey. A survey based on the responses of a sample of individuals, rather than the entire population. 238 Sample Variance The sample variance is the square of the sample standard deviation S. It is an unbiased estimator of the square of the population standard deviation, which is also called the variance of the population. 239 Sampling distribution. The sampling distribution of an estimator is the probability distribution of the estimator when it is applied to random samples. The tool on this page allows you to explore empirically the sampling distribution of the sample mean and the sample percentage of random draws with or without replacement draws from a box of numbered tickets. 240 Sampling error. In estimating from a random sample, the difference between the estimator and the parameter can be written as the sum of two components: bias and sampling error. The bias is the average error of the estimator over all possible samples. The bias is not random. Sampling error is the component of error that varies from sample to sample. The sampling error is random: it comes from “the luck of the draw” in which units happen to be in the sample. It is the chance variation of the estimator. The average of the sampling error over all possible samples (the expected value of the sampling error) is zero. The standard error of the estimator is a measure of the typical size of the sampling error. 241 Sampling unit. A sample from a population can be drawn one unit at a time, or more than one unit at a time (one can sample clusters of units). The fundamental unit of the sample is called the sampling unit. It need not be a unit of the population. 242 Scatterplot. A scatterplot is a way to visualize bivariate data. A scatterplot is a plot of pairs of measurements on a collection of “individuals” (which need not be people). For example, suppose we record the heights and weights of a group of 100 people. The scatterplot of those data would be 100 points. Each point represents one person’s height and weight. In a scatterplot of weight against height, the x-coordinate of each point would be height of one person, the y-coordinate of that point would be the weight of the same person. In a scatterplot of height against weight, the x-coordinates would be the weights and the y-coordinates would be the heights. 243 Scientific Method. The scientific method…. 244 SD line. For a scatterplot, a line that goes through the point of averages, with slope equal to the ratio of the standard deviations of the two plotted variables. If the variable plotted on the horizontal axis is called X and the variable plotted on the vertical axis is called Y, the slope of the SD line is the SD of Y, divided by the SD of X. 245 Secular Trend. A linear association (trend) with time. 246 Selection Bias. A systematic tendency for a sampling procedure to include and/or exclude units of a certain type. For example, in a quota sample, unconscious prejudices or predilections on the part of the interviewer can result in selection bias. Selection bias is a potential problem whenever a human has latitude in selecting individual units for the sample; it tends to be eliminated by probability sampling schemes in which the interviewer is told exactly whom to contact (with no room for individual choice). 247 Self-Selection. Self-selection occurs when individuals decide for themselves whether they are in the control group or the treatment group. Self-selection is quite common in studies of human behavior. For example, studies of the effect of smoking on human health involve self-selection: individuals choose for themselves whether or not to smoke. Self-selection precludes an experiment; it results in an observational study. When there is self-selection, one must be wary of possible confounding from factors that influence individuals’ decisions to belong to the treatment group. 248 Set. A set is a collection of things (called elements), without regard to their order. An item is either in a set (it is an element of the set), or it is not. It cannot be in the set more than once. Two sets are equal if they contain electly the same elements. For instance, the set {1, 2, 3, 4} is equal to the set {1, 4, 3, 2}, but not to the set {0, 1, 2, 3}. As another example, the set {1, 2, 2} is equal to the set {1, 2}: they have the same two (distinct) elements, 1 and 2. 249 Significance, Significance level, Statistical significance. The significance level of an hypothesis test is the chance that the test erroneously rejects the null hypothesis when the null hypothesis is true. 250 Simple Random Sample. A simple random sample of n units from a population is a random sample drawn by a procedure that is equally likely to give every collection of n units from the population; that is, the probability that the sample will consist of any given subset of n of the N units in the population is 1/NCn. Simple random sampling is sampling at random without replacement (without replacing the units between draws). A simple random sample of size n from a population of N ≥ n units can be constructed by assigning a random number between zero and one to each unit in the population, then taking those units that were assigned the n largest random numbers to be the sample. 251 Simpson’s Paradox. What is true for the parts is not necessarily true for the whole. See also confounding. 252 Skewed Distribution. A distribution that is not symmetrical. 253 Spread, Measure of. See also inter-quartile range, range, and standard deviation. 254 Square-Root Law. The Square-Root Law says that the standard error (SE) of the sample sum of n random draws with replacement from a box of tickets with numbers on them is SE(sample sum) = n½×SD(box), and the standard error of the sample mean of n random draws with replacement from a box of tickets is SE(sample mean) = n−½×SD(box), where SD(box) is the standard deviation of the list of the numbers on all the tickets in the box (including repeated values). 255 Standard Deviation (SD). The standard deviation of a set of numbers is the rms of the set of deviations between each element of the set and the mean of the set. See also sample standard deviation. 256 Standard Error (SE). The Standard Error of a random variable is a measure of how far it is likely to be from its expected value; that is, its scatter in repeated experiments. The SE of a random variable X is defined to be SE(X) = [E( (X − E(X))2 )] ½. That is, the standard error is the square-root of the expected squared difference between the random variable and its expected value. The SE of a random variable is analogous to the SD of a list. 257 Standard Normal Curve. See normal curve. 258 Standard Units. A variable (a set of data) is said to be in standard units if its mean is zero and its standard deviation is one. You transform a set of data into standard units by subtracting the mean from each element of the list, and dividing the results by the standard deviation. A random variable is said to be in standard units if its expected value is zero and its standard error is one. You transform a random variable to standard units by subtracting its expected value then dividing by its standard error. 259 Standardize. To transform into standard units. 260 Statistic. A number that can be computed from data, involving no unknown parameters. As a function of a random sample, a statistic is a random variable. Statistics are used to estimate parameters, and to test hypotheses. 261 Stratified Sample. In a stratified sample, subsets of sampling units are selected separately from different strata, rather than from the frame as a whole. 262 Stratified sampling The act of drawing a stratified sample. 263 Stratum In random sampling, sometimes the sample is drawn separately from different disjoint subsets of the population. Each such subset is called a stratum. (The plural of stratum is strata.) Samples drawn in such a way are called stratified samples. Estimators based on stratified random samples can have smaller sampling errors than estimators computed from simple random samples of the same size, if the average variability of the variable of interest within strata is smaller than it is across the entire population; that is, if stratum membership is associated with the variable. For example, to determine average home prices in the U.S., it would be advantageous to stratify on geography, because average home prices vary enormously with location. We might divide the country into states, then divide each state into urban, suburban, and rural areas; then draw random samples separately from each such division. 264 Studentized score The observed value of a statistic, minus the expected value of the statistic, divided by the estimated standard error of the statistic. 265 Student’s t curve. Student’s t curve is a family of curves indexed by a parameter called the degrees of freedom, which can take the values 1, 2, … Student’s t curve is used to approximate some probability histograms. Consider a population of numbers that are nearly normally distributed and have population mean is μ. Consider drawing a random sample of size n with replacement from the population, and computing the sample mean M and the sample standard deviation S. Define the random variable T = (M − μ)/(S/n½). If the sample size n is large, the probability histogram of T can be approximated accurately by the normal curve. However, for small and intermediate values of n, Student’s t curve with n − 1 degrees of freedom gives a better approximation. That is, P(a &lt; T &lt; b) is approximately the area under Student’s T curve with n − 1 degrees of freedom, from a to b. Student’s t curve can be used to test hypotheses about the population mean and construct confidence intervals for the population mean, when the population distribution is known to be nearly normally distributed. This page contains a tool that shows Student’s t curve and lets you find the area under parts of the curve. 266 Subject, Experimental Subject. A member of the control group or the treatment group. 267 Subset. A subset of a given set is a collection of things that belong to the original set. Every element of the subset must belong to the original set, but not every element of the original set need be in a subset (otherwise, a subset would always be identical to the set it came from). 268 Survey. See sample survey. 269 Symmetric Distribution. The probability distribution of a random variable X is symmetric if there is a number a such that the chance that X≥a&#43;b is the same as the chance that X≤a−b for every value of b. A list of numbers has a symmetric distribution if there is a number a such that the fraction of numbers in the list that are greater than or equal to a&#43;b is the same as the fraction of numbers in the list that are less than or equal to a−b, for every value of b. In either case, the histogram or the probability histogram will be symmetrical about a vertical line drawn at x=a. 270 Systematic error. An error that affects all the measurements similarly. For example, if a ruler is too short, everything measured with it will appear to be longer than it really is (ignoring random error). If your watch runs fast, every time interval you measure with it will appear to be longer than it really is (again, ignoring random error). Systematic errors do not tend to average out. 271 Systematic sample. A systematic sample from a frame of units is one drawn by listing the units and selecting every kth element of the list. For example, if there are N units in the frame, and we want a sample of size N/10, we would take every tenth unit: the first unit, the eleventh unit, the 21st unit, etc. Systematic samples are not random samples, but they often behave essentially as if they were random, if the order in which the units appears in the list is haphazard. Systematic samples are a special case of cluster samples. 272 Systematic random sample. A systematic sample starting at a random point in the listing of units in the of frame, instead of starting at the first unit. Systematic random sampling is better than systematic sampling, but typically not as good as simple random sampling. 273 t test. An hypothesis test based on approximating the probability histogram of the test statistic by Student’s t curve. t tests usually are used to test hypotheses about the mean of a population when the sample size is intermediate and the distribution of the population is known to be nearly normal. 274 Test Statistic. A statistic used to test hypotheses. An hypothesis test can be constructed by deciding to reject the null hypothesis when the value of the test statistic is in some range or collection of ranges. To get a test with a specified significance level, the chance when the null hypothesis is true that the test statistic falls in the range where the hypothesis would be rejected must be at most the specified significance level. The Z statistic is a common test statistic. 275 Transformation. Transformations turn lists into other lists, or variables into other variables. For example, to transform a list of temperatures in degrees Celsius into the corresponding list of temperatures in degrees Fahrenheit, you multiply each element by 9/5, and add 32 to each product. This is an example of an affine transformation: multiply by something and add something (y = ax &#43; b is the general affine transformation of x; it’s the familiar equation of a straight line). In a linear transformation, you only multiply by something (y = ax). Affine transformations are used to put variables in standard units. In that case, you subtract the mean and divide the results by the SD. This is equivalent to multiplying by the reciprocal of the SD and adding the negative of the mean, divided by the SD, so it is an affine transformation. Affine transformations with positive multiplicative constants have a simple effect on the mean, median, mode, quartiles, and other percentiles: the new value of any of these is the old one, transformed using exactly the same formula. When the multiplicative constant is negative, the mean, median, mode, are still transformed by the same rule, but quartiles and percentiles are reversed: the qth quantile of the transformed distribution is the transformed value of the 1−qth quantile of the original distribution (ignoring the effect of data spacing). The effect of an affine transformation on the SD, range, and IQR, is to make the new value the old value times the absolute value of the number you multiplied the first list by: what you added does not affect them. 276 Treatment. The substance or procedure studied in an experiment or observational study. At issue is whether the treatment has an effect on the outcome or variable of interest. 277 Treatment Effect. The effect of the treatment on the variable of interest. Establishing whether the treatment has an effect is the point of an experiment. 278 Treatment group. The individuals who receive the treatment, as opposed to those in the control group, who do not. 279 Tuple, n-tuple. A tuple is an ordered collection of things. Two tuples are equal if they contain the same things, in the same order. For instance, the tuple (1, 2, 3) is equal to the tuple (1, 2, 3) but not equal to the tuple (1, 3, 2). Tuples can contain repeated elements. For instance, the tuple (1, 2, 2) is not equal to the tuple (1, 2), nor to the tuple (2, 2, 1). An n-tuple, where n is an integer, is a tuple with n positions. For example, (1, 2) is a 2-tuple (aka ordered pair) and (7, 3, 2, 2, 2, 1) is a 6-tuple. 280 Two-sided Hypothesis test. C.f. one-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against the alternative hypothesis that either μ &lt; μ0 or μ &gt; μ0 (the alternative hypothesis contains values on both sides of the null value). For example, a significance level 5%, two-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero would reject the null hypothesis for values of $$ /z/ = \left / \frac{\mbox{sample mean}}{\mbox{SE}} \right / &gt; 1.96.$$ 281 Type I and Type II errors. These refer to hypothesis testing. A Type I error occurs when the null hypothesis is rejected erroneously when it is in fact true. A Type II error occurs if the null hypothesis is not rejected when it is in fact false. See also significance level and power. 282 Unbiased. Not biased; having zero bias. 283 Uncontrolled Experiment. An experiment in which there is no control group; i.e., in which the method of comparison is not used: the experimenter decides who gets the treatment, but the outcome of the treated group is not compared with the outcome of a control group that does not receive treatment. 284 Uncorrelated. A set of bivariate data is uncorrelated if its correlation coefficient is zero. Two random variables are uncorrelated if the expected value of their product equals the product of their expected values. If two random variables are independent, they are uncorrelated. (The converse is not true in general.) 285 Uncountable. A set is uncountable if it is not countable, that is, if its elements cannot be put in one-to-one correspondence with the positive integers. 286 Unimodal. Having exactly one mode. 287 Union. The union of two or more sets is the set of objects contained by at least one of the sets. The union of the events A and B is denoted “A&#43;B”, “A or B”, and “A∪B”. C.f. intersection. 288 Unit. A member of a population. 289 Univariate. Having or having to do with a single variable. Some univariate techniques and statistics include the histogram, IQR, mean, median, percentiles, quantiles, and SD. C.f. bivariate. 290 Upper Quartile (UQ). See quartiles. 291 Valid (logical) argument. A valid logical argument is one in which the truth of the premises indeed guarantees the truth of the conclusion. For example, the following logical argument is valdraft: false id: If the forecast calls for rain, I will not wear sandals. The forecast calls for rain. Therefore, I will not wear sandals. This argument has two premises which, together, guarantee the truth of the conclusion. An argument can be logically valid even if its premises are false. See also invalid argument and sound argument. 292 Variable. A numerical value or a characteristic that can differ from individual to individual. See also categorical variable, qualitative variable, quantitative variable, discrete variable, continuous variable, and random variable. 293 Variance, population variance The variance of a list is the square of the standard deviation of the list, that is, the average of the squares of the deviations of the numbers in the list from their mean. The variance of a random variable X, Var(X), is the expected value of the squared difference between the variable and its expected value: Var(X) = E((X − E(X))2). The variance of a random variable is the square of the standard error (SE) of the variable. 294 Venn Diagram. A pictorial way of showing the relations among sets or events. The universal set or outcome space is usually drawn as a rectangle; sets are regions within the rectangle. The overlap of the regions corresponds to the intersection of the sets. If the regions do not overlap, the sets are disjoint. The part of the rectangle included in one or more of the regions corresponds to the union of the sets. This page contains a tool that illustrates Venn diagrams; the tool represents the probability of an event by the area of the event. 295 XOR, exclusive disjunction. XOR is an operation on two logical propositions. If p and q are two propositions, (p XOR q) is a proposition that is true if either p is true or if q is true, but not both. (p XOR q) is logically equivalent to ((p 296 z-score The observed value of the Z statistic. 297 Z statistic A Z statistic is a test statistic whose distribution under the null hypothesis has expected value zero and can be approximated well by the normal curve. Usually, Z statistics are constructed by standardizing some other statistic. The Z statistic is related to the original statistic by Z = (original − expected value of original)/SE(original). 298 z-test An hypothesis test based on approximating thehttps://dasarpai.com/300-important-statistical-terms/ probability histogram of the Z statistic under the null hypothesis by the normal curve. These definitions are taken from - https://www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm">



<link rel="stylesheet" href="/site/css/prism.css"/>

<link href="/site/scss/main.css" rel="stylesheet">

<link rel="stylesheet" type="text/css" href=http://localhost:1313/site/css/asciinema-player.css />
<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

  </head>
  <body class="td-page">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-light  nav-shadow flex-column flex-md-row td-navbar">

	<a id="agones-top"  class="navbar-brand" href="/site/">
		<svg xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" viewBox="0 0 276 276" height="30" width="30" id="svg2"><defs id="defs6"><clipPath id="clipPath18" clipPathUnits="userSpaceOnUse"><path id="path16" d="M0 8e2H8e2V0H0z"/></clipPath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-398.3522,928.28029)" id="g10"><g transform="translate(2.5702576,82.614887)" id="g12"><circle transform="scale(1,-1)" r="102.69205" cy="-510.09534" cx="399.71484" id="path930" style="opacity:1;vector-effect:none;fill:#fff;fill-opacity:1;stroke:none;stroke-width:.65861601;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1"/><g id="g40" transform="translate(239.9974,355.2515)"/><g transform="translate(4.931459e-6,39.355242)" id="g917"><g transform="translate(386.7049,451.9248)" id="g44"><path id="path46" style="fill:#2d70de;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c.087-2.62-1.634-4.953-4.163-5.646-7.609-2.083-14.615-5.497-21.089-10.181-5.102-3.691-10.224-7.371-15.52-10.769-3.718-2.385-7.711-4.257-12.438-3.601-6.255.868-10.629 4.828-12.313 11.575-.619 2.478-1.169 4.997-1.457 7.53-.47 4.135-.699 8.297-1.031 12.448.32 18.264 5.042 35.123 15.47 50.223 6.695 9.693 16.067 14.894 27.708 16.085 4.103.419 8.134.365 12.108-.059 3.313-.353 5.413-3.475 5.034-6.785-.039-.337-.059-.682-.059-1.033.0-.2.008-.396.021-.593-.03-1.164-.051-1.823-.487-3.253-.356-1.17-1.37-3.116-4.045-3.504h-10.267c-3.264.0-5.91-3.291-5.91-7.35.0-4.059 2.646-7.35 5.91-7.35H4.303C6.98 37.35 7.996 35.403 8.352 34.232 8.81 32.726 8.809 32.076 8.843 30.787 8.837 30.655 8.834 30.521 8.834 30.387c0-4.059 2.646-7.349 5.911-7.349h3.7c3.264.0 5.911-3.292 5.911-7.35.0-4.06-2.647-7.351-5.911-7.351H5.878c-3.264.0-5.911-3.291-5.911-7.35z"/></g><g transform="translate(467.9637,499.8276)" id="g48"><path id="path50" style="fill:#17252e;fill-opacity:1;fill-rule:nonzero;stroke:none" d="m0 0c-8.346 13.973-20.665 20.377-36.728 20.045-1.862-.038-3.708-.16-5.539-.356-1.637-.175-2.591-2.02-1.739-3.428.736-1.219 1.173-2.732 1.173-4.377.0-4.059-2.646-7.35-5.912-7.35h-17.733c-3.264.0-5.911-3.291-5.911-7.35.0-4.059 2.647-7.35 5.911-7.35h13.628c3.142.0 5.71-3.048 5.899-6.895l.013.015c.082-1.94-.032-2.51.52-4.321.354-1.165 1.359-3.095 4.001-3.498h14.69c3.265.0 5.911-3.292 5.911-7.35.0-4.06-2.646-7.351-5.911-7.351h-23.349c-2.838-.311-3.897-2.33-4.263-3.532-.434-1.426-.456-2.085-.485-3.246.011-.189.019-.379.019-.572.0-.341-.019-.677-.055-1.006-.281-2.535 1.584-4.771 4.057-5.396 8.245-2.084 15.933-5.839 23.112-11.209 5.216-3.901 10.678-7.497 16.219-10.922 2.152-1.331 4.782-2.351 7.279-2.578 8.033-.731 13.657 3.531 15.686 11.437 1.442 5.615 2.093 11.343 2.244 17.134C13.198-31.758 9.121-15.269.0.0"/></g></g></g></g></svg> <span class="text-uppercase fw-bold">Agones</span>
	</a>

	<div class="td-navbar-nav-scroll ms-md-auto" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link active" href="/site/docs/"><span class="active">Documentation</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/blog/"><span>Blog</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				
				
				
				
				<a class="nav-link" href="/site/community/"><span>Community</span></a>
			</li>
			
			<li class="nav-item mr-4 mb-2 mb-lg-0">
				<a class="nav-link" href="https://github.com/googleforgames/agones">GitHub</a>
			</li>
			<li class="nav-item dropdown d-none d-lg-block">
				<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
					Release
				</a>
				<div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
					<a class="dropdown-item" href="https://development.agones.dev">Development</a>
					<a class="dropdown-item" href="https://agones.dev">1.48.0</a>
					<a class="dropdown-item" href="https://1-47-0.agones.dev">1.47.0</a>
					<a class="dropdown-item" href="https://1-46-0.agones.dev">1.46.0</a>
					<a class="dropdown-item" href="https://1-45-0.agones.dev">1.45.0</a>
					<a class="dropdown-item" href="https://1-44-0.agones.dev">1.44.0</a>
					<a class="dropdown-item" href="https://1-43-0.agones.dev">1.43.0</a>
					<a class="dropdown-item" href="https://1-42-0.agones.dev">1.42.0</a>
					<a class="dropdown-item" href="https://1-41-0.agones.dev">1.41.0</a>
					<a class="dropdown-item" href="https://1-40-0.agones.dev">1.40.0</a>
					<a class="dropdown-item" href="https://1-39-0.agones.dev">1.39.0</a>
					<a class="dropdown-item" href="https://1-38-0.agones.dev">1.38.0</a>
					<a class="dropdown-item" href="https://1-37-0.agones.dev">1.37.0</a>
					<a class="dropdown-item" href="https://1-36-0.agones.dev">1.36.0</a>
					<a class="dropdown-item" href="https://1-35-0.agones.dev">1.35.0</a>
					<a class="dropdown-item" href="https://1-34-0.agones.dev">1.34.0</a>
					<a class="dropdown-item" href="https://1-33-0.agones.dev">1.33.0</a>
					<a class="dropdown-item" href="https://1-32-0.agones.dev">1.32.0</a>
					<a class="dropdown-item" href="https://1-31-0.agones.dev">1.31.0</a>
				</div>
			</li>
			
		</ul>
	</div>
	<div class="navbar-nav mx-lg-2 d-none d-lg-block"><div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete="off">
</div></div>
</nav>

    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
            <div id="td-sidebar-menu" class="td-sidebar__inner">
  <form class="td-sidebar__search d-flex align-items-center">
    <div class="td-search">
  <div class="td-search__icon"></div>
  <input id="agones-search" type="search" class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete="off">
</div>
    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ms-3 fas fa-bars" type="button" data-bs-toggle="collapse" data-bs-target="#td-section-nav" aria-controls="td-section-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  <nav class="td-sidebar-nav collapse" id="td-section-nav">
    <ul class="td-sidebar-nav__section pe-md-3 ul-0">
      <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-sitedocs-li">
  <a href="/site/docs/" title="Agones Documentation" class="align-left ps-0 td-sidebar-link td-sidebar-link__section tree-root" id="m-sitedocs"><span class="">Documentation</span></a>
  <ul class="ul-1">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsoverview-li">
  <a href="/site/docs/overview/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsoverview"><span class="">Overview</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsprerequisite-knowledge-li">
  <a href="/site/docs/prerequisite-knowledge/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsprerequisite-knowledge"><span class="">Prerequisite Knowledge</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsinstallation-li">
  <a href="/site/docs/installation/" title="Install and configure Agones on Kubernetes" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsinstallation"><span class="">Installation</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsinstallationcreating-cluster-li">
  <a href="/site/docs/installation/creating-cluster/" title="Create Kubernetes Cluster" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsinstallationcreating-cluster"><span class="">Create Cluster</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationcreating-clustergke-li">
  <a href="/site/docs/installation/creating-cluster/gke/" title="Google Kubernetes Engine" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationcreating-clustergke"><span class="">Google Cloud</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationcreating-clustereks-li">
  <a href="/site/docs/installation/creating-cluster/eks/" title="Amazon Elastic Kubernetes Service" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationcreating-clustereks"><span class="">Amazon Web Services</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationcreating-clusteraks-li">
  <a href="/site/docs/installation/creating-cluster/aks/" title="Azure Kubernetes Service" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationcreating-clusteraks"><span class="">Azure</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationcreating-clusteroke-li">
  <a href="/site/docs/installation/creating-cluster/oke/" title="OCI Kubernetes Engine" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationcreating-clusteroke"><span class="">Oracle Cloud</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationcreating-clusterminikube-li">
  <a href="/site/docs/installation/creating-cluster/minikube/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationcreating-clusterminikube"><span class="">Minikube</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsinstallationinstall-agones-li">
  <a href="/site/docs/installation/install-agones/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsinstallationinstall-agones"><span class="">Install Agones</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationinstall-agonesyaml-li">
  <a href="/site/docs/installation/install-agones/yaml/" title="Install Agones using YAML" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationinstall-agonesyaml"><span class="">YAML</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationinstall-agoneshelm-li">
  <a href="/site/docs/installation/install-agones/helm/" title="Install Agones using Helm" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationinstall-agoneshelm"><span class="">Helm</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsinstallationterraform-li">
  <a href="/site/docs/installation/terraform/" title="Deploy Kubernetes cluster and install Agones using Terraform" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsinstallationterraform"><span class="">Install with Terraform</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationterraformgke-li">
  <a href="/site/docs/installation/terraform/gke/" title="Installing Agones on Google Kubernetes Engine using Terraform" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationterraformgke"><span class="">Google Cloud</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationterraformeks-li">
  <a href="/site/docs/installation/terraform/eks/" title="Installing Agones on AWS Elastic Kubernetes Service using Terraform" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationterraformeks"><span class="">AWS</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationterraformaks-li">
  <a href="/site/docs/installation/terraform/aks/" title="Installing Agones on Azure Kubernetes Service using Terraform" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationterraformaks"><span class="">Azure</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationterraformoke-li">
  <a href="/site/docs/installation/terraform/oke/" title="Installing Agones on OCI Kubernetes Engine using Terraform" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationterraformoke"><span class="">OCI</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationconfirm-li">
  <a href="/site/docs/installation/confirm/" title="Confirming Agones Installation" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationconfirm"><span class="">Confirm Installation</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsinstallationupgrading-li">
  <a href="/site/docs/installation/upgrading/" title="Upgrading Agones and Kubernetes" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsinstallationupgrading"><span class="">Upgrading</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsgetting-started-li">
  <a href="/site/docs/getting-started/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsgetting-started"><span class="">Getting Started</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsgetting-startedcreate-gameserver-li">
  <a href="/site/docs/getting-started/create-gameserver/" title="Quickstart: Create a Game Server" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsgetting-startedcreate-gameserver"><span class="">Create a Game Server</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsgetting-startedcreate-fleet-li">
  <a href="/site/docs/getting-started/create-fleet/" title="Quickstart: Create a Game Server Fleet" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsgetting-startedcreate-fleet"><span class="">Create a Fleet</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsgetting-startedcreate-fleetautoscaler-li">
  <a href="/site/docs/getting-started/create-fleetautoscaler/" title="Quickstart: Create a Fleet Autoscaler" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsgetting-startedcreate-fleetautoscaler"><span class="">Create a Fleetautoscaler</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsgetting-startedcreate-webhook-fleetautoscaler-li">
  <a href="/site/docs/getting-started/create-webhook-fleetautoscaler/" title="Quickstart: Create a Fleet Autoscaler with Webhook Policy" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsgetting-startedcreate-webhook-fleetautoscaler"><span class="">Create a Webhook Fleetautoscaler</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsgetting-startededit-first-gameserver-go-li">
  <a href="/site/docs/getting-started/edit-first-gameserver-go/" title="Quickstart: Edit a Game Server" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsgetting-startededit-first-gameserver-go"><span class="">Edit Your First Game Server (Go)</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsguides-li">
  <a href="/site/docs/guides/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsguides"><span class="">Guides</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesfeature-stages-li">
  <a href="/site/docs/guides/feature-stages/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesfeature-stages"><span class="">Feature Stages</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsguidesbest-practices-li">
  <a href="/site/docs/guides/best-practices/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsguidesbest-practices"><span class="">Best Practices</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesbest-practicesgke-li">
  <a href="/site/docs/guides/best-practices/gke/" title="Google Kubernetes Engine Best Practices" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesbest-practicesgke"><span class="">Google Cloud</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsguidesclient-sdks-li">
  <a href="/site/docs/guides/client-sdks/" title="Agones Game Server Client SDKs" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsguidesclient-sdks"><span class="">Client SDKs</span></a>
  <ul class="ul-3 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdksunreal-li">
  <a href="/site/docs/guides/client-sdks/unreal/" title="Unreal Engine Game Server Client Plugin" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdksunreal"><span class="">Unreal Engine</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdksunity-li">
  <a href="/site/docs/guides/client-sdks/unity/" title="Unity Game Server Client SDK" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdksunity"><span class="">Unity</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdkscpp-li">
  <a href="/site/docs/guides/client-sdks/cpp/" title="C&#43;&#43; Game Server Client SDK" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdkscpp"><span class="">C&#43;&#43;</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdksgo-li">
  <a href="/site/docs/guides/client-sdks/go/" title="Go Game Server Client SDK" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdksgo"><span class="">Go</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdkscsharp-li">
  <a href="/site/docs/guides/client-sdks/csharp/" title="C# Game Server Client SDK" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdkscsharp"><span class="">C#</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdksnodejs-li">
  <a href="/site/docs/guides/client-sdks/nodejs/" title="Node.js Game Server Client SDK" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdksnodejs"><span class="">Node.js</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdksrust-li">
  <a href="/site/docs/guides/client-sdks/rust/" title="Rust Game Server Client SDK" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdksrust"><span class="">Rust</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdksrest-li">
  <a href="/site/docs/guides/client-sdks/rest/" title="REST Game Server Client API" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdksrest"><span class="">Rest</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesclient-sdkslocal-li">
  <a href="/site/docs/guides/client-sdks/local/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesclient-sdkslocal"><span class="">Local Development</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguideswindows-gameservers-li">
  <a href="/site/docs/guides/windows-gameservers/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguideswindows-gameservers"><span class="">Windows Gameservers</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesfleet-updates-li">
  <a href="/site/docs/guides/fleet-updates/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesfleet-updates"><span class="">Fleet Updates</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguideshealth-checking-li">
  <a href="/site/docs/guides/health-checking/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguideshealth-checking"><span class="">GameServer Health Checking</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidescounters-and-lists-li">
  <a href="/site/docs/guides/counters-and-lists/" title="GameServer Counters and Lists" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidescounters-and-lists"><span class="">Counters and Lists</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesplayer-tracking-li">
  <a href="/site/docs/guides/player-tracking/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesplayer-tracking"><span class="">Player Tracking</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguideslocal-game-server-li">
  <a href="/site/docs/guides/local-game-server/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguideslocal-game-server"><span class="">Local Game Server</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesping-service-li">
  <a href="/site/docs/guides/ping-service/" title="Latency Testing with Multiple Clusters" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesping-service"><span class="">Latency Testing Services</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesmetrics-li">
  <a href="/site/docs/guides/metrics/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesmetrics"><span class="">Metrics</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidesaccess-api-li">
  <a href="/site/docs/guides/access-api/" title="Access Agones via the Kubernetes API" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidesaccess-api"><span class="">Access Agones via the K8s API</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsguidestroubleshooting-li">
  <a href="/site/docs/guides/troubleshooting/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsguidestroubleshooting"><span class="">Troubleshooting</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsintegration-patterns-li">
  <a href="/site/docs/integration-patterns/" title="Common Integration Patterns" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsintegration-patterns"><span class="">Integration Patterns</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsintegration-patternsallocation-from-fleet-li">
  <a href="/site/docs/integration-patterns/allocation-from-fleet/" title="Matchmaker requests a GameServer from a Fleet" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsintegration-patternsallocation-from-fleet"><span class="">Allocation from a Fleet</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsintegration-patternsmatchmaker-registration-li">
  <a href="/site/docs/integration-patterns/matchmaker-registration/" title="Matchmaker requires game server process registration" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsintegration-patternsmatchmaker-registration"><span class="">Matchmaker registration</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsintegration-patternscanary-testing-li">
  <a href="/site/docs/integration-patterns/canary-testing/" title="Canary Testing a new Fleet" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsintegration-patternscanary-testing"><span class="">Canary Testing</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsintegration-patternsreusing-gameservers-li">
  <a href="/site/docs/integration-patterns/reusing-gameservers/" title="Reusing Allocated GameServers for more than one game session" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsintegration-patternsreusing-gameservers"><span class="">Reusing Gameservers</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsintegration-patternshigh-density-gameservers-li">
  <a href="/site/docs/integration-patterns/high-density-gameservers/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsintegration-patternshigh-density-gameservers"><span class="">High Density GameServers</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsintegration-patternsplayer-capacity-li">
  <a href="/site/docs/integration-patterns/player-capacity/" title="Allocating based on GameServer Player Capacity" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsintegration-patternsplayer-capacity"><span class="">Player Capacity</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocstutorials-li">
  <a href="/site/docs/tutorials/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocstutorials"><span class="">Tutorials</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocstutorialssimple-gameserver-rust-li">
  <a href="/site/docs/tutorials/simple-gameserver-rust/" title="Tutorial Build and Run a Simple Gameserver (Rust)" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocstutorialssimple-gameserver-rust"><span class="">Build and Run a Simple Gameserver (Rust)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocstutorialssimple-gameserver-nodejs-li">
  <a href="/site/docs/tutorials/simple-gameserver-nodejs/" title="Tutorial Build and Run a Simple Gameserver (node.js)" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocstutorialssimple-gameserver-nodejs"><span class="">Build and Run a Simple Gameserver (node.js)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocstutorialssimple-gameserver-cpp-li">
  <a href="/site/docs/tutorials/simple-gameserver-cpp/" title="Tutorial Build and Run a Simple Gameserver (C&#43;&#43;)" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocstutorialssimple-gameserver-cpp"><span class="">Build and Run a Simple Gameserver (C&#43;&#43;)</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id="m-sitedocsdsblog-li">
  <a href="/site/docs/dsblog/" title="Data Science Blog" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsdsblog"><span class="">DSBlog</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-27-6268-what-is-digital-twin-li">
  <a href="/site/docs/dsblog/2025-04-27-6268-what-is-digital-twin/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-27-6268-what-is-digital-twin"><span class="">What is a Digital Twin?</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-26-6267-hunyuan3d-2_0_-high-resolution-3dasset-generation-li">
  <a href="/site/docs/dsblog/2025-04-26-6267-hunyuan3d-2_0_-high-resolution-3dasset-generation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-26-6267-hunyuan3d-2_0_-high-resolution-3dasset-generation"><span class="">Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-25-6266-whisper_-large-scale-weak-supervision-speech-recognition-li">
  <a href="/site/docs/dsblog/2025-04-25-6266-whisper_-large-scale-weak-supervision-speech-recognition/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-25-6266-whisper_-large-scale-weak-supervision-speech-recognition"><span class="">Whisper: Robust Speech Recognition via Large-Scale Weak Supervision</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-24-6265-frequencies-in-time-and-space-li">
  <a href="/site/docs/dsblog/2025-04-24-6265-frequencies-in-time-and-space/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-24-6265-frequencies-in-time-and-space"><span class="">Frequencies in Time and Space: Understanding Nyquist Theorem &amp; its Applications</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-23-6264-the-real-story-of-nyquist-shannon-li">
  <a href="/site/docs/dsblog/2025-04-23-6264-the-real-story-of-nyquist-shannon/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-23-6264-the-real-story-of-nyquist-shannon"><span class="">The Real Story of Nyquist, Shannon, and the Science of Sampling</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-21-6263-ps-bitnet-b158-2b4t-li">
  <a href="/site/docs/dsblog/2025-04-21-6263-ps-bitnet-b1.58-2b4t/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-21-6263-ps-bitnet-b158-2b4t"><span class="">BitNet b1.58-2B4T: Revolutionary Binary Neural Network for Efficient AI</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-19-6262-ollama-setup-and-running-models-li">
  <a href="/site/docs/dsblog/2025-04-19-6262-ollama-setup-and-running-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-19-6262-ollama-setup-and-running-models"><span class="">Ollama Setup and Running Models</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-17-6261-ps-madam-rag-li">
  <a href="/site/docs/dsblog/2025-04-17-6261-ps-madam-rag/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-17-6261-ps-madam-rag"><span class="">Retrieval-Augmented Generation with Conflicting Evidence</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-15-6260-ps-llm-internal-encoding-of-truthfulness-and-hallucinations-li">
  <a href="/site/docs/dsblog/2025-04-15-6260-ps-llm-internal-encoding-of-truthfulness-and-hallucinations/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-15-6260-ps-llm-internal-encoding-of-truthfulness-and-hallucinations"><span class="">LLM Internal Encoding of Truthfulness and Hallucinations</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-14-6259-future-of-programming-li">
  <a href="/site/docs/dsblog/2025-04-14-6259-future-of-programming/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-14-6259-future-of-programming"><span class="">Future of Programming - Capabilities of AI Coding IDEs</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-13-6258-cybersecurity-concepts-in-ai-age-li">
  <a href="/site/docs/dsblog/2025-04-13-6258-cybersecurity-concepts-in-ai-age/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-13-6258-cybersecurity-concepts-in-ai-age"><span class="">Cybersecurity Concepts in AI Age</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-09-6257-quantum-physics-and-vedanta-li">
  <a href="/site/docs/dsblog/2025-04-09-6257-quantum-physics-and-vedanta/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-09-6257-quantum-physics-and-vedanta"><span class="">Quantum Physics and Vedanta: Bridging Science and Philosophy</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-04-07-6256-ai-benchmarks-for-2025-li">
  <a href="/site/docs/dsblog/2025-04-07-6256-ai-benchmarks-for-2025/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-04-07-6256-ai-benchmarks-for-2025"><span class="">AI Benchmarks for 2025</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-26-6252-audio-video-processing-concepts-li">
  <a href="/site/docs/dsblog/2025-03-26-6252-audio-video-processing-concepts/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-26-6252-audio-video-processing-concepts"><span class="">Audio Video Processing Concepts</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-29-6255-dsr-ai-benchmark-explorer-li">
  <a href="/site/docs/dsblog/2025-03-29-6255-dsr-ai-benchmark-explorer/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-29-6255-dsr-ai-benchmark-explorer"><span class="">Overview of AI Benchmark Explorer Tool</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-27-6253-quantum-physics-li">
  <a href="/site/docs/dsblog/2025-03-27-6253-quantum-physics/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-27-6253-quantum-physics"><span class="">Quantum Physics with Deeper Questions with ChatGPT</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-28-6254-understanding-electronics-chips-li">
  <a href="/site/docs/dsblog/2025-03-28-6254-understanding-electronics-chips/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-28-6254-understanding-electronics-chips"><span class="">Understanding Electronics Chips</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-25-6251-a-deep-dive-into-ai-model-marketplace-for-business-managers-li">
  <a href="/site/docs/dsblog/2025-03-25-6251-a-deep-dive-into-ai-model-marketplace-for-business-managers/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-25-6251-a-deep-dive-into-ai-model-marketplace-for-business-managers"><span class="">A Deep Dive into AI Model Marketplaces for Business Managers</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-24-6250-exploring-designer-roles-and-their-favorite-free-tools-li">
  <a href="/site/docs/dsblog/2025-03-24-6250-exploring-designer-roles-and-their-favorite-free-tools/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-24-6250-exploring-designer-roles-and-their-favorite-free-tools"><span class="">Exploring Designer Roles and Their Favorite Free Tools</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-23-6249-exploring-hosting-types-li">
  <a href="/site/docs/dsblog/2025-03-23-6249-exploring-hosting-types/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-23-6249-exploring-hosting-types"><span class="">Exploring Hosting Types</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-22-6248-video-editing-concepts-li">
  <a href="/site/docs/dsblog/2025-03-22-6248-video-editing-concepts/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-22-6248-video-editing-concepts"><span class="">Video Editing Concepts</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-21-6247-grok3model-and-sanskrit-li">
  <a href="/site/docs/dsblog/2025-03-21-6247-grok3model-and-sanskrit/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-21-6247-grok3model-and-sanskrit"><span class="">Grok 3 Model and Sanskrit</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-20-6246-implementing-secure-authentication-and-authorization-li">
  <a href="/site/docs/dsblog/2025-03-20-6246-implementing-secure-authentication-and-authorization/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-20-6246-implementing-secure-authentication-and-authorization"><span class="">Implementing Secure Authentication and Authorization</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-18-6244-codereview-with-coderabbit-li">
  <a href="/site/docs/dsblog/2025-03-18-6244-codereview-with-coderabbit/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-18-6244-codereview-with-coderabbit"><span class="">CodeReview with CodeRabbit AI - A Better Way to Automate Code Reviews</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-18-6245-return-of-sunita-william-from-iss-li">
  <a href="/site/docs/dsblog/2025-03-18-6245-return-of-sunita-william-from-iss/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-18-6245-return-of-sunita-william-from-iss"><span class="">Return of Sunita William from ISS</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-17-6243-what-are-life-science-li">
  <a href="/site/docs/dsblog/2025-03-17-6243-what-are-life-science/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-17-6243-what-are-life-science"><span class="">What are Life Sciences?</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-16-6242-real-life-application-of-maths-li">
  <a href="/site/docs/dsblog/2025-03-16-6242-real-life-application-of-maths/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-16-6242-real-life-application-of-maths"><span class="">Real-Life Applications of Mathematics</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-15-6241-exploring-reactjs-library-li">
  <a href="/site/docs/dsblog/2025-03-15-6241-exploring-reactjs-library/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-15-6241-exploring-reactjs-library"><span class="">Exploring Reactjs Library</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-14-6240-guide-to-evaluate-generative-models-li">
  <a href="/site/docs/dsblog/2025-03-14-6240-guide-to-evaluate-generative-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-14-6240-guide-to-evaluate-generative-models"><span class="">A Comprehensive Guide to Evaluate Generative Models</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-13-6239-software-development-toolsets-explained-li">
  <a href="/site/docs/dsblog/2025-03-13-6239-software-development-toolsets-explained/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-13-6239-software-development-toolsets-explained"><span class="">Software Development Toolsets Explained</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-12-6238-implementing-comments-in-jekyll-blogs-li">
  <a href="/site/docs/dsblog/2025-03-12-6238-implementing-comments-in-jekyll-blogs/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-12-6238-implementing-comments-in-jekyll-blogs"><span class="">Implementing Comments in Jekyll Blogs</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-11-6237-exploring-graphdb-and-neo4j-li">
  <a href="/site/docs/dsblog/2025-03-11-6237-exploring-graphdb-and-neo4j/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-11-6237-exploring-graphdb-and-neo4j"><span class="">Exploring GraphDB and Neo4j - A Guide to Graph Databases</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-10-6236-understanding-the-business-of-marketplaces-li">
  <a href="/site/docs/dsblog/2025-03-10-6236-understanding-the-business-of-marketplaces/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-10-6236-understanding-the-business-of-marketplaces"><span class="">Understanding the Business of Marketplaces</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-08-6235-exploring-and-evaluating-ides-li">
  <a href="/site/docs/dsblog/2025-03-08-6235-exploring-and-evaluating-ides/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-08-6235-exploring-and-evaluating-ides"><span class="">Exploring and Evaluating Integrated Development Environments (IDEs)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-02-6234-working-with-github-from-wsl-and-windows-folders-li">
  <a href="/site/docs/dsblog/2025-03-02-6234-working-with-github-from-wsl-and-windows-folders/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-02-6234-working-with-github-from-wsl-and-windows-folders"><span class="">Working with GitHub from WSL and Windows Folders</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-03-01-6233-why-to-use-offline-ai-models-li">
  <a href="/site/docs/dsblog/2025-03-01-6233-why-to-use-offline-ai-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-03-01-6233-why-to-use-offline-ai-models"><span class="">Why Use Offline AI Models?</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-28-6232-online-coding-tools-li">
  <a href="/site/docs/dsblog/2025-02-28-6232-online-coding-tools/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-28-6232-online-coding-tools"><span class="">Online Coding Tools: Choosing the Right IDE for Your Project</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-27-6230-basics-of-microcessor-and-microcontroller-li">
  <a href="/site/docs/dsblog/2025-02-27-6230-basics-of-microcessor-and-microcontroller/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-27-6230-basics-of-microcessor-and-microcontroller"><span class="">The Basics of Microprocessor and Microcontroller</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-26-6229-exploring-reasoning-models-li">
  <a href="/site/docs/dsblog/2025-02-26-6229-exploring-reasoning-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-26-6229-exploring-reasoning-models"><span class="">Exploring Reasoning Models in AI Marketplace, Feb 25</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-25-6228-exploring-deepseek-r1-li">
  <a href="/site/docs/dsblog/2025-02-25-6228-exploring-deepseek-r1/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-25-6228-exploring-deepseek-r1"><span class="">Exploring DeepSeek R1: The AI That Thinks Like a Human</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-24-6227-integrating-ollama-with-openwebui-li">
  <a href="/site/docs/dsblog/2025-02-24-6227-integrating-ollama-with-openwebui/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-24-6227-integrating-ollama-with-openwebui"><span class="">Integrating Ollama AI Models and Open WebUI with Docker: A Step-by-Step Guide</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-23-6226-unlocking-the-power-of-prompts-li">
  <a href="/site/docs/dsblog/2025-02-23-6226-unlocking-the-power-of-prompts/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-23-6226-unlocking-the-power-of-prompts"><span class="">Unlocking the Power of Prompts: A Comprehensive Guide to Prompt Engineering</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-22-6225-exploring-reinformement-learning-concepts-li">
  <a href="/site/docs/dsblog/2025-02-22-6225-exploring-reinformement-learning-concepts/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-22-6225-exploring-reinformement-learning-concepts"><span class="">Exploring Reinforcement Learning Concepts: A Comprehensive Guide</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-21-6224-dsr-ai-resources-li">
  <a href="/site/docs/dsblog/2025-02-21-6224-dsr-ai-resources/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-21-6224-dsr-ai-resources"><span class="">AI Resources: Ultimate Collection of Cutting-Edge Tools for AI Enthusiasts</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-27-6231-dsr-mybookmark-blog-articles-li">
  <a href="/site/docs/dsblog/2025-02-27-6231-dsr-mybookmark-blog-articles/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-27-6231-dsr-mybookmark-blog-articles"><span class="">Bookmark Blog Articles from Browser</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-20-6223-exploring-possibilities-with-css-li">
  <a href="/site/docs/dsblog/2025-02-20-6223-exploring-possibilities-with-css/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-20-6223-exploring-possibilities-with-css"><span class="">Exploring Possibilities with CSS: A Comprehensive Guide to Advanced CSS Coding</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-19-6222-devenvironment-with-windowswsl2docker-li">
  <a href="/site/docs/dsblog/2025-02-19-6222-devenvironment-with-windows&#43;wsl2&#43;docker/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-19-6222-devenvironment-with-windowswsl2docker"><span class="">Setting Up a Development Environment with Windows 11, WSL2 and Docker</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-18-6221-exploring-shell-scrips-and-batch-files-li">
  <a href="/site/docs/dsblog/2025-02-18-6221-exploring-shell-scrips-and-batch-files/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-18-6221-exploring-shell-scrips-and-batch-files"><span class="">Exploring Shell Scripting and Batch Files</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-17-6220-exploring-the-local-location-of-model-li">
  <a href="/site/docs/dsblog/2025-02-17-6220-exploring-the-local-location-of-model/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-17-6220-exploring-the-local-location-of-model"><span class="">Exploring the Local Location of Ollama Models on WSL2</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-16-6219-state-of-the-art-computer-vision-models-li">
  <a href="/site/docs/dsblog/2025-02-16-6219-state-of-the-art-computer-vision-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-16-6219-state-of-the-art-computer-vision-models"><span class="">State of the Art Image Generation Models in Computer Vision: A Comprehensive Overview</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-14-6218-the-road-of-bharat-llm-goes-via-sanskri-li">
  <a href="/site/docs/dsblog/2025-02-14-6218-the-road-of-bharat-llm-goes-via-sanskri/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-14-6218-the-road-of-bharat-llm-goes-via-sanskri"><span class="">The Road to Bharat LLMs Goes Via Sanskrit</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-10-6217-exploring-the-energy-demand-and-supply-of-india-li">
  <a href="/site/docs/dsblog/2025-02-10-6217-exploring-the-energy-demand-and-supply-of-india/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-10-6217-exploring-the-energy-demand-and-supply-of-india"><span class="">Exploring the Energy Demand and Supply of India</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-02-09-6216-demystify-nvidia-gpus-li">
  <a href="/site/docs/dsblog/2025-02-09-6216-demystify-nvidia-gpus/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-02-09-6216-demystify-nvidia-gpus"><span class="">Demystifying NVIDIA GPUs</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-31-6215-exploring-tokenization-in-ai-li">
  <a href="/site/docs/dsblog/2025-01-31-6215-exploring-tokenization-in-ai/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-31-6215-exploring-tokenization-in-ai"><span class="">Exploring Tokenization and Embedding in NLP</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-30-6214-understanding-contextual-embedding-in-transformers-li">
  <a href="/site/docs/dsblog/2025-01-30-6214-understanding-contextual-embedding-in-transformers/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-30-6214-understanding-contextual-embedding-in-transformers"><span class="">Understanding Contextual Embedding in Transformers</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-29-6213-understaning-working-of-cnn-li">
  <a href="/site/docs/dsblog/2025-01-29-6213-understaning-working-of-cnn/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-29-6213-understaning-working-of-cnn"><span class="">Understanding the Working of CNN</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-28-6212-power-of-chinese-ai-models-li">
  <a href="/site/docs/dsblog/2025-01-28-6212-power-of-chinese-ai-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-28-6212-power-of-chinese-ai-models"><span class="">Power of Chinese AI Models</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-27-6211-computer-vision-research-work-li">
  <a href="/site/docs/dsblog/2025-01-27-6211-computer-vision-research-work/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-27-6211-computer-vision-research-work"><span class="">Computer Vision Research Work</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-26-6210-exploring-ai-benchmarks-and-leaderboards-li">
  <a href="/site/docs/dsblog/2025-01-26-6210-exploring-ai-benchmarks-and-leaderboards/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-26-6210-exploring-ai-benchmarks-and-leaderboards"><span class="">Exploring AI Benchmarks &amp; Leaderboards</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-25-6209-exploring-types-of-models-li">
  <a href="/site/docs/dsblog/2025-01-25-6209-exploring-types-of-models/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-25-6209-exploring-types-of-models"><span class="">Exploring Types of Models</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-24-6208-understanding-callbacks-li">
  <a href="/site/docs/dsblog/2025-01-24-6208-understanding-callbacks/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-24-6208-understanding-callbacks"><span class="">Understanding Callbacks</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-23-6207-exploring-ai-agents-li">
  <a href="/site/docs/dsblog/2025-01-23-6207-exploring-ai-agents/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-23-6207-exploring-ai-agents"><span class="">Exploring AI Agents</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-22-6206-ai-product-ideas-2025-li">
  <a href="/site/docs/dsblog/2025-01-22-6206-ai-product-ideas-2025/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-22-6206-ai-product-ideas-2025"><span class="">AI Product Ideas 2025</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-21-6205-aiml-project-ideas-li">
  <a href="/site/docs/dsblog/2025-01-21-6205-aiml-project-ideas/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-21-6205-aiml-project-ideas"><span class="">AI ML Project Ideas</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-20-6204-whatsapp-integration-li">
  <a href="/site/docs/dsblog/2025-01-20-6204-whatsapp-integration/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-20-6204-whatsapp-integration"><span class="">WhatsApp Integration: Webhooks, Messaging, and Architecture Design</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-19-6203-unraveling-wireless-communication-li">
  <a href="/site/docs/dsblog/2025-01-19-6203-unraveling-wireless-communication/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-19-6203-unraveling-wireless-communication"><span class="">Unraveling Wireless Communication: Key Protocols and Technologies</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-17-6202-adapting-ai-models-to-the-latest-information-methods-and-approaches-li">
  <a href="/site/docs/dsblog/2025-01-17-6202-adapting-ai-models-to-the-latest-information-methods-and-approaches/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-17-6202-adapting-ai-models-to-the-latest-information-methods-and-approaches"><span class="">Adapting AI Models to the Latest Information: Methods and Approaches</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-16-6200-exploring-makecom-a-comprehensive-guide-li">
  <a href="/site/docs/dsblog/2025-01-16-6200-exploring-make.com-a-comprehensive-guide/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-16-6200-exploring-makecom-a-comprehensive-guide"><span class="">Exploring Make.com: A Comprehensive Guide to Automation and Its Alternatives</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-16-6201-navigating-open-source-licensing-in-the-age-of-ai-li">
  <a href="/site/docs/dsblog/2025-01-16-6201-navigating-open-source-licensing-in-the-age-of-ai/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-16-6201-navigating-open-source-licensing-in-the-age-of-ai"><span class="">Navigating Open-Source Licensing in the Age of AI: Challenges and Considerations</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-08-6199-rethinking-ai-infrastructure-advantages-of-on-prem-over-cloud-solutions-li">
  <a href="/site/docs/dsblog/2025-01-08-6199-rethinking-ai-infrastructure-advantages-of-on-prem-over-cloud-solutions/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-08-6199-rethinking-ai-infrastructure-advantages-of-on-prem-over-cloud-solutions"><span class="">Rethinking AI Infrastructure: Advantages of On-Prem Over Cloud Solutions</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-07-6198-shaping-tomorrow-with-ai-nvidia-li">
  <a href="/site/docs/dsblog/2025-01-07-6198-shaping-tomorrow-with-ai-nvidia/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-07-6198-shaping-tomorrow-with-ai-nvidia"><span class="">Shaping Tomorrow with AI: Nvidia’s Innovations in Graphics, Robotics, and Intelligence</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-03-6197-ai-predictions-for-2025-li">
  <a href="/site/docs/dsblog/2025-01-03-6197-ai-predictions-for-2025/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-03-6197-ai-predictions-for-2025"><span class="">AI Predictions for 2025</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2025-01-02-6196-the-complete-ecosystem-of-software-development-li">
  <a href="/site/docs/dsblog/2025-01-02-6196-the-complete-ecosystem-of-software-development/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2025-01-02-6196-the-complete-ecosystem-of-software-development"><span class="">The Complete Ecosystem of Software Development</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-29-6195-curl-commands-li">
  <a href="/site/docs/dsblog/2024-12-29-6195-curl-commands/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-29-6195-curl-commands"><span class="">curl Commands</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-28-6194-how-alphafold-is-revolutionizing-protein-science-li">
  <a href="/site/docs/dsblog/2024-12-28-6194-how-alphafold-is-revolutionizing-protein-science/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-28-6194-how-alphafold-is-revolutionizing-protein-science"><span class="">How AlphaFold is Revolutionizing Protein Science</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-22-6193-openai-12-days-2024-announcements-li">
  <a href="/site/docs/dsblog/2024-12-22-6193-openai-12-days-2024-announcements/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-22-6193-openai-12-days-2024-announcements"><span class="">OpenAI 12 Days 2024 Announcements</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-17-6192-framework-for-using-llm-li">
  <a href="/site/docs/dsblog/2024-12-17-6192-framework-for-using-llm/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-17-6192-framework-for-using-llm"><span class="">Framework for using LLM</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-16-6191-ai-imperialism-western-dominance-and-the-future-li">
  <a href="/site/docs/dsblog/2024-12-16-6191-ai-imperialism-western-dominance-and-the-future/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-16-6191-ai-imperialism-western-dominance-and-the-future"><span class="">AI Imperialism: Western Dominance and the Future of Global Technology </span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-13-6189-visualizing-transformers-and-attention-li">
  <a href="/site/docs/dsblog/2024-12-13-6189-visualizing-transformers-and-attention/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-13-6189-visualizing-transformers-and-attention"><span class="">Visualizing Transformers and Attention</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-12-6188-exploring-gpus-li">
  <a href="/site/docs/dsblog/2024-12-12-6188-exploring-gpus/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-12-6188-exploring-gpus"><span class="">Exploring Graphics Processing Units (GPUs)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-10-6187-ai-models-and-creators-li">
  <a href="/site/docs/dsblog/2024-12-10-6187-ai-models-and-creators/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-10-6187-ai-models-and-creators"><span class="">AI Models and Creators</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-09-6186-shazam-features-li">
  <a href="/site/docs/dsblog/2024-12-09-6186-shazam-features/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-09-6186-shazam-features"><span class="">Features ofShazam Econometrics Software</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-08-6185-serverless-llm-deployment-li">
  <a href="/site/docs/dsblog/2024-12-08-6185-serverless-llm-deployment/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-08-6185-serverless-llm-deployment"><span class="">Serverless LLM Deployment Platform</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-07-6184-microsoft-ai-products-li">
  <a href="/site/docs/dsblog/2024-12-07-6184-microsoft-ai-products/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-07-6184-microsoft-ai-products"><span class="">Microsoft AI Products</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-06-6183-google-ai-studio-vs-vertexai-li">
  <a href="/site/docs/dsblog/2024-12-06-6183-google-ai-studio-vs-vertexai/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-06-6183-google-ai-studio-vs-vertexai"><span class="">Google AI Studio vs Vertex AI</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-12-05-6182-introduction-to-nvidia-and-products-li">
  <a href="/site/docs/dsblog/2024-12-05-6182-introduction-to-nvidia-and-products/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-12-05-6182-introduction-to-nvidia-and-products"><span class="">Introduction to NVIDIA and Products</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-11-14-6181-navigating-the-llm-infrastructure-landscape-li">
  <a href="/site/docs/dsblog/2024-11-14-6181-navigating-the-llm-infrastructure-landscape/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-11-14-6181-navigating-the-llm-infrastructure-landscape"><span class="">Navigating the LLM Infrastructure Landscape</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-11-12-6180-exploring-gguf-and-other-model-formats-li">
  <a href="/site/docs/dsblog/2024-11-12-6180-exploring-gguf-and-other-model-formats/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-11-12-6180-exploring-gguf-and-other-model-formats"><span class="">Exploring GGUF and Other Model Formats</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-11-11-6179-exploring-anythingllm-li">
  <a href="/site/docs/dsblog/2024-11-11-6179-exploring-anythingllm/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-11-11-6179-exploring-anythingllm"><span class="">Exploring AnythingLLM</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-11-01-6178-navigating-python-ecosystem-li">
  <a href="/site/docs/dsblog/2024-11-01-6178-navigating-python-ecosystem/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-11-01-6178-navigating-python-ecosystem"><span class="">Navigating Python Ecosystem</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-30-6177-processors-for-html-css-js-code-li">
  <a href="/site/docs/dsblog/2024-10-30-6177-processors-for-html-css-js-code/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-30-6177-processors-for-html-css-js-code"><span class="">Processors for HTML CSS JS Code</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-29-6176-exploring-popular-web-servers-li">
  <a href="/site/docs/dsblog/2024-10-29-6176-exploring-popular-web-servers/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-29-6176-exploring-popular-web-servers"><span class="">Exploring Popular Web Server</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-28-6175-exploring-all-dimensions-of-application-development-li">
  <a href="/site/docs/dsblog/2024-10-28-6175-exploring-all-dimensions-of-application-development/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-28-6175-exploring-all-dimensions-of-application-development"><span class="">Exploring All Dimensions of Application Development</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-27-6174-exploring-llm-app-development-li">
  <a href="/site/docs/dsblog/2024-10-27-6174-exploring-llm-app-development/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-27-6174-exploring-llm-app-development"><span class="">Exploring LLM Application Development</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-26-6173-ai-benchmarks-explained-li">
  <a href="/site/docs/dsblog/2024-10-26-6173-ai-benchmarks-explained/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-26-6173-ai-benchmarks-explained"><span class="">AI Benchmarks Explained</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-25-6172-transfer-learning-key-ai-techniques-explained-li">
  <a href="/site/docs/dsblog/2024-10-25-6172-transfer-learning-key-ai-techniques-explained/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-25-6172-transfer-learning-key-ai-techniques-explained"><span class="">Transfer Learning Key AI Techniques Explained</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-24-6171-types-of-llm-li">
  <a href="/site/docs/dsblog/2024-10-24-6171-types-of-llm/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-24-6171-types-of-llm"><span class="">Types of Large Language Models (LLM)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-23-6170-nativagting-javascript-ecosystem-li">
  <a href="/site/docs/dsblog/2024-10-23-6170-nativagting-javascript-ecosystem/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-23-6170-nativagting-javascript-ecosystem"><span class="">Navigating the JavaScript Ecosystem</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-22-6169-applications-of-genai-li">
  <a href="/site/docs/dsblog/2024-10-22-6169-applications-of-genai/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-22-6169-applications-of-genai"><span class="">Applications of GenAI</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsdsblog2024-10-21-6168-understanding-jekyll-framework-li">
  <a href="/site/docs/dsblog/2024-10-21-6168-understanding-jekyll-framework/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsdsblog2024-10-21-6168-understanding-jekyll-framework"><span class="">Understanding Jekyll Framework</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsreference-li">
  <a href="/site/docs/reference/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsreference"><span class="">Reference</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsreferencegameserver-li">
  <a href="/site/docs/reference/gameserver/" title="GameServer Specification" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsreferencegameserver"><span class="">Gameserver</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsreferencefleet-li">
  <a href="/site/docs/reference/fleet/" title="Fleet Specification" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsreferencefleet"><span class="">Fleet</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsreferencegameserverallocation-li">
  <a href="/site/docs/reference/gameserverallocation/" title="GameServerAllocation Specification" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsreferencegameserverallocation"><span class="">GameServerAllocation</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsreferencefleetautoscaler-li">
  <a href="/site/docs/reference/fleetautoscaler/" title="Fleet Autoscaler Specification" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsreferencefleetautoscaler"><span class="">Fleet Autoscaler</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsreferenceagones_crd_api_reference-li">
  <a href="/site/docs/reference/agones_crd_api_reference/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsreferenceagones_crd_api_reference"><span class="">Agones Kubernetes API</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsexamples-li">
  <a href="/site/docs/examples/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsexamples"><span class="">Examples</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsexamplescustom-controller-li">
  <a href="/site/docs/examples/custom-controller/" title="Custom Controller for Agones Game Servers" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsexamplescustom-controller"><span class="">Custom Controller</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsexamplessimple-genai-gameserver-li">
  <a href="/site/docs/examples/simple-genai-gameserver/" title="Build and Run a Simple Game Server that Connects to an Inference Server" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsexamplessimple-genai-gameserver"><span class="">GenAI Game Server</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsexamplessupertuxkart-li">
  <a href="/site/docs/examples/supertuxkart/" title="Deploying and Running SuperTuxKart Server Using Agones" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsexamplessupertuxkart"><span class="">SuperTuxKart</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsexamplesxonotic-li">
  <a href="/site/docs/examples/xonotic/" title="Deploying and Running Xonotic Server Using Agones" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsexamplesxonotic"><span class="">Xonotic</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsadvanced-li">
  <a href="/site/docs/advanced/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsadvanced"><span class="">Advanced</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedsystem-diagram-li">
  <a href="/site/docs/advanced/system-diagram/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedsystem-diagram"><span class="">System Diagram</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedscheduling-and-autoscaling-li">
  <a href="/site/docs/advanced/scheduling-and-autoscaling/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedscheduling-and-autoscaling"><span class="">Scheduling and Autoscaling</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedhigh-availability-agones-li">
  <a href="/site/docs/advanced/high-availability-agones/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedhigh-availability-agones"><span class="">High Availability Agones</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedcontrolling-disruption-li">
  <a href="/site/docs/advanced/controlling-disruption/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedcontrolling-disruption"><span class="">Controlling Disruption</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedlimiting-resources-li">
  <a href="/site/docs/advanced/limiting-resources/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedlimiting-resources"><span class="">Limiting CPU &amp; Memory</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedout-of-cluster-dev-server-li">
  <a href="/site/docs/advanced/out-of-cluster-dev-server/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedout-of-cluster-dev-server"><span class="">Out of Cluster Dev Server</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedallocator-service-li">
  <a href="/site/docs/advanced/allocator-service/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedallocator-service"><span class="">Allocator Service</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedmulti-cluster-allocation-li">
  <a href="/site/docs/advanced/multi-cluster-allocation/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedmulti-cluster-allocation"><span class="">Multi-cluster Allocation</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsadvancedservice-accounts-li">
  <a href="/site/docs/advanced/service-accounts/" title="GameServer Pod Service Accounts" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsadvancedservice-accounts"><span class="">Service Accounts</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsfaq-li">
  <a href="/site/docs/faq/" title="Frequently Asked Questions" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsfaq"><span class="">FAQ</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocsthird-party-content-li">
  <a href="/site/docs/third-party-content/" title="Third Party Content" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocsthird-party-content"><span class="">Third Party</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsthird-party-contentvideos-and-presentations-li">
  <a href="/site/docs/third-party-content/videos-and-presentations/" title="Third Party Videos and Presentations" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsthird-party-contentvideos-and-presentations"><span class="">Videos and Presentations</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsthird-party-contentexamples-li">
  <a href="/site/docs/third-party-content/examples/" title="Third Party Examples" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsthird-party-contentexamples"><span class="">Examples</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsthird-party-contentpodcasts-li">
  <a href="/site/docs/third-party-content/podcasts/" title="Third Party Podcasts" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsthird-party-contentpodcasts"><span class="">Podcasts</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocsthird-party-contentlibraries-tools-li">
  <a href="/site/docs/third-party-content/libraries-tools/" title="Third Party Libraries and Tools" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocsthird-party-contentlibraries-tools"><span class="">Libraries and Tools</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocscontribute-li">
  <a href="/site/docs/contribute/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocscontribute"><span class="">Contribute</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocscontributeagones-feature-proposal-li">
  <a href="/site/docs/contribute/agones-feature-proposal/" title="Agones Feature Proposal (AFP)" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocscontributeagones-feature-proposal"><span class="">Agones Feature Proposal</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocscontributedocumentation-editing-contribution-li">
  <a href="/site/docs/contribute/documentation-editing-contribution/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocscontributedocumentation-editing-contribution"><span class="">Documentation Editing and Contribution</span></a>
</li>
  </ul>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child" id="m-sitedocssamskrutyatra-li">
  <a href="/site/docs/samskrutyatra/" class="align-left ps-0 td-sidebar-link td-sidebar-link__section" id="m-sitedocssamskrutyatra"><span class="">Samskrut Yatra Blog</span></a>
  <ul class="ul-2 foldable">
    <li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2025-01-16-4173-guha-geeta-li">
  <a href="/site/docs/samskrutyatra/2025-01-16-4173-guha-geeta/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2025-01-16-4173-guha-geeta"><span class="">Guhagita - गुहगीता</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2023-04-26-4172-shri-hanuman-sahasranamam-li">
  <a href="/site/docs/samskrutyatra/2023-04-26-4172-shri-hanuman-sahasranamam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2023-04-26-4172-shri-hanuman-sahasranamam"><span class="">Hanuman Sahsranamavali - हनुमान सहस्त्रनामावली</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2022-09-05-4171-vigyan-bhairav-tantra-li">
  <a href="/site/docs/samskrutyatra/2022-09-05-4171-vigyan-bhairav-tantra/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2022-09-05-4171-vigyan-bhairav-tantra"><span class="">Vijnana Bhairava Tantra Sutras - विज्ञान भैरव तंत्र के सूत्र</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2022-09-03-4170-lalitasaharanam-with-meaning-li">
  <a href="/site/docs/samskrutyatra/2022-09-03-4170-lalitasaharanam-with-meaning/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2022-09-03-4170-lalitasaharanam-with-meaning"><span class="">Lalitasahasranamam with Meaning - ललितासहस्रनामं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-21-4169-song-of-sanyasi-sanskrit-li">
  <a href="/site/docs/samskrutyatra/2021-05-21-4169-song-of-sanyasi-sanskrit/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-21-4169-song-of-sanyasi-sanskrit"><span class="">Song of Sanyasi - सन्यासी का गीत (संस्कृतं)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-19-4168-bhajan-without-text-li">
  <a href="/site/docs/samskrutyatra/2021-05-19-4168-bhajan-without-text/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-19-4168-bhajan-without-text"><span class="">Bhajan without Text</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-18-4167-chanakya-jivanasya-katha-li">
  <a href="/site/docs/samskrutyatra/2021-05-18-4167-chanakya-jivanasya-katha/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-18-4167-chanakya-jivanasya-katha"><span class="">चाणक्य जीवनस्य (लघुकथा:)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-17-4165-punishment-of-enjoyment-li">
  <a href="/site/docs/samskrutyatra/2021-05-17-4165-punishment-of-enjoyment/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-17-4165-punishment-of-enjoyment"><span class="">कर्मकर्याः दण्डं (लघुकथा:)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-17-4163-gamanugat-lokvyavhar-li">
  <a href="/site/docs/samskrutyatra/2021-05-17-4163-gamanugat-lokvyavhar/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-17-4163-gamanugat-lokvyavhar"><span class="">गमानुगतः लोकव्यवहारः (लघुकथा:)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-17-4164-madanmohal-malviya-sanskrit-katha-li">
  <a href="/site/docs/samskrutyatra/2021-05-17-4164-madanmohal-malviya-sanskrit-katha/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-17-4164-madanmohal-malviya-sanskrit-katha"><span class="">मदनमोहन मालवीय: (लघुकथा:)</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-17-4166-sanskrit-question-answers-li">
  <a href="/site/docs/samskrutyatra/2021-05-17-4166-sanskrit-question-answers/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-17-4166-sanskrit-question-answers"><span class="">संस्कृत प्रश्नोत्तर भाग-1</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-05-14-4162-work-of-rajiv-dixit-li">
  <a href="/site/docs/samskrutyatra/2021-05-14-4162-work-of-rajiv-dixit/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-05-14-4162-work-of-rajiv-dixit"><span class="">Work of Rajiv Dixit</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-03-17-4161-hindu-festivals-and-conspiracy-li">
  <a href="/site/docs/samskrutyatra/2021-03-17-4161-hindu-festivals-and-conspiracy/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-03-17-4161-hindu-festivals-and-conspiracy"><span class="">सनातन संस्कृति एवं षडयंत्र</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-02-08-4160-gayatri-sahasranama-li">
  <a href="/site/docs/samskrutyatra/2021-02-08-4160-gayatri-sahasranama/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-02-08-4160-gayatri-sahasranama"><span class="">Gayatri Sahasranama - श्री गायत्री सहस्रनामस्तोत्रं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-02-08-4159-building-a-bridge-li">
  <a href="/site/docs/samskrutyatra/2021-02-08-4159-building-a-bridge/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-02-08-4159-building-a-bridge"><span class="">लघुकथा: सेतुः निर्माणं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-01-11-4153-bajarang-baan-li">
  <a href="/site/docs/samskrutyatra/2021-01-11-4153-bajarang-baan/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-01-11-4153-bajarang-baan"><span class="">Bajarang Baan  - बजरंग बाण</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-01-11-4154-hanuman-ashtakam-li">
  <a href="/site/docs/samskrutyatra/2021-01-11-4154-hanuman-ashtakam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-01-11-4154-hanuman-ashtakam"><span class="">Hanuman Ashtakam - संकटमोचन हनुमानाष्टकम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-01-11-4155-maheshwara-sutrani-li">
  <a href="/site/docs/samskrutyatra/2021-01-11-4155-maheshwara-sutrani/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-01-11-4155-maheshwara-sutrani"><span class="">Maheshwara Sutrani - माहेश्वर सूत्र</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-01-11-4157-sundarkand-dhyanam-li">
  <a href="/site/docs/samskrutyatra/2021-01-11-4157-sundarkand-dhyanam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-01-11-4157-sundarkand-dhyanam"><span class="">Sundarkand Dhyanam - सुन्दरकांड ध्यान श्लोकं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-01-11-4158-upnishadic-prarthana-li">
  <a href="/site/docs/samskrutyatra/2021-01-11-4158-upnishadic-prarthana/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-01-11-4158-upnishadic-prarthana"><span class="">Upnishadic Prarthana - उपनिषदिय प्रार्थना</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2021-01-01-4152-namaste-sada-vatsale-li">
  <a href="/site/docs/samskrutyatra/2021-01-01-4152-namaste-sada-vatsale/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2021-01-01-4152-namaste-sada-vatsale"><span class="">Namaste Sada Vatsale - नमस्ते सदा वत्सले</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-18-4150-durga-dwatrimshan-namamaalaa-li">
  <a href="/site/docs/samskrutyatra/2020-06-18-4150-durga-dwatrimshan-namamaalaa/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-18-4150-durga-dwatrimshan-namamaalaa"><span class="">Durga DwaTrimShan NamaMaalaa - अथ दुर्गाद्वात्रिंशन्नाममाला</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-17-4149-shri-durga-manas-puja-li">
  <a href="/site/docs/samskrutyatra/2020-06-17-4149-shri-durga-manas-puja/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-17-4149-shri-durga-manas-puja"><span class="">Shri Durga Manas Puja - श्रीदुर्गामानस-पूजा</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-16-4147-devi-murti-rahasyam-li">
  <a href="/site/docs/samskrutyatra/2020-06-16-4147-devi-murti-rahasyam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-16-4147-devi-murti-rahasyam"><span class="">Devi Murti Rahasyam - मूर्तिरहस्यम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-16-4148-devi-vaikrutikam-rahashy-li">
  <a href="/site/docs/samskrutyatra/2020-06-16-4148-devi-vaikrutikam-rahashy/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-16-4148-devi-vaikrutikam-rahashy"><span class="">Devi Vaikrutikam Rahashyam - वैकृतिकं रहस्यम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-15-4146-devi-praahanikam-rahasyam-li">
  <a href="/site/docs/samskrutyatra/2020-06-15-4146-devi-praahanikam-rahasyam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-15-4146-devi-praahanikam-rahasyam"><span class="">Devi Prahanikam Rahasyam - प्राधानिकं रहस्यम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-14-4145-durga-saptashati-chapter13-li">
  <a href="/site/docs/samskrutyatra/2020-06-14-4145-durga-saptashati-chapter13/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-14-4145-durga-saptashati-chapter13"><span class="">Durga Saptashati Chapter 13 - श्रीदुर्गासप्तशती - त्रयोदशोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-13-4144-durga-saptashati-chapter12-li">
  <a href="/site/docs/samskrutyatra/2020-06-13-4144-durga-saptashati-chapter12/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-13-4144-durga-saptashati-chapter12"><span class="">Durga Saptashati Chapter 12 - श्रीदुर्गासप्तशती - द्वादशोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-12-4143-durga-saptashati-chapter11-li">
  <a href="/site/docs/samskrutyatra/2020-06-12-4143-durga-saptashati-chapter11/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-12-4143-durga-saptashati-chapter11"><span class="">Durga Saptashati Chapter 11 - श्रीदुर्गासप्तशती -एकादशोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-11-4142-durga-saptashati-chapter10-li">
  <a href="/site/docs/samskrutyatra/2020-06-11-4142-durga-saptashati-chapter10/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-11-4142-durga-saptashati-chapter10"><span class="">Durga Saptashati Chapter 10 - श्रीदुर्गासप्तशती दशमोऽध्यायः शुम्भ वध</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-10-4141-durga-saptashati-chapter9-li">
  <a href="/site/docs/samskrutyatra/2020-06-10-4141-durga-saptashati-chapter9/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-10-4141-durga-saptashati-chapter9"><span class="">Durga Saptashati Chapter 9 - श्रीदुर्गासप्तशती - नवमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-09-4140-durga-saptashati-chapter8-li">
  <a href="/site/docs/samskrutyatra/2020-06-09-4140-durga-saptashati-chapter8/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-09-4140-durga-saptashati-chapter8"><span class="">Durga Saptashati Chapter 8 - श्रीदुर्गासप्तशती - अष्टमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-08-4139-durga-saptashati-chapter7-li">
  <a href="/site/docs/samskrutyatra/2020-06-08-4139-durga-saptashati-chapter7/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-08-4139-durga-saptashati-chapter7"><span class="">Durga Saptashati Chapter 7 - श्रीदुर्गासप्तशती - सप्तमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-07-4138-durga-saptashati-chapter6-li">
  <a href="/site/docs/samskrutyatra/2020-06-07-4138-durga-saptashati-chapter6/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-07-4138-durga-saptashati-chapter6"><span class="">Durga Saptashati Chapter 6 - श्रीदुर्गासप्तशती - षष्ठोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-06-4137-durga-saptashati-chapter5-li">
  <a href="/site/docs/samskrutyatra/2020-06-06-4137-durga-saptashati-chapter5/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-06-4137-durga-saptashati-chapter5"><span class="">Durga Saptashati Chapter 5 - श्रीदुर्गासप्तशती - पञ्चमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-05-4136-durga-saptashati-chapter4-li">
  <a href="/site/docs/samskrutyatra/2020-06-05-4136-durga-saptashati-chapter4/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-05-4136-durga-saptashati-chapter4"><span class="">Durga Saptashati Chapter 4 - श्रीदुर्गासप्तशती - चतुर्थोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-04-4135-durga-saptashati-chapter3-li">
  <a href="/site/docs/samskrutyatra/2020-06-04-4135-durga-saptashati-chapter3/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-04-4135-durga-saptashati-chapter3"><span class="">Durga Saptashati Chapter 3 - श्रीदुर्गासप्तशती - तृतीयोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-03-4134-durga-saptashati-chapter2-li">
  <a href="/site/docs/samskrutyatra/2020-06-03-4134-durga-saptashati-chapter2/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-03-4134-durga-saptashati-chapter2"><span class="">Durga Saptashati Chapter 2 - श्रीदुर्गासप्तशती - द्वितीयोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-06-02-4133-durga-saptashati-chapter1-li">
  <a href="/site/docs/samskrutyatra/2020-06-02-4133-durga-saptashati-chapter1/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-06-02-4133-durga-saptashati-chapter1"><span class="">Durga Saptashati Chapter 1 - श्रीदुर्गासप्तशती - प्रथमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-30-4132-song-of-sanyasi-li">
  <a href="/site/docs/samskrutyatra/2020-05-30-4132-song-of-sanyasi/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-30-4132-song-of-sanyasi"><span class="">Song of Sanyasi  - सन्यासी का गीत</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-27-4131-jai-sagun-nirgun-roop-li">
  <a href="/site/docs/samskrutyatra/2020-05-27-4131-jai-sagun-nirgun-roop/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-27-4131-jai-sagun-nirgun-roop"><span class="">Jai Sagun Nirgun Roop  - जय सगुन निर्गुन रूप</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-24-4129-mundakoupnishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-24-4129-mundakoupnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-24-4129-mundakoupnishad"><span class="">Mundakoupnishad  - मुण्डकोपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-24-4130-shvetashvatara-upanishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-24-4130-shvetashvatara-upanishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-24-4130-shvetashvatara-upanishad"><span class="">Shvetashvatara Upanishad - श्वेताश्वतरोपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4121-chandogyopnishad-1-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4121-chandogyopnishad-1/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4121-chandogyopnishad-1"><span class="">Chandogyopnishad 1 - छान्दोग्योपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4122-chandogyopnishad-2-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4122-chandogyopnishad-2/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4122-chandogyopnishad-2"><span class="">Chandogyopnishad 2 - छान्दोग्योपनिषत् द्वितीयोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4123-chandogyopnishad-3-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4123-chandogyopnishad-3/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4123-chandogyopnishad-3"><span class="">Chandogyopnishad 3 - छान्दोग्योपनिषत् तृतीयोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4124-chandogyopnishad-4-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4124-chandogyopnishad-4/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4124-chandogyopnishad-4"><span class="">Chandogyopnishad 4 - छान्दोग्योपनिषत् चतुर्थोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4125-chandogyopnishad-5-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4125-chandogyopnishad-5/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4125-chandogyopnishad-5"><span class="">Chandogyopnishad 5 - छान्दोग्योपनिषत् पञ्चमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4126-chandogyopnishad-6-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4126-chandogyopnishad-6/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4126-chandogyopnishad-6"><span class="">Chandogyopnishad 6 - छान्दोग्योपनिषत् षष्ठोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4127-chandogyopnishad-7-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4127-chandogyopnishad-7/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4127-chandogyopnishad-7"><span class="">Chandogyopnishad 7 - छान्दोग्योपनिषत् सप्तमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-21-4128-chandogyopnishad-8-li">
  <a href="/site/docs/samskrutyatra/2020-05-21-4128-chandogyopnishad-8/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-21-4128-chandogyopnishad-8"><span class="">Chandogyopnishad 8 - छान्दोग्योपनिषत् अष्टमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-20-4120-mandukyopnishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-20-4120-mandukyopnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-20-4120-mandukyopnishad"><span class="">Mandukyopnishad  - माण्डूक्योपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-17-4117-jai-raama-sada-sada-sukhdham-hare-li">
  <a href="/site/docs/samskrutyatra/2020-05-17-4117-jai-raama-sada-sada-sukhdham-hare/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-17-4117-jai-raama-sada-sada-sukhdham-hare"><span class="">Jai Raama Sada Sada Sukhdham Hare - जय राम सदा सुखधाम</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-17-4118-jai-raama-sobha-dham-li">
  <a href="/site/docs/samskrutyatra/2020-05-17-4118-jai-raama-sobha-dham/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-17-4118-jai-raama-sobha-dham"><span class="">Jai Raama Sobha Dham - जय राम सोभा धाम</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-15-4113-ishaavashyopnishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-15-4113-ishaavashyopnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-15-4113-ishaavashyopnishad"><span class="">IshaaVashyopnishad  - ईशावास्योपषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-15-4114-rudraghan-paath-li">
  <a href="/site/docs/samskrutyatra/2020-05-15-4114-rudraghan-paath/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-15-4114-rudraghan-paath"><span class="">Rudraghan Paath - रुद्रघन पाठ</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-12-4108-gunasagar-nirguna-li">
  <a href="/site/docs/samskrutyatra/2020-05-12-4108-gunasagar-nirguna/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-12-4108-gunasagar-nirguna"><span class="">Gunasagar Nirguna Paramesha - गुणसागर निर्गुण परमेशा</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-12-4109-naam-ramayan-li">
  <a href="/site/docs/samskrutyatra/2020-05-12-4109-naam-ramayan/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-12-4109-naam-ramayan"><span class="">Naam Ramayan  - नाम रामायण</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-11-4106-shrisuktam-li">
  <a href="/site/docs/samskrutyatra/2020-05-11-4106-shrisuktam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-11-4106-shrisuktam"><span class="">Shrisuktam - श्रीसूक्तम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-10-4104-navgruhsuktam-li">
  <a href="/site/docs/samskrutyatra/2020-05-10-4104-navgruhsuktam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-10-4104-navgruhsuktam"><span class="">Navgruhsuktam - नवग्रहसूक्तम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-09-4102-narayanopnishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-09-4102-narayanopnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-09-4102-narayanopnishad"><span class="">Narayanopnishad - नारायणोपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4089-aditya-hridayam-stotram-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4089-aditya-hridayam-stotram/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4089-aditya-hridayam-stotram"><span class="">AdityaHrudyam Stotram - आदित्यहृदयम् स्त्रोत्रं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4092-durgatinaashinidurge-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4092-durgatinaashinidurge/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4092-durgatinaashinidurge"><span class="">Durgati Naashini Durge - दुर्गतिनाशिनी दुर्गे</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4093-jai-jai-surnayak-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4093-jai-jai-surnayak/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4093-jai-jai-surnayak"><span class="">Jai Jai Surnayak - जय जय सुरनायक</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4094-jai-raam-rama-ramanam-samanam-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4094-jai-raam-rama-ramanam-samanam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4094-jai-raam-rama-ramanam-samanam"><span class="">Jai Raam Rama Ramanam Samanam - जय राम रमा रमनं समनं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4095-kenopnishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4095-kenopnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4095-kenopnishad"><span class="">Kenopnishad - केनोपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4096-parsat-pad-paavan-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4096-parsat-pad-paavan/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4096-parsat-pad-paavan"><span class="">Parsat Pad Paavan - परसत पद पावन</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4097-ramchandra-stavan-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4097-ramchandra-stavan/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4097-ramchandra-stavan"><span class="">Ramchandra Stavan - रामचन्द्र स्तवन</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4098-ramchandra-stuti-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4098-ramchandra-stuti/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4098-ramchandra-stuti"><span class="">Ramchandra Stuti  - श्री राम स्तुति:</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4091-devi-suktam-saswara-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4091-devi-suktam-saswara/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4091-devi-suktam-saswara"><span class="">Rigvediya DeviSuktam - देवीसूक्तम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4099-shankarya-mangalam-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4099-shankarya-mangalam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4099-shankarya-mangalam"><span class="">Shankarya Mangalam - शंकराय मंगलं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-08-4100-shiv-jai-jai-kaar-li">
  <a href="/site/docs/samskrutyatra/2020-05-08-4100-shiv-jai-jai-kaar/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-08-4100-shiv-jai-jai-kaar"><span class="">शिव जयजयकार स्तोत्रं - शिव जयजयकार स्तोत्रं</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4082-aitreyopnishad-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4082-aitreyopnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4082-aitreyopnishad"><span class="">Aitreyopnishad - ऐतरेयोपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4084-bhavani-ashtakam-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4084-bhavani-ashtakam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4084-bhavani-ashtakam"><span class="">Bhavani Ashtakam - भवान्यष्टकम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4085-devi-kshama-prarthana-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4085-devi-kshama-prarthana/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4085-devi-kshama-prarthana"><span class="">Devi Kshama Prarthana - क्षमा-प्रार्थना</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4086-guru-stotram-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4086-guru-stotram/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4086-guru-stotram"><span class="">Guru Stotram  - गुरु स्तोत्रम्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4087-hanuman-chalisa-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4087-hanuman-chalisa/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4087-hanuman-chalisa"><span class="">Hanuman Chalisa - हनुमान चालीसा</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4080-kabirdaas-dohe-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4080-kabirdaas-dohe/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4080-kabirdaas-dohe"><span class="">Kabirdaas Dohe - कबीरदास दोहे</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4081-lakshmi-nrusimha-karavalamba-stotram-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4081-lakshmi-nrusimha-karavalamba-stotram/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4081-lakshmi-nrusimha-karavalamba-stotram"><span class="">Lakshmi Nrusimha Karavalamba Stotram - लक्ष्मीनृसिंह करावलम्ब स्तोत्र</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-07-4088-sankat-mochan-hanumanasht-li">
  <a href="/site/docs/samskrutyatra/2020-05-07-4088-sankat-mochan-hanumanasht/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-07-4088-sankat-mochan-hanumanasht"><span class="">Sankat Mochan HanumanAshtakam  - संकटमोचन हनुमानाष्टक</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-06-4115-bhagwat-gita-chapter-16-li">
  <a href="/site/docs/samskrutyatra/2020-05-06-4115-bhagwat-gita-chapter-16/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-06-4115-bhagwat-gita-chapter-16"><span class="">Bhagwat Gita Chapter 16 - श्रीमद्भगवद्गीता षोडशोऽध्यायः - दैवासुरसम्पद्विभागयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-06-4116-bhagwat-gita-chapter-17-li">
  <a href="/site/docs/samskrutyatra/2020-05-06-4116-bhagwat-gita-chapter-17/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-06-4116-bhagwat-gita-chapter-17"><span class="">Bhagwat Gita Chapter 17 - श्रीमद्भगवद्गीता सप्तदशोऽध्यायः - श्रद्धात्रयविभागयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-06-4119-bhagwat-gita-chapter-18-li">
  <a href="/site/docs/samskrutyatra/2020-05-06-4119-bhagwat-gita-chapter-18/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-06-4119-bhagwat-gita-chapter-18"><span class="">Bhagwat Gita Chapter 18 - श्रीमद्भगवद्गीता अथाष्टादशोऽध्यायः - मोक्षसंन्यासयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-05-4110-bhagwat-gita-chapter-13-li">
  <a href="/site/docs/samskrutyatra/2020-05-05-4110-bhagwat-gita-chapter-13/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-05-4110-bhagwat-gita-chapter-13"><span class="">Bhagwat Gita Chapter 13 - श्रीमद्भगवद्गीता त्रयोदशोऽध्यायः - क्षेत्रक्षेत्रज्ञविभागयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-05-4111-bhagwat-gita-chapter-14-li">
  <a href="/site/docs/samskrutyatra/2020-05-05-4111-bhagwat-gita-chapter-14/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-05-4111-bhagwat-gita-chapter-14"><span class="">Bhagwat Gita Chapter 14 - श्रीमद्भगवद्गीता चतुर्दशोऽध्यायः - गुणत्रयविभागयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-05-4112-bhagwat-gita-chapter-15-li">
  <a href="/site/docs/samskrutyatra/2020-05-05-4112-bhagwat-gita-chapter-15/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-05-4112-bhagwat-gita-chapter-15"><span class="">Bhagwat Gita Chapter 15 - श्रीमद्भगवद्गीता पञ्चदशोऽध्यायः - पुरुषोत्तमयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-04-4103-bhagwat-gita-chapter-10-li">
  <a href="/site/docs/samskrutyatra/2020-05-04-4103-bhagwat-gita-chapter-10/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-04-4103-bhagwat-gita-chapter-10"><span class="">Bhagwat Gita Chapter 10 - श्रीमद्भगवद्गीता दशमोऽध्यायः - विभूतियोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-04-4105-bhagwat-gita-chapter-11-li">
  <a href="/site/docs/samskrutyatra/2020-05-04-4105-bhagwat-gita-chapter-11/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-04-4105-bhagwat-gita-chapter-11"><span class="">Bhagwat Gita Chapter 11 - श्रीमद्भगवद्गीता एकादशोऽध्यायः - विश्वरूपदर्शनयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-04-4107-bhagwat-gita-chapter-12-li">
  <a href="/site/docs/samskrutyatra/2020-05-04-4107-bhagwat-gita-chapter-12/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-04-4107-bhagwat-gita-chapter-12"><span class="">Bhagwat Gita Chapter 12 - श्रीमद्भगवद्गीता द्वादशोऽध्यायः - भक्तियोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-03-4083-bhagwat-gita-chapter-7-li">
  <a href="/site/docs/samskrutyatra/2020-05-03-4083-bhagwat-gita-chapter-7/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-03-4083-bhagwat-gita-chapter-7"><span class="">Bhagwat Gita Chapter 7 - श्रीमद्भगवद्गीता सप्तमोऽध्यायः - ज्ञानविज्ञानयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-03-4090-bhagwat-gita-chapter-8-li">
  <a href="/site/docs/samskrutyatra/2020-05-03-4090-bhagwat-gita-chapter-8/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-03-4090-bhagwat-gita-chapter-8"><span class="">Bhagwat Gita Chapter 8 - श्रीमद्भगवद्गीता अष्टमोऽध्यायः - अक्षरब्रह्मयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-03-4101-bhagwat-gita-chapter-9-li">
  <a href="/site/docs/samskrutyatra/2020-05-03-4101-bhagwat-gita-chapter-9/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-03-4101-bhagwat-gita-chapter-9"><span class="">Bhagwat Gita Chapter 9 - श्रीमद्भगवद्गीता नवमोऽध्यायः - राजविद्याराजगुह्ययोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-02-4075-bhagwat-gita-chapter-4-li">
  <a href="/site/docs/samskrutyatra/2020-05-02-4075-bhagwat-gita-chapter-4/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-02-4075-bhagwat-gita-chapter-4"><span class="">Bhagwat Gita Chapter 4- श्रीमद्भगवद्गीता चतुर्थोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-02-4077-bhagwat-gita-chapter-5-li">
  <a href="/site/docs/samskrutyatra/2020-05-02-4077-bhagwat-gita-chapter-5/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-02-4077-bhagwat-gita-chapter-5"><span class="">Bhagwat Gita Chapter 5 - श्रीमद्भगवद्गीता पञ्चमोऽध्यायः - संन्यासयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-02-4079-bhagwat-gita-chapter-6-li">
  <a href="/site/docs/samskrutyatra/2020-05-02-4079-bhagwat-gita-chapter-6/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-02-4079-bhagwat-gita-chapter-6"><span class="">Bhagwat Gita Chapter 6 - श्रीमद्भगवद्गीता षष्ठोऽध्यायः - ध्यानयोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-01-4068-bhagwat-gita-chapter-1-li">
  <a href="/site/docs/samskrutyatra/2020-05-01-4068-bhagwat-gita-chapter-1/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-01-4068-bhagwat-gita-chapter-1"><span class="">Bhagwat Gita Chapter 1 - श्रीमद्भगवद्गीता प्रथमोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-01-4070-bhagwat-gita-chapter-2-li">
  <a href="/site/docs/samskrutyatra/2020-05-01-4070-bhagwat-gita-chapter-2/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-01-4070-bhagwat-gita-chapter-2"><span class="">Bhagwat Gita Chapter 2 - अथ द्वितीयोऽध्यायः - साङ्ख्ययोगः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-05-01-4073-bhagwat-gita-chapter-3-li">
  <a href="/site/docs/samskrutyatra/2020-05-01-4073-bhagwat-gita-chapter-3/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-05-01-4073-bhagwat-gita-chapter-3"><span class="">Bhagwat Gita Chapter 3 - श्रीमद्भगवद्गीता तृतीयोऽध्यायः</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-04-30-4076-aatmopnishad-li">
  <a href="/site/docs/samskrutyatra/2020-04-30-4076-aatmopnishad/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-04-30-4076-aatmopnishad"><span class="">Aatmopnishad - आत्मोपनिषत्</span></a>
</li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id="m-sitedocssamskrutyatra2020-04-30-4069-achyutashtakam-acyutam-keshavam-ramanarayanam-li">
  <a href="/site/docs/samskrutyatra/2020-04-30-4069-achyutashtakam-acyutam-keshavam-ramanarayanam/" class="align-left ps-0 td-sidebar-link td-sidebar-link__page" id="m-sitedocssamskrutyatra2020-04-30-4069-achyutashtakam-acyutam-keshavam-ramanarayanam"><span class="">Achyutashtakam  - अच्युतस्याष्टकम्</span></a>
</li>
  </ul>
</li>
  </ul>
</li>
    </ul>
  </nav>
</div>

          </aside>
          <aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none">
            

  





<div class="td-page-meta ml-2 pb-1 pt-2 mb-0">








<a href="https://github.com/googleforgames/agones/edit/main/site/content/en/docs%5cdsblog%5c2021-09-29-6011-300-Important-Statistical-Terms.md" data-proofer-ignore target="_blank"><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/googleforgames/agones/issues/new?title=300%20Important%20Statistical%20Terms&amp;body=%0a%20%20**Link%20to%20page**:%20%0a%20%20http://localhost:1313/site/docs/dsblog/2021-09-29-6011-300-important-statistical-terms/%0a%20%20%0a%20%20**Description%20of%20documentation%20issue**:%0a%20%20A%20clear%20and%20concise%20description%20of%20what%20the%20problem%20is%20with%20this%20documentation%20-%20e.g.%20%22I%20find%20it%20hard%20to%20understand....%22%0a%20%20%0a%20%20**What%20improvements%20to%20documentation%20would%20you%20like%20to%20see?**:%0a%20%20A%20clear%20and%20concise%20description%20of%20what%20modifications%20or%20additions%20to%20documentation%20you%20would%20like%20to%20see.%0a%0a&amp;labels=kind/documentation,area/site" target="_blank"><i class="fab fa-github fa-fw"></i> Create documentation issue</a>
</div>



            
            <div class="taxonomy taxonomy-terms-cloud taxo-categories">
      <h5 class="taxonomy-title">Categories</h5>
      <ul class="taxonomy-terms">
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/ai/" data-taxonomy-term="ai"><span class="taxonomy-label">AI</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/ai-in-space/" data-taxonomy-term="ai-in-space"><span class="taxonomy-label">AI in Space</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/ai-and-computer-vision/" data-taxonomy-term="ai-and-computer-vision"><span class="taxonomy-label">Ai-and-Computer-Vision</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/ai-and-machine-learning/" data-taxonomy-term="ai-and-machine-learning"><span class="taxonomy-label">Ai-and-Machine-Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/ai-and-nlp/" data-taxonomy-term="ai-and-nlp"><span class="taxonomy-label">Ai-and-Nlp</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/biology/" data-taxonomy-term="biology"><span class="taxonomy-label">Biology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/chanting/" data-taxonomy-term="chanting"><span class="taxonomy-label">Chanting</span><span class="taxonomy-count">48</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/cloud-computing/" data-taxonomy-term="cloud-computing"><span class="taxonomy-label">Cloud Computing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/code-review/" data-taxonomy-term="code-review"><span class="taxonomy-label">Code Review</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/coderabbit/" data-taxonomy-term="coderabbit"><span class="taxonomy-label">Coderabbit</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/computer-vision/" data-taxonomy-term="computer-vision"><span class="taxonomy-label">Computer-Vision</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/cybersecurity/" data-taxonomy-term="cybersecurity"><span class="taxonomy-label">Cybersecurity</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/databases/" data-taxonomy-term="databases"><span class="taxonomy-label">Databases</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/design/" data-taxonomy-term="design"><span class="taxonomy-label">Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/development/" data-taxonomy-term="development"><span class="taxonomy-label">Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/dsblog/" data-taxonomy-term="dsblog"><span class="taxonomy-label">Dsblog</span><span class="taxonomy-count">272</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/dsphilosophy/" data-taxonomy-term="dsphilosophy"><span class="taxonomy-label">Dsphilosophy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/dsresources/" data-taxonomy-term="dsresources"><span class="taxonomy-label">Dsresources</span><span class="taxonomy-count">28</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/electronics/" data-taxonomy-term="electronics"><span class="taxonomy-label">Electronics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/engineering/" data-taxonomy-term="engineering"><span class="taxonomy-label">Engineering</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/entrepreneurship/" data-taxonomy-term="entrepreneurship"><span class="taxonomy-label">Entrepreneurship</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/frontend/" data-taxonomy-term="frontend"><span class="taxonomy-label">Frontend</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/grok3model/" data-taxonomy-term="grok3model"><span class="taxonomy-label">Grok3Model</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/iot-and-industry-4.0/" data-taxonomy-term="iot-and-industry-4.0"><span class="taxonomy-label">Iot-and-Industry-4.0</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/life-sciences/" data-taxonomy-term="life-sciences"><span class="taxonomy-label">Life Sciences</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/machine-learning/" data-taxonomy-term="machine-learning"><span class="taxonomy-label">Machine-Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/math/" data-taxonomy-term="math"><span class="taxonomy-label">Math</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/media-and-ai/" data-taxonomy-term="media-and-ai"><span class="taxonomy-label">Media and AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/nlp/" data-taxonomy-term="nlp"><span class="taxonomy-label">Nlp</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/programming/" data-taxonomy-term="programming"><span class="taxonomy-label">Programming</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/sanskrit-and-ai/" data-taxonomy-term="sanskrit-and-ai"><span class="taxonomy-label">Sanskrit and AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/science/" data-taxonomy-term="science"><span class="taxonomy-label">Science</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/security-in-web-development/" data-taxonomy-term="security-in-web-development"><span class="taxonomy-label">Security in Web Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/signal-processing/" data-taxonomy-term="signal-processing"><span class="taxonomy-label">Signal-Processing</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/software-development/" data-taxonomy-term="software-development"><span class="taxonomy-label">Software-Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/technology/" data-taxonomy-term="technology"><span class="taxonomy-label">Technology</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/video-editing/" data-taxonomy-term="video-editing"><span class="taxonomy-label">Video Editing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/web-development/" data-taxonomy-term="web-development"><span class="taxonomy-label">Web Development</span><span class="taxonomy-count">3</span></a></li>
        </ul>
    </div>
  <div class="taxonomy taxonomy-terms-cloud taxo-tags">
      <h5 class="taxonomy-title">Tags</h5>
      <ul class="taxonomy-terms">
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/3d/" data-taxonomy-term="3d"><span class="taxonomy-label">3D</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/academic-research/" data-taxonomy-term="academic-research"><span class="taxonomy-label">Academic Research</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/academic-resources/" data-taxonomy-term="academic-resources"><span class="taxonomy-label">Academic Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/academic-writing/" data-taxonomy-term="academic-writing"><span class="taxonomy-label">Academic Writing</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/access-control-in-web-development/" data-taxonomy-term="access-control-in-web-development"><span class="taxonomy-label">Access Control in Web Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/adding-comments-to-jekyll/" data-taxonomy-term="adding-comments-to-jekyll"><span class="taxonomy-label">Adding Comments to Jekyll</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/advanced-css-techniques/" data-taxonomy-term="advanced-css-techniques"><span class="taxonomy-label">Advanced CSS Techniques</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/agents/" data-taxonomy-term="agents"><span class="taxonomy-label">Agents</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/agricultural-innovation/" data-taxonomy-term="agricultural-innovation"><span class="taxonomy-label">Agricultural Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/agriculture-technology/" data-taxonomy-term="agriculture-technology"><span class="taxonomy-label">Agriculture Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai/" data-taxonomy-term="ai"><span class="taxonomy-label">AI</span><span class="taxonomy-count">32</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-algorithms/" data-taxonomy-term="ai-algorithms"><span class="taxonomy-label">AI Algorithms</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-and-nlp/" data-taxonomy-term="ai-and-nlp"><span class="taxonomy-label">AI and NLP</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-and-sanskrit/" data-taxonomy-term="ai-and-sanskrit"><span class="taxonomy-label">AI and Sanskrit</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-applications/" data-taxonomy-term="ai-applications"><span class="taxonomy-label">AI Applications</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-architecture/" data-taxonomy-term="ai-architecture"><span class="taxonomy-label">AI Architecture</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-automation/" data-taxonomy-term="ai-automation"><span class="taxonomy-label">AI Automation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-benchmark-explorer/" data-taxonomy-term="ai-benchmark-explorer"><span class="taxonomy-label">AI Benchmark Explorer</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-benchmarks/" data-taxonomy-term="ai-benchmarks"><span class="taxonomy-label">AI Benchmarks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-benchmarks-2025/" data-taxonomy-term="ai-benchmarks-2025"><span class="taxonomy-label">AI Benchmarks 2025</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-blogs/" data-taxonomy-term="ai-blogs"><span class="taxonomy-label">AI Blogs</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-business-tools/" data-taxonomy-term="ai-business-tools"><span class="taxonomy-label">AI Business Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-capabilities/" data-taxonomy-term="ai-capabilities"><span class="taxonomy-label">AI Capabilities</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-code-debugging/" data-taxonomy-term="ai-code-debugging"><span class="taxonomy-label">AI Code Debugging</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-code-generation/" data-taxonomy-term="ai-code-generation"><span class="taxonomy-label">AI Code Generation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-code-reviewer/" data-taxonomy-term="ai-code-reviewer"><span class="taxonomy-label">AI Code Reviewer</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-code-understanding/" data-taxonomy-term="ai-code-understanding"><span class="taxonomy-label">AI Code Understanding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-coding-ides/" data-taxonomy-term="ai-coding-ides"><span class="taxonomy-label">AI Coding IDEs</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-companies/" data-taxonomy-term="ai-companies"><span class="taxonomy-label">AI Companies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-conferences/" data-taxonomy-term="ai-conferences"><span class="taxonomy-label">AI Conferences</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-conversation/" data-taxonomy-term="ai-conversation"><span class="taxonomy-label">AI Conversation</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-conversations-on-vedanta/" data-taxonomy-term="ai-conversations-on-vedanta"><span class="taxonomy-label">AI Conversations on Vedanta</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-customization/" data-taxonomy-term="ai-customization"><span class="taxonomy-label">AI Customization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-cybersecurity-defense/" data-taxonomy-term="ai-cybersecurity-defense"><span class="taxonomy-label">AI Cybersecurity Defense</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-cybersecurity-governance/" data-taxonomy-term="ai-cybersecurity-governance"><span class="taxonomy-label">AI Cybersecurity Governance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-cybersecurity-tools/" data-taxonomy-term="ai-cybersecurity-tools"><span class="taxonomy-label">AI Cybersecurity Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-democratization/" data-taxonomy-term="ai-democratization"><span class="taxonomy-label">AI Democratization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-development/" data-taxonomy-term="ai-development"><span class="taxonomy-label">AI Development</span><span class="taxonomy-count">9</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-development-tools/" data-taxonomy-term="ai-development-tools"><span class="taxonomy-label">AI Development Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-diary/" data-taxonomy-term="ai-diary"><span class="taxonomy-label">AI Diary</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-education/" data-taxonomy-term="ai-education"><span class="taxonomy-label">AI Education</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-efficiency/" data-taxonomy-term="ai-efficiency"><span class="taxonomy-label">AI Efficiency</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-ethics/" data-taxonomy-term="ai-ethics"><span class="taxonomy-label">AI Ethics</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-fundamentals/" data-taxonomy-term="ai-fundamentals"><span class="taxonomy-label">AI Fundamentals</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-governance/" data-taxonomy-term="ai-governance"><span class="taxonomy-label">AI Governance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-hardware/" data-taxonomy-term="ai-hardware"><span class="taxonomy-label">AI Hardware</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-history/" data-taxonomy-term="ai-history"><span class="taxonomy-label">AI History</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-implementation/" data-taxonomy-term="ai-implementation"><span class="taxonomy-label">AI Implementation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-cybersecurity/" data-taxonomy-term="ai-in-cybersecurity"><span class="taxonomy-label">AI in Cybersecurity</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-education/" data-taxonomy-term="ai-in-education"><span class="taxonomy-label">AI in Education</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-government/" data-taxonomy-term="ai-in-government"><span class="taxonomy-label">AI in Government</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-media/" data-taxonomy-term="ai-in-media"><span class="taxonomy-label">AI in Media</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-programming/" data-taxonomy-term="ai-in-programming"><span class="taxonomy-label">AI in Programming</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-sanskrit-studies/" data-taxonomy-term="ai-in-sanskrit-studies"><span class="taxonomy-label">AI in Sanskrit Studies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-in-space-exploration/" data-taxonomy-term="ai-in-space-exploration"><span class="taxonomy-label">AI in Space Exploration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-infrastructure/" data-taxonomy-term="ai-infrastructure"><span class="taxonomy-label">AI Infrastructure</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-innovation/" data-taxonomy-term="ai-innovation"><span class="taxonomy-label">AI Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-integration/" data-taxonomy-term="ai-integration"><span class="taxonomy-label">AI Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-interaction/" data-taxonomy-term="ai-interaction"><span class="taxonomy-label">AI Interaction</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-interview-questions/" data-taxonomy-term="ai-interview-questions"><span class="taxonomy-label">AI Interview Questions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-job-search/" data-taxonomy-term="ai-job-search"><span class="taxonomy-label">AI Job Search</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-language/" data-taxonomy-term="ai-language"><span class="taxonomy-label">AI Language</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-language-models/" data-taxonomy-term="ai-language-models"><span class="taxonomy-label">AI Language Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-comparison/" data-taxonomy-term="ai-model-comparison"><span class="taxonomy-label">AI Model Comparison</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-deployment/" data-taxonomy-term="ai-model-deployment"><span class="taxonomy-label">AI Model Deployment</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-development/" data-taxonomy-term="ai-model-development"><span class="taxonomy-label">AI Model Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-evaluation/" data-taxonomy-term="ai-model-evaluation"><span class="taxonomy-label">AI Model Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-evaluation-benchmarks/" data-taxonomy-term="ai-model-evaluation-benchmarks"><span class="taxonomy-label">AI Model Evaluation Benchmarks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-integration/" data-taxonomy-term="ai-model-integration"><span class="taxonomy-label">AI Model Integration</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-leaderboards/" data-taxonomy-term="ai-model-leaderboards"><span class="taxonomy-label">AI Model Leaderboards</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-management/" data-taxonomy-term="ai-model-management"><span class="taxonomy-label">AI Model Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-marketplaces/" data-taxonomy-term="ai-model-marketplaces"><span class="taxonomy-label">AI Model Marketplaces</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-reasoning/" data-taxonomy-term="ai-model-reasoning"><span class="taxonomy-label">AI Model Reasoning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-security/" data-taxonomy-term="ai-model-security"><span class="taxonomy-label">AI Model Security</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-model-selection/" data-taxonomy-term="ai-model-selection"><span class="taxonomy-label">AI Model Selection</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-models/" data-taxonomy-term="ai-models"><span class="taxonomy-label">AI Models</span><span class="taxonomy-count">8</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-paper/" data-taxonomy-term="ai-paper"><span class="taxonomy-label">AI Paper</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-pioneers/" data-taxonomy-term="ai-pioneers"><span class="taxonomy-label">AI Pioneers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-platform/" data-taxonomy-term="ai-platform"><span class="taxonomy-label">AI Platform</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-policy/" data-taxonomy-term="ai-policy"><span class="taxonomy-label">AI Policy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-predictions/" data-taxonomy-term="ai-predictions"><span class="taxonomy-label">AI Predictions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-problem-solving/" data-taxonomy-term="ai-problem-solving"><span class="taxonomy-label">AI Problem Solving</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-project-failure/" data-taxonomy-term="ai-project-failure"><span class="taxonomy-label">AI Project Failure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-project-ideas/" data-taxonomy-term="ai-project-ideas"><span class="taxonomy-label">AI Project Ideas</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-prompting/" data-taxonomy-term="ai-prompting"><span class="taxonomy-label">AI Prompting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-reasoning/" data-taxonomy-term="ai-reasoning"><span class="taxonomy-label">AI Reasoning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-research/" data-taxonomy-term="ai-research"><span class="taxonomy-label">AI Research</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-research-evaluation/" data-taxonomy-term="ai-research-evaluation"><span class="taxonomy-label">AI Research Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-research-papers/" data-taxonomy-term="ai-research-papers"><span class="taxonomy-label">AI Research Papers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-research-tools/" data-taxonomy-term="ai-research-tools"><span class="taxonomy-label">AI Research Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-researchers/" data-taxonomy-term="ai-researchers"><span class="taxonomy-label">AI Researchers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-resources/" data-taxonomy-term="ai-resources"><span class="taxonomy-label">AI Resources</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-safety/" data-taxonomy-term="ai-safety"><span class="taxonomy-label">AI Safety</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-scaling/" data-taxonomy-term="ai-scaling"><span class="taxonomy-label">AI Scaling</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-services/" data-taxonomy-term="ai-services"><span class="taxonomy-label">AI Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-solutions/" data-taxonomy-term="ai-solutions"><span class="taxonomy-label">AI Solutions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-system-evaluation/" data-taxonomy-term="ai-system-evaluation"><span class="taxonomy-label">AI System Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-technology/" data-taxonomy-term="ai-technology"><span class="taxonomy-label">AI Technology</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-terminology/" data-taxonomy-term="ai-terminology"><span class="taxonomy-label">AI Terminology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-tools/" data-taxonomy-term="ai-tools"><span class="taxonomy-label">AI Tools</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-transformers/" data-taxonomy-term="ai-transformers"><span class="taxonomy-label">AI Transformers</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-transparency/" data-taxonomy-term="ai-transparency"><span class="taxonomy-label">AI Transparency</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-trends/" data-taxonomy-term="ai-trends"><span class="taxonomy-label">AI Trends</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-use-cases/" data-taxonomy-term="ai-use-cases"><span class="taxonomy-label">AI Use Cases</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ai-usecases/" data-taxonomy-term="ai-usecases"><span class="taxonomy-label">AI Usecases</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/algorithm-analysis/" data-taxonomy-term="algorithm-analysis"><span class="taxonomy-label">Algorithm Analysis</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/algorithm-design/" data-taxonomy-term="algorithm-design"><span class="taxonomy-label">Algorithm Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/algorithms/" data-taxonomy-term="algorithms"><span class="taxonomy-label">Algorithms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/alphafold/" data-taxonomy-term="alphafold"><span class="taxonomy-label">AlphaFold</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/amazon-web-services/" data-taxonomy-term="amazon-web-services"><span class="taxonomy-label">Amazon Web Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/analog-to-digital-conversion/" data-taxonomy-term="analog-to-digital-conversion"><span class="taxonomy-label">Analog to Digital Conversion</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/analytical-thinking/" data-taxonomy-term="analytical-thinking"><span class="taxonomy-label">Analytical Thinking</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ancient-knowledge/" data-taxonomy-term="ancient-knowledge"><span class="taxonomy-label">Ancient Knowledge</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ancient-texts/" data-taxonomy-term="ancient-texts"><span class="taxonomy-label">Ancient Texts</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/android-development/" data-taxonomy-term="android-development"><span class="taxonomy-label">Android Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/apache-hive/" data-taxonomy-term="apache-hive"><span class="taxonomy-label">Apache Hive</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/api-design/" data-taxonomy-term="api-design"><span class="taxonomy-label">API Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/api-integration/" data-taxonomy-term="api-integration"><span class="taxonomy-label">API Integration</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/api-vs-library/" data-taxonomy-term="api-vs-library"><span class="taxonomy-label">API vs Library</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/apis/" data-taxonomy-term="apis"><span class="taxonomy-label">APIs</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/app-development/" data-taxonomy-term="app-development"><span class="taxonomy-label">App Development</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/application-integration/" data-taxonomy-term="application-integration"><span class="taxonomy-label">Application Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/argumentation/" data-taxonomy-term="argumentation"><span class="taxonomy-label">Argumentation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/arima-models/" data-taxonomy-term="arima-models"><span class="taxonomy-label">ARIMA Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/artificial-intelligence/" data-taxonomy-term="artificial-intelligence"><span class="taxonomy-label">Artificial Intelligence</span><span class="taxonomy-count">11</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/asset/" data-taxonomy-term="asset"><span class="taxonomy-label">Asset</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/astrology/" data-taxonomy-term="astrology"><span class="taxonomy-label">Astrology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/astronomy/" data-taxonomy-term="astronomy"><span class="taxonomy-label">Astronomy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/asynchronous-programming/" data-taxonomy-term="asynchronous-programming"><span class="taxonomy-label">Asynchronous Programming</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/attention/" data-taxonomy-term="attention"><span class="taxonomy-label">Attention</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/attention-mechanism/" data-taxonomy-term="attention-mechanism"><span class="taxonomy-label">Attention Mechanism</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/audio-processing/" data-taxonomy-term="audio-processing"><span class="taxonomy-label">Audio Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/authentication/" data-taxonomy-term="authentication"><span class="taxonomy-label">Authentication</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/authentication-and-authorization/" data-taxonomy-term="authentication-and-authorization"><span class="taxonomy-label">Authentication and Authorization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/authentication-vs-authorization/" data-taxonomy-term="authentication-vs-authorization"><span class="taxonomy-label">Authentication vs Authorization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/automate-code-reviews/" data-taxonomy-term="automate-code-reviews"><span class="taxonomy-label">Automate Code Reviews</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/automated-evaluation/" data-taxonomy-term="automated-evaluation"><span class="taxonomy-label">Automated Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/automation/" data-taxonomy-term="automation"><span class="taxonomy-label">Automation</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/automl/" data-taxonomy-term="automl"><span class="taxonomy-label">AutoML</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/autoregressive-models/" data-taxonomy-term="autoregressive-models"><span class="taxonomy-label">Autoregressive Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/aws/" data-taxonomy-term="aws"><span class="taxonomy-label">AWS</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/aws-cloud/" data-taxonomy-term="aws-cloud"><span class="taxonomy-label">AWS Cloud</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/aws-platform/" data-taxonomy-term="aws-platform"><span class="taxonomy-label">AWS Platform</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure/" data-taxonomy-term="azure"><span class="taxonomy-label">Azure</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure-ai/" data-taxonomy-term="azure-ai"><span class="taxonomy-label">Azure AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure-cloud/" data-taxonomy-term="azure-cloud"><span class="taxonomy-label">Azure Cloud</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure-cognitive-services/" data-taxonomy-term="azure-cognitive-services"><span class="taxonomy-label">Azure Cognitive Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure-machine-learning/" data-taxonomy-term="azure-machine-learning"><span class="taxonomy-label">Azure Machine Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure-openai-service/" data-taxonomy-term="azure-openai-service"><span class="taxonomy-label">Azure OpenAI Service</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/azure-platform/" data-taxonomy-term="azure-platform"><span class="taxonomy-label">Azure Platform</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/backend-development/" data-taxonomy-term="backend-development"><span class="taxonomy-label">Backend Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/banking/" data-taxonomy-term="banking"><span class="taxonomy-label">Banking</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/basic-mathematics/" data-taxonomy-term="basic-mathematics"><span class="taxonomy-label">Basic Mathematics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/basic-statistics/" data-taxonomy-term="basic-statistics"><span class="taxonomy-label">Basic Statistics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/batch-files/" data-taxonomy-term="batch-files"><span class="taxonomy-label">Batch Files</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/behavioral-economics/" data-taxonomy-term="behavioral-economics"><span class="taxonomy-label">Behavioral Economics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/behavioral-psychology/" data-taxonomy-term="behavioral-psychology"><span class="taxonomy-label">Behavioral Psychology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/benchmarks/" data-taxonomy-term="benchmarks"><span class="taxonomy-label">Benchmarks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bert/" data-taxonomy-term="bert"><span class="taxonomy-label">BERT</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bert-models/" data-taxonomy-term="bert-models"><span class="taxonomy-label">BERT Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/best-practices/" data-taxonomy-term="best-practices"><span class="taxonomy-label">Best Practices</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bhagwat-gita/" data-taxonomy-term="bhagwat-gita"><span class="taxonomy-label">Bhagwat Gita</span><span class="taxonomy-count">18</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bhajan/" data-taxonomy-term="bhajan"><span class="taxonomy-label">Bhajan</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bharat-llm/" data-taxonomy-term="bharat-llm"><span class="taxonomy-label">Bharat LLM</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/big-data/" data-taxonomy-term="big-data"><span class="taxonomy-label">Big Data</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/biology/" data-taxonomy-term="biology"><span class="taxonomy-label">Biology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bitnet/" data-taxonomy-term="bitnet"><span class="taxonomy-label">BitNet</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/book-review/" data-taxonomy-term="book-review"><span class="taxonomy-label">Book Review</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/book-summaries/" data-taxonomy-term="book-summaries"><span class="taxonomy-label">Book Summaries</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bookmarked-articles/" data-taxonomy-term="bookmarked-articles"><span class="taxonomy-label">Bookmarked Articles</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/bot-prevention/" data-taxonomy-term="bot-prevention"><span class="taxonomy-label">Bot Prevention</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/branches-of-biology/" data-taxonomy-term="branches-of-biology"><span class="taxonomy-label">Branches of Biology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/business-ai-adoption/" data-taxonomy-term="business-ai-adoption"><span class="taxonomy-label">Business AI Adoption</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/business-analytics/" data-taxonomy-term="business-analytics"><span class="taxonomy-label">Business Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/business-applications/" data-taxonomy-term="business-applications"><span class="taxonomy-label">Business Applications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/business-communication/" data-taxonomy-term="business-communication"><span class="taxonomy-label">Business Communication</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/business-innovation/" data-taxonomy-term="business-innovation"><span class="taxonomy-label">Business Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/business-models/" data-taxonomy-term="business-models"><span class="taxonomy-label">Business Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/calculus/" data-taxonomy-term="calculus"><span class="taxonomy-label">Calculus</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/callbacks/" data-taxonomy-term="callbacks"><span class="taxonomy-label">Callbacks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/career-development/" data-taxonomy-term="career-development"><span class="taxonomy-label">Career Development</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/celestial-objects/" data-taxonomy-term="celestial-objects"><span class="taxonomy-label">Celestial Objects</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chandogya-upanishad/" data-taxonomy-term="chandogya-upanishad"><span class="taxonomy-label">Chandogya Upanishad</span><span class="taxonomy-count">8</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chanting/" data-taxonomy-term="chanting"><span class="taxonomy-label">Chanting</span><span class="taxonomy-count">51</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/character-encoding/" data-taxonomy-term="character-encoding"><span class="taxonomy-label">Character Encoding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/characteristics-of-life/" data-taxonomy-term="characteristics-of-life"><span class="taxonomy-label">Characteristics of Life</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chatbot/" data-taxonomy-term="chatbot"><span class="taxonomy-label">Chatbot</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chatbots/" data-taxonomy-term="chatbots"><span class="taxonomy-label">Chatbots</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chatgpt/" data-taxonomy-term="chatgpt"><span class="taxonomy-label">ChatGPT</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chinese-ai/" data-taxonomy-term="chinese-ai"><span class="taxonomy-label">Chinese AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chrome-extensions/" data-taxonomy-term="chrome-extensions"><span class="taxonomy-label">Chrome Extensions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/chrome-tools/" data-taxonomy-term="chrome-tools"><span class="taxonomy-label">Chrome Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ci/cd/" data-taxonomy-term="ci/cd"><span class="taxonomy-label">CI/CD</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/classification-algorithms/" data-taxonomy-term="classification-algorithms"><span class="taxonomy-label">Classification Algorithms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cli/" data-taxonomy-term="cli"><span class="taxonomy-label">CLI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cli-vs-gui/" data-taxonomy-term="cli-vs-gui"><span class="taxonomy-label">CLI vs GUI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/clinical-ai/" data-taxonomy-term="clinical-ai"><span class="taxonomy-label">Clinical AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/closed-source/" data-taxonomy-term="closed-source"><span class="taxonomy-label">Closed Source</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud/" data-taxonomy-term="cloud"><span class="taxonomy-label">Cloud</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-ai/" data-taxonomy-term="cloud-ai"><span class="taxonomy-label">Cloud AI</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-ai-services/" data-taxonomy-term="cloud-ai-services"><span class="taxonomy-label">Cloud AI Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-architecture/" data-taxonomy-term="cloud-architecture"><span class="taxonomy-label">Cloud Architecture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-automl/" data-taxonomy-term="cloud-automl"><span class="taxonomy-label">Cloud AutoML</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-computing/" data-taxonomy-term="cloud-computing"><span class="taxonomy-label">Cloud Computing</span><span class="taxonomy-count">11</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-databases/" data-taxonomy-term="cloud-databases"><span class="taxonomy-label">Cloud Databases</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-development/" data-taxonomy-term="cloud-development"><span class="taxonomy-label">Cloud Development</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-hosting/" data-taxonomy-term="cloud-hosting"><span class="taxonomy-label">Cloud Hosting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-infrastructure/" data-taxonomy-term="cloud-infrastructure"><span class="taxonomy-label">Cloud Infrastructure</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-integration/" data-taxonomy-term="cloud-integration"><span class="taxonomy-label">Cloud Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-services/" data-taxonomy-term="cloud-services"><span class="taxonomy-label">Cloud Services</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-solutions/" data-taxonomy-term="cloud-solutions"><span class="taxonomy-label">Cloud Solutions</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-technology/" data-taxonomy-term="cloud-technology"><span class="taxonomy-label">Cloud Technology</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cloud-based-proprietary-models/" data-taxonomy-term="cloud-based-proprietary-models"><span class="taxonomy-label">Cloud-Based Proprietary Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cnn/" data-taxonomy-term="cnn"><span class="taxonomy-label">CNN</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/code-editors/" data-taxonomy-term="code-editors"><span class="taxonomy-label">Code Editors</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/code-optimization/" data-taxonomy-term="code-optimization"><span class="taxonomy-label">Code Optimization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/code-resources/" data-taxonomy-term="code-resources"><span class="taxonomy-label">Code Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/code-review-ai/" data-taxonomy-term="code-review-ai"><span class="taxonomy-label">Code Review AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/code-style/" data-taxonomy-term="code-style"><span class="taxonomy-label">Code Style</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/coderabbit-ai/" data-taxonomy-term="coderabbit-ai"><span class="taxonomy-label">CodeRabbit AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/coderabbit-alternative/" data-taxonomy-term="coderabbit-alternative"><span class="taxonomy-label">CodeRabbit Alternative</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/coding-playgrounds/" data-taxonomy-term="coding-playgrounds"><span class="taxonomy-label">Coding Playgrounds</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/coding-resources/" data-taxonomy-term="coding-resources"><span class="taxonomy-label">Coding Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cognitive-computing/" data-taxonomy-term="cognitive-computing"><span class="taxonomy-label">Cognitive Computing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cognitive-psychology/" data-taxonomy-term="cognitive-psychology"><span class="taxonomy-label">Cognitive Psychology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cognitive-science/" data-taxonomy-term="cognitive-science"><span class="taxonomy-label">Cognitive Science</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/collaboration-tools/" data-taxonomy-term="collaboration-tools"><span class="taxonomy-label">Collaboration Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/collections/" data-taxonomy-term="collections"><span class="taxonomy-label">Collections</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/command-line/" data-taxonomy-term="command-line"><span class="taxonomy-label">Command-Line</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/commenting-system-in-jekyll/" data-taxonomy-term="commenting-system-in-jekyll"><span class="taxonomy-label">Commenting System in Jekyll</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/communication/" data-taxonomy-term="communication"><span class="taxonomy-label">Communication</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/computational-linguistics/" data-taxonomy-term="computational-linguistics"><span class="taxonomy-label">Computational Linguistics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/computer-science/" data-taxonomy-term="computer-science"><span class="taxonomy-label">Computer Science</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/computer-vision/" data-taxonomy-term="computer-vision"><span class="taxonomy-label">Computer Vision</span><span class="taxonomy-count">13</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/computer-vision-model-evaluation/" data-taxonomy-term="computer-vision-model-evaluation"><span class="taxonomy-label">Computer Vision Model Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/computing-infrastructure/" data-taxonomy-term="computing-infrastructure"><span class="taxonomy-label">Computing Infrastructure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/computing-resources/" data-taxonomy-term="computing-resources"><span class="taxonomy-label">Computing Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/configuration-management/" data-taxonomy-term="configuration-management"><span class="taxonomy-label">Configuration Management</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/conflicting-evidence/" data-taxonomy-term="conflicting-evidence"><span class="taxonomy-label">Conflicting Evidence</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/confusion-matrix/" data-taxonomy-term="confusion-matrix"><span class="taxonomy-label">Confusion Matrix</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/consciousness/" data-taxonomy-term="consciousness"><span class="taxonomy-label">Consciousness</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/consciousness-research/" data-taxonomy-term="consciousness-research"><span class="taxonomy-label">Consciousness Research</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/consciousness-studies/" data-taxonomy-term="consciousness-studies"><span class="taxonomy-label">Consciousness Studies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/container-images/" data-taxonomy-term="container-images"><span class="taxonomy-label">Container Images</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/container-management/" data-taxonomy-term="container-management"><span class="taxonomy-label">Container Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/container-orchestration/" data-taxonomy-term="container-orchestration"><span class="taxonomy-label">Container Orchestration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/container-registry/" data-taxonomy-term="container-registry"><span class="taxonomy-label">Container Registry</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/containerization/" data-taxonomy-term="containerization"><span class="taxonomy-label">Containerization</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/content-creation/" data-taxonomy-term="content-creation"><span class="taxonomy-label">Content Creation</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/continuous-integration/" data-taxonomy-term="continuous-integration"><span class="taxonomy-label">Continuous Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cost-savings/" data-taxonomy-term="cost-savings"><span class="taxonomy-label">Cost Savings</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cpu/" data-taxonomy-term="cpu"><span class="taxonomy-label">CPU</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/critical-thinking/" data-taxonomy-term="critical-thinking"><span class="taxonomy-label">Critical Thinking</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/crop-management/" data-taxonomy-term="crop-management"><span class="taxonomy-label">Crop Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cross-validation/" data-taxonomy-term="cross-validation"><span class="taxonomy-label">Cross Validation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cryptography/" data-taxonomy-term="cryptography"><span class="taxonomy-label">Cryptography</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/css/" data-taxonomy-term="css"><span class="taxonomy-label">CSS</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/css-animations/" data-taxonomy-term="css-animations"><span class="taxonomy-label">CSS Animations</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/css-pseudo-elements/" data-taxonomy-term="css-pseudo-elements"><span class="taxonomy-label">CSS Pseudo-Elements</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cuda/" data-taxonomy-term="cuda"><span class="taxonomy-label">CUDA</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/curl/" data-taxonomy-term="curl"><span class="taxonomy-label">Curl</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/customer-segmentation/" data-taxonomy-term="customer-segmentation"><span class="taxonomy-label">Customer Segmentation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/customizable-ai-models/" data-taxonomy-term="customizable-ai-models"><span class="taxonomy-label">Customizable AI Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cuts/" data-taxonomy-term="cuts"><span class="taxonomy-label">Cuts</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cybersecurity/" data-taxonomy-term="cybersecurity"><span class="taxonomy-label">Cybersecurity</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cybersecurity-concepts-in-ai/" data-taxonomy-term="cybersecurity-concepts-in-ai"><span class="taxonomy-label">Cybersecurity Concepts in AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/cybersecurity-threats-in-ai-age/" data-taxonomy-term="cybersecurity-threats-in-ai-age"><span class="taxonomy-label">Cybersecurity Threats in AI Age</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/daily-tools/" data-taxonomy-term="daily-tools"><span class="taxonomy-label">Daily Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-analysis/" data-taxonomy-term="data-analysis"><span class="taxonomy-label">Data Analysis</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-analytics/" data-taxonomy-term="data-analytics"><span class="taxonomy-label">Data Analytics</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-anonymization/" data-taxonomy-term="data-anonymization"><span class="taxonomy-label">Data Anonymization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-architecture/" data-taxonomy-term="data-architecture"><span class="taxonomy-label">Data Architecture</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-center/" data-taxonomy-term="data-center"><span class="taxonomy-label">Data Center</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-collection/" data-taxonomy-term="data-collection"><span class="taxonomy-label">Data Collection</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-culture/" data-taxonomy-term="data-culture"><span class="taxonomy-label">Data Culture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-encryption/" data-taxonomy-term="data-encryption"><span class="taxonomy-label">Data Encryption</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-engineering/" data-taxonomy-term="data-engineering"><span class="taxonomy-label">Data Engineering</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-ethics/" data-taxonomy-term="data-ethics"><span class="taxonomy-label">Data Ethics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-governance/" data-taxonomy-term="data-governance"><span class="taxonomy-label">Data Governance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-infrastructure/" data-taxonomy-term="data-infrastructure"><span class="taxonomy-label">Data Infrastructure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-integration/" data-taxonomy-term="data-integration"><span class="taxonomy-label">Data Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-integrity/" data-taxonomy-term="data-integrity"><span class="taxonomy-label">Data Integrity</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-lake/" data-taxonomy-term="data-lake"><span class="taxonomy-label">Data Lake</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-management/" data-taxonomy-term="data-management"><span class="taxonomy-label">Data Management</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-philosophy/" data-taxonomy-term="data-philosophy"><span class="taxonomy-label">Data Philosophy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-privacy/" data-taxonomy-term="data-privacy"><span class="taxonomy-label">Data Privacy</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-privacy-ai/" data-taxonomy-term="data-privacy-ai"><span class="taxonomy-label">Data Privacy AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-processing/" data-taxonomy-term="data-processing"><span class="taxonomy-label">Data Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-protection/" data-taxonomy-term="data-protection"><span class="taxonomy-label">Data Protection</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-representation/" data-taxonomy-term="data-representation"><span class="taxonomy-label">Data Representation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science/" data-taxonomy-term="data-science"><span class="taxonomy-label">Data Science</span><span class="taxonomy-count">16</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-book/" data-taxonomy-term="data-science-book"><span class="taxonomy-label">Data Science Book</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-books/" data-taxonomy-term="data-science-books"><span class="taxonomy-label">Data Science Books</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-cheatsheets/" data-taxonomy-term="data-science-cheatsheets"><span class="taxonomy-label">Data Science Cheatsheets</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-datasets/" data-taxonomy-term="data-science-datasets"><span class="taxonomy-label">Data Science Datasets</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-education/" data-taxonomy-term="data-science-education"><span class="taxonomy-label">Data Science Education</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-fundamentals/" data-taxonomy-term="data-science-fundamentals"><span class="taxonomy-label">Data Science Fundamentals</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-interview/" data-taxonomy-term="data-science-interview"><span class="taxonomy-label">Data Science Interview</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-learning/" data-taxonomy-term="data-science-learning"><span class="taxonomy-label">Data Science Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-project-ideas/" data-taxonomy-term="data-science-project-ideas"><span class="taxonomy-label">Data Science Project Ideas</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-resources/" data-taxonomy-term="data-science-resources"><span class="taxonomy-label">Data Science Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-science-tools/" data-taxonomy-term="data-science-tools"><span class="taxonomy-label">Data Science Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-security/" data-taxonomy-term="data-security"><span class="taxonomy-label">Data Security</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-sources/" data-taxonomy-term="data-sources"><span class="taxonomy-label">Data Sources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-storage/" data-taxonomy-term="data-storage"><span class="taxonomy-label">Data Storage</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-structures/" data-taxonomy-term="data-structures"><span class="taxonomy-label">Data Structures</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-visualization/" data-taxonomy-term="data-visualization"><span class="taxonomy-label">Data Visualization</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-warehouse/" data-taxonomy-term="data-warehouse"><span class="taxonomy-label">Data Warehouse</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/data-warehousing/" data-taxonomy-term="data-warehousing"><span class="taxonomy-label">Data Warehousing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database/" data-taxonomy-term="database"><span class="taxonomy-label">Database</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-architecture/" data-taxonomy-term="database-architecture"><span class="taxonomy-label">Database Architecture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-design/" data-taxonomy-term="database-design"><span class="taxonomy-label">Database Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-management/" data-taxonomy-term="database-management"><span class="taxonomy-label">Database Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-selection/" data-taxonomy-term="database-selection"><span class="taxonomy-label">Database Selection</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-services/" data-taxonomy-term="database-services"><span class="taxonomy-label">Database Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-systems/" data-taxonomy-term="database-systems"><span class="taxonomy-label">Database Systems</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-technology/" data-taxonomy-term="database-technology"><span class="taxonomy-label">Database Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/database-theory/" data-taxonomy-term="database-theory"><span class="taxonomy-label">Database Theory</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/databases/" data-taxonomy-term="databases"><span class="taxonomy-label">Databases</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/dataops/" data-taxonomy-term="dataops"><span class="taxonomy-label">DataOps</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/decision-making/" data-taxonomy-term="decision-making"><span class="taxonomy-label">Decision Making</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/decoder/" data-taxonomy-term="decoder"><span class="taxonomy-label">Decoder</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/dedicated-hosting/" data-taxonomy-term="dedicated-hosting"><span class="taxonomy-label">Dedicated Hosting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-learning/" data-taxonomy-term="deep-learning"><span class="taxonomy-label">Deep Learning</span><span class="taxonomy-count">28</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-learning-conferences/" data-taxonomy-term="deep-learning-conferences"><span class="taxonomy-label">Deep Learning Conferences</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-learning-project-ideas/" data-taxonomy-term="deep-learning-project-ideas"><span class="taxonomy-label">Deep Learning Project Ideas</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-learning-projects/" data-taxonomy-term="deep-learning-projects"><span class="taxonomy-label">Deep Learning Projects</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-learning-research/" data-taxonomy-term="deep-learning-research"><span class="taxonomy-label">Deep Learning Research</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-learning-setup/" data-taxonomy-term="deep-learning-setup"><span class="taxonomy-label">Deep Learning Setup</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deep-rl/" data-taxonomy-term="deep-rl"><span class="taxonomy-label">Deep RL</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deepseek-r1/" data-taxonomy-term="deepseek-r1"><span class="taxonomy-label">DeepSeek R1</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/definition-of-life/" data-taxonomy-term="definition-of-life"><span class="taxonomy-label">Definition of Life</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/dependency-management/" data-taxonomy-term="dependency-management"><span class="taxonomy-label">Dependency Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/deployment/" data-taxonomy-term="deployment"><span class="taxonomy-label">Deployment</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/design-patterns/" data-taxonomy-term="design-patterns"><span class="taxonomy-label">Design Patterns</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/design-tools/" data-taxonomy-term="design-tools"><span class="taxonomy-label">Design Tools</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/designer-roles/" data-taxonomy-term="designer-roles"><span class="taxonomy-label">Designer Roles</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/developer-tools/" data-taxonomy-term="developer-tools"><span class="taxonomy-label">Developer Tools</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/development-articles/" data-taxonomy-term="development-articles"><span class="taxonomy-label">Development Articles</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/development-environment/" data-taxonomy-term="development-environment"><span class="taxonomy-label">Development Environment</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/development-tools/" data-taxonomy-term="development-tools"><span class="taxonomy-label">Development Tools</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/devi/" data-taxonomy-term="devi"><span class="taxonomy-label">Devi</span><span class="taxonomy-count">45</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/devops/" data-taxonomy-term="devops"><span class="taxonomy-label">DevOps</span><span class="taxonomy-count">7</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/diffusion-models/" data-taxonomy-term="diffusion-models"><span class="taxonomy-label">Diffusion Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-banking/" data-taxonomy-term="digital-banking"><span class="taxonomy-label">Digital Banking</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-economy/" data-taxonomy-term="digital-economy"><span class="taxonomy-label">Digital Economy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-evolution/" data-taxonomy-term="digital-evolution"><span class="taxonomy-label">Digital Evolution</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-governance/" data-taxonomy-term="digital-governance"><span class="taxonomy-label">Digital Governance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-health/" data-taxonomy-term="digital-health"><span class="taxonomy-label">Digital Health</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-humanities/" data-taxonomy-term="digital-humanities"><span class="taxonomy-label">Digital Humanities</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-infrastructure/" data-taxonomy-term="digital-infrastructure"><span class="taxonomy-label">Digital Infrastructure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-learning/" data-taxonomy-term="digital-learning"><span class="taxonomy-label">Digital Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-signal-processing/" data-taxonomy-term="digital-signal-processing"><span class="taxonomy-label">Digital Signal Processing</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-signatures/" data-taxonomy-term="digital-signatures"><span class="taxonomy-label">Digital Signatures</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-solutions/" data-taxonomy-term="digital-solutions"><span class="taxonomy-label">Digital Solutions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-technologies/" data-taxonomy-term="digital-technologies"><span class="taxonomy-label">Digital Technologies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-tools/" data-taxonomy-term="digital-tools"><span class="taxonomy-label">Digital Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-transformation/" data-taxonomy-term="digital-transformation"><span class="taxonomy-label">Digital Transformation</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/digital-twin/" data-taxonomy-term="digital-twin"><span class="taxonomy-label">Digital Twin</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/dimensionality-reduction/" data-taxonomy-term="dimensionality-reduction"><span class="taxonomy-label">Dimensionality Reduction</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/directory-structure/" data-taxonomy-term="directory-structure"><span class="taxonomy-label">Directory Structure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/distance-metrics/" data-taxonomy-term="distance-metrics"><span class="taxonomy-label">Distance Metrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/distributed-systems/" data-taxonomy-term="distributed-systems"><span class="taxonomy-label">Distributed Systems</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/docker/" data-taxonomy-term="docker"><span class="taxonomy-label">Docker</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/docker-build/" data-taxonomy-term="docker-build"><span class="taxonomy-label">Docker Build</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/docker-containerization/" data-taxonomy-term="docker-containerization"><span class="taxonomy-label">Docker Containerization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/docker-containers/" data-taxonomy-term="docker-containers"><span class="taxonomy-label">Docker Containers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/document-clustering/" data-taxonomy-term="document-clustering"><span class="taxonomy-label">Document Clustering</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/document-databases/" data-taxonomy-term="document-databases"><span class="taxonomy-label">Document Databases</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/document-preparation/" data-taxonomy-term="document-preparation"><span class="taxonomy-label">Document Preparation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/domain-expertise/" data-taxonomy-term="domain-expertise"><span class="taxonomy-label">Domain Expertise</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ds-resources/" data-taxonomy-term="ds-resources"><span class="taxonomy-label">DS Resources</span><span class="taxonomy-count">16</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/durga-saptashati/" data-taxonomy-term="durga-saptashati"><span class="taxonomy-label">Durga Saptashati</span><span class="taxonomy-count">23</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/e-commerce/" data-taxonomy-term="e-commerce"><span class="taxonomy-label">E-Commerce</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/e-governance/" data-taxonomy-term="e-governance"><span class="taxonomy-label">E-Governance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ebook/" data-taxonomy-term="ebook"><span class="taxonomy-label">EBook</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/econometrics/" data-taxonomy-term="econometrics"><span class="taxonomy-label">Econometrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/eda/" data-taxonomy-term="eda"><span class="taxonomy-label">EDA</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/edtech/" data-taxonomy-term="edtech"><span class="taxonomy-label">EdTech</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/educational-content/" data-taxonomy-term="educational-content"><span class="taxonomy-label">Educational Content</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/educational-material/" data-taxonomy-term="educational-material"><span class="taxonomy-label">Educational Material</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/educational-technology/" data-taxonomy-term="educational-technology"><span class="taxonomy-label">Educational Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/educational-tools/" data-taxonomy-term="educational-tools"><span class="taxonomy-label">Educational Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/efficient-ai-models/" data-taxonomy-term="efficient-ai-models"><span class="taxonomy-label">Efficient AI Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/electronics-chips/" data-taxonomy-term="electronics-chips"><span class="taxonomy-label">Electronics Chips</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/embedded-systems/" data-taxonomy-term="embedded-systems"><span class="taxonomy-label">Embedded Systems</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/embeddings/" data-taxonomy-term="embeddings"><span class="taxonomy-label">Embeddings</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/emerging-technologies/" data-taxonomy-term="emerging-technologies"><span class="taxonomy-label">Emerging Technologies</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/encoder/" data-taxonomy-term="encoder"><span class="taxonomy-label">Encoder</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/energy/" data-taxonomy-term="energy"><span class="taxonomy-label">Energy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/energy-analytics/" data-taxonomy-term="energy-analytics"><span class="taxonomy-label">Energy Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/energy-efficiency/" data-taxonomy-term="energy-efficiency"><span class="taxonomy-label">Energy Efficiency</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/energy-management/" data-taxonomy-term="energy-management"><span class="taxonomy-label">Energy Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ensemble-methods/" data-taxonomy-term="ensemble-methods"><span class="taxonomy-label">Ensemble Methods</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/enterprise-ai/" data-taxonomy-term="enterprise-ai"><span class="taxonomy-label">Enterprise AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/enterprise-cloud/" data-taxonomy-term="enterprise-cloud"><span class="taxonomy-label">Enterprise Cloud</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/enterprise-data/" data-taxonomy-term="enterprise-data"><span class="taxonomy-label">Enterprise Data</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/epic-poetry/" data-taxonomy-term="epic-poetry"><span class="taxonomy-label">Epic Poetry</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/error-rate/" data-taxonomy-term="error-rate"><span class="taxonomy-label">Error Rate</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ethical-ai/" data-taxonomy-term="ethical-ai"><span class="taxonomy-label">Ethical AI</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/etl/" data-taxonomy-term="etl"><span class="taxonomy-label">ETL</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/etl-tools/" data-taxonomy-term="etl-tools"><span class="taxonomy-label">ETL Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/evaluation-benchmarks/" data-taxonomy-term="evaluation-benchmarks"><span class="taxonomy-label">Evaluation Benchmarks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/evaluation-of-generative-models/" data-taxonomy-term="evaluation-of-generative-models"><span class="taxonomy-label">Evaluation of Generative Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/explainable-ai/" data-taxonomy-term="explainable-ai"><span class="taxonomy-label">Explainable AI</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/exploration-vs.-exploitation/" data-taxonomy-term="exploration-vs.-exploitation"><span class="taxonomy-label">Exploration vs. Exploitation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/exploratory-data-analysis/" data-taxonomy-term="exploratory-data-analysis"><span class="taxonomy-label">Exploratory Data Analysis</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/f1-score/" data-taxonomy-term="f1-score"><span class="taxonomy-label">F1 Score</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/fallacies/" data-taxonomy-term="fallacies"><span class="taxonomy-label">Fallacies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/farm-automation/" data-taxonomy-term="farm-automation"><span class="taxonomy-label">Farm Automation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/fasttext/" data-taxonomy-term="fasttext"><span class="taxonomy-label">FastText</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/feature-engineering/" data-taxonomy-term="feature-engineering"><span class="taxonomy-label">Feature Engineering</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/file-management/" data-taxonomy-term="file-management"><span class="taxonomy-label">File Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/file-system/" data-taxonomy-term="file-system"><span class="taxonomy-label">File System</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/financial-innovation/" data-taxonomy-term="financial-innovation"><span class="taxonomy-label">Financial Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/financial-services/" data-taxonomy-term="financial-services"><span class="taxonomy-label">Financial Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/financial-technology/" data-taxonomy-term="financial-technology"><span class="taxonomy-label">Financial Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/fintech/" data-taxonomy-term="fintech"><span class="taxonomy-label">FinTech</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/flutter/" data-taxonomy-term="flutter"><span class="taxonomy-label">Flutter</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/food-processing/" data-taxonomy-term="food-processing"><span class="taxonomy-label">Food Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/food-safety/" data-taxonomy-term="food-safety"><span class="taxonomy-label">Food Safety</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/food-technology/" data-taxonomy-term="food-technology"><span class="taxonomy-label">Food Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/forecasting/" data-taxonomy-term="forecasting"><span class="taxonomy-label">Forecasting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/free-design-resources/" data-taxonomy-term="free-design-resources"><span class="taxonomy-label">Free Design Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/free-will/" data-taxonomy-term="free-will"><span class="taxonomy-label">Free Will</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/frequency/" data-taxonomy-term="frequency"><span class="taxonomy-label">Frequency</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/front-matter/" data-taxonomy-term="front-matter"><span class="taxonomy-label">Front Matter</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/frontend-development/" data-taxonomy-term="frontend-development"><span class="taxonomy-label">Frontend Development</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/future-of-ai/" data-taxonomy-term="future-of-ai"><span class="taxonomy-label">Future of AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/future-of-programming/" data-taxonomy-term="future-of-programming"><span class="taxonomy-label">Future of Programming</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/future-of-work/" data-taxonomy-term="future-of-work"><span class="taxonomy-label">Future of Work</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/future-skills/" data-taxonomy-term="future-skills"><span class="taxonomy-label">Future Skills</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/future-technology/" data-taxonomy-term="future-technology"><span class="taxonomy-label">Future Technology</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/future-trends/" data-taxonomy-term="future-trends"><span class="taxonomy-label">Future Trends</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gaming/" data-taxonomy-term="gaming"><span class="taxonomy-label">Gaming</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gan/" data-taxonomy-term="gan"><span class="taxonomy-label">GAN</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gan-architecture/" data-taxonomy-term="gan-architecture"><span class="taxonomy-label">GAN Architecture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gans/" data-taxonomy-term="gans"><span class="taxonomy-label">GANs</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gcp/" data-taxonomy-term="gcp"><span class="taxonomy-label">GCP</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gdpr-compliance/" data-taxonomy-term="gdpr-compliance"><span class="taxonomy-label">GDPR Compliance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/generalized-ml-model/" data-taxonomy-term="generalized-ml-model"><span class="taxonomy-label">Generalized ML Model</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/generation/" data-taxonomy-term="generation"><span class="taxonomy-label">Generation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/generative-ai/" data-taxonomy-term="generative-ai"><span class="taxonomy-label">Generative AI</span><span class="taxonomy-count">9</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/generative-models-evaluation/" data-taxonomy-term="generative-models-evaluation"><span class="taxonomy-label">Generative Models Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gguf/" data-taxonomy-term="gguf"><span class="taxonomy-label">GGUF</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/git/" data-taxonomy-term="git"><span class="taxonomy-label">Git</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/github/" data-taxonomy-term="github"><span class="taxonomy-label">GitHub</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/github-repos/" data-taxonomy-term="github-repos"><span class="taxonomy-label">Github Repos</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/github-resources/" data-taxonomy-term="github-resources"><span class="taxonomy-label">GitHub Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/google-ai-studio/" data-taxonomy-term="google-ai-studio"><span class="taxonomy-label">Google AI Studio</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/google-cloud/" data-taxonomy-term="google-cloud"><span class="taxonomy-label">Google Cloud</span><span class="taxonomy-count">7</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/google-cloud-platform/" data-taxonomy-term="google-cloud-platform"><span class="taxonomy-label">Google Cloud Platform</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/google-ecosystem/" data-taxonomy-term="google-ecosystem"><span class="taxonomy-label">Google Ecosystem</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/google-products/" data-taxonomy-term="google-products"><span class="taxonomy-label">Google Products</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/google-services/" data-taxonomy-term="google-services"><span class="taxonomy-label">Google Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/government-technology/" data-taxonomy-term="government-technology"><span class="taxonomy-label">Government Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gpt/" data-taxonomy-term="gpt"><span class="taxonomy-label">GPT</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gpt3/" data-taxonomy-term="gpt3"><span class="taxonomy-label">GPT3</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gpu-architecture/" data-taxonomy-term="gpu-architecture"><span class="taxonomy-label">GPU Architecture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gpu-computing/" data-taxonomy-term="gpu-computing"><span class="taxonomy-label">GPU Computing</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gpus/" data-taxonomy-term="gpus"><span class="taxonomy-label">GPUs</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gradient-descent/" data-taxonomy-term="gradient-descent"><span class="taxonomy-label">Gradient Descent</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/grant-sanderson/" data-taxonomy-term="grant-sanderson"><span class="taxonomy-label">Grant Sanderson</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/graph-database-advantages/" data-taxonomy-term="graph-database-advantages"><span class="taxonomy-label">Graph Database Advantages</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/graph-database-performance/" data-taxonomy-term="graph-database-performance"><span class="taxonomy-label">Graph Database Performance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/graph-databases/" data-taxonomy-term="graph-databases"><span class="taxonomy-label">Graph Databases</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/graph-of-thoughts/" data-taxonomy-term="graph-of-thoughts"><span class="taxonomy-label">Graph of Thoughts</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/graphdb/" data-taxonomy-term="graphdb"><span class="taxonomy-label">GraphDB</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/graphql/" data-taxonomy-term="graphql"><span class="taxonomy-label">GraphQL</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/green-technology/" data-taxonomy-term="green-technology"><span class="taxonomy-label">Green Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/grok3-model/" data-taxonomy-term="grok3-model"><span class="taxonomy-label">Grok3 Model</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/gsm/" data-taxonomy-term="gsm"><span class="taxonomy-label">GSM</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/guru/" data-taxonomy-term="guru"><span class="taxonomy-label">Guru</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hadoop/" data-taxonomy-term="hadoop"><span class="taxonomy-label">Hadoop</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hanumanji/" data-taxonomy-term="hanumanji"><span class="taxonomy-label">Hanumanji</span><span class="taxonomy-count">8</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hardware-acceleration/" data-taxonomy-term="hardware-acceleration"><span class="taxonomy-label">Hardware Acceleration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hardware-requirements/" data-taxonomy-term="hardware-requirements"><span class="taxonomy-label">Hardware Requirements</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hash-functions/" data-taxonomy-term="hash-functions"><span class="taxonomy-label">Hash Functions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/healthcare-ai/" data-taxonomy-term="healthcare-ai"><span class="taxonomy-label">Healthcare AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/healthcare-analytics/" data-taxonomy-term="healthcare-analytics"><span class="taxonomy-label">Healthcare Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/high-performance-computing/" data-taxonomy-term="high-performance-computing"><span class="taxonomy-label">High Performance Computing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/high-school-math/" data-taxonomy-term="high-school-math"><span class="taxonomy-label">High School Math</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/high-resolution/" data-taxonomy-term="high-resolution"><span class="taxonomy-label">High-Resolution</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/higher-education/" data-taxonomy-term="higher-education"><span class="taxonomy-label">Higher Education</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hindi-bhajan/" data-taxonomy-term="hindi-bhajan"><span class="taxonomy-label">Hindi Bhajan</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/historical-documents/" data-taxonomy-term="historical-documents"><span class="taxonomy-label">Historical Documents</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hosting-types/" data-taxonomy-term="hosting-types"><span class="taxonomy-label">Hosting Types</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/how-ai-shapes-public-perception/" data-taxonomy-term="how-ai-shapes-public-perception"><span class="taxonomy-label">How AI Shapes Public Perception</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/html/" data-taxonomy-term="html"><span class="taxonomy-label">HTML</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/http-protocol/" data-taxonomy-term="http-protocol"><span class="taxonomy-label">HTTP Protocol</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hugging-face/" data-taxonomy-term="hugging-face"><span class="taxonomy-label">Hugging Face</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-behavior/" data-taxonomy-term="human-behavior"><span class="taxonomy-label">Human Behavior</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-consciousness/" data-taxonomy-term="human-consciousness"><span class="taxonomy-label">Human Consciousness</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-evaluation/" data-taxonomy-term="human-evaluation"><span class="taxonomy-label">Human Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-skills/" data-taxonomy-term="human-skills"><span class="taxonomy-label">Human Skills</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-ai-interaction/" data-taxonomy-term="human-ai-interaction"><span class="taxonomy-label">Human-AI Interaction</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-like-ai/" data-taxonomy-term="human-like-ai"><span class="taxonomy-label">Human-Like AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/human-like-intelligence/" data-taxonomy-term="human-like-intelligence"><span class="taxonomy-label">Human-Like Intelligence</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hunyuan3d/" data-taxonomy-term="hunyuan3d"><span class="taxonomy-label">Hunyuan3D</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hyperparameter-tuning/" data-taxonomy-term="hyperparameter-tuning"><span class="taxonomy-label">Hyperparameter Tuning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/hypothesis-testing/" data-taxonomy-term="hypothesis-testing"><span class="taxonomy-label">Hypothesis Testing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/i/o-peripherals/" data-taxonomy-term="i/o-peripherals"><span class="taxonomy-label">I/O Peripherals</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ide/" data-taxonomy-term="ide"><span class="taxonomy-label">IDE</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ide-integration/" data-taxonomy-term="ide-integration"><span class="taxonomy-label">IDE Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/image-generation/" data-taxonomy-term="image-generation"><span class="taxonomy-label">Image Generation</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/image-processing/" data-taxonomy-term="image-processing"><span class="taxonomy-label">Image Processing</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/image-synthesis/" data-taxonomy-term="image-synthesis"><span class="taxonomy-label">Image Synthesis</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/image-synthesis-models/" data-taxonomy-term="image-synthesis-models"><span class="taxonomy-label">Image Synthesis Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/imperialism/" data-taxonomy-term="imperialism"><span class="taxonomy-label">Imperialism</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/in-context-learning/" data-taxonomy-term="in-context-learning"><span class="taxonomy-label">In Context Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/india/" data-taxonomy-term="india"><span class="taxonomy-label">India</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/indian-literature/" data-taxonomy-term="indian-literature"><span class="taxonomy-label">Indian Literature</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/industrial-ai/" data-taxonomy-term="industrial-ai"><span class="taxonomy-label">Industrial AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/industrial-revolution/" data-taxonomy-term="industrial-revolution"><span class="taxonomy-label">Industrial Revolution</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/industrial-technology/" data-taxonomy-term="industrial-technology"><span class="taxonomy-label">Industrial Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/industry-4.0/" data-taxonomy-term="industry-4.0"><span class="taxonomy-label">Industry 4.0</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/industry-applications/" data-taxonomy-term="industry-applications"><span class="taxonomy-label">Industry Applications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/industry-knowledge/" data-taxonomy-term="industry-knowledge"><span class="taxonomy-label">Industry Knowledge</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/information-age/" data-taxonomy-term="information-age"><span class="taxonomy-label">Information Age</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/information-security/" data-taxonomy-term="information-security"><span class="taxonomy-label">Information Security</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/information-technology/" data-taxonomy-term="information-technology"><span class="taxonomy-label">Information Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/infrastructure/" data-taxonomy-term="infrastructure"><span class="taxonomy-label">Infrastructure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/innovation/" data-taxonomy-term="innovation"><span class="taxonomy-label">Innovation</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/innovations/" data-taxonomy-term="innovations"><span class="taxonomy-label">Innovations</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/insurance/" data-taxonomy-term="insurance"><span class="taxonomy-label">Insurance</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/integrated-circuits/" data-taxonomy-term="integrated-circuits"><span class="taxonomy-label">Integrated Circuits</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/integrated-development-environments/" data-taxonomy-term="integrated-development-environments"><span class="taxonomy-label">Integrated Development Environments</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/integration/" data-taxonomy-term="integration"><span class="taxonomy-label">Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/intellectual-discussion/" data-taxonomy-term="intellectual-discussion"><span class="taxonomy-label">Intellectual Discussion</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/international-standards/" data-taxonomy-term="international-standards"><span class="taxonomy-label">International Standards</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/interview-preparation/" data-taxonomy-term="interview-preparation"><span class="taxonomy-label">Interview Preparation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/interview-skills/" data-taxonomy-term="interview-skills"><span class="taxonomy-label">Interview Skills</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/iot/" data-taxonomy-term="iot"><span class="taxonomy-label">IoT</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/iss-mission/" data-taxonomy-term="iss-mission"><span class="taxonomy-label">ISS Mission</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/it-infrastructure/" data-taxonomy-term="it-infrastructure"><span class="taxonomy-label">IT Infrastructure</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/it-security/" data-taxonomy-term="it-security"><span class="taxonomy-label">IT Security</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/java-script/" data-taxonomy-term="java-script"><span class="taxonomy-label">Java Script</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/javascript/" data-taxonomy-term="javascript"><span class="taxonomy-label">JavaScript</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/jekyll/" data-taxonomy-term="jekyll"><span class="taxonomy-label">Jekyll</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/jekyll-blog-comments/" data-taxonomy-term="jekyll-blog-comments"><span class="taxonomy-label">Jekyll Blog Comments</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/jekyll-comments/" data-taxonomy-term="jekyll-comments"><span class="taxonomy-label">Jekyll Comments</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/katha/" data-taxonomy-term="katha"><span class="taxonomy-label">Katha</span><span class="taxonomy-count">9</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/key-value-stores/" data-taxonomy-term="key-value-stores"><span class="taxonomy-label">Key-Value Stores</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/keyword-planning/" data-taxonomy-term="keyword-planning"><span class="taxonomy-label">Keyword Planning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/knowledge-distillation/" data-taxonomy-term="knowledge-distillation"><span class="taxonomy-label">Knowledge Distillation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/knowledge-representation/" data-taxonomy-term="knowledge-representation"><span class="taxonomy-label">Knowledge Representation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/krishna/" data-taxonomy-term="krishna"><span class="taxonomy-label">Krishna</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/kubernetes/" data-taxonomy-term="kubernetes"><span class="taxonomy-label">Kubernetes</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/language-ai/" data-taxonomy-term="language-ai"><span class="taxonomy-label">Language AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/language-models/" data-taxonomy-term="language-models"><span class="taxonomy-label">Language Models</span><span class="taxonomy-count">16</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/language-understanding/" data-taxonomy-term="language-understanding"><span class="taxonomy-label">Language Understanding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/large-language-model/" data-taxonomy-term="large-language-model"><span class="taxonomy-label">Large Language Model</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/large-language-models/" data-taxonomy-term="large-language-models"><span class="taxonomy-label">Large Language Models</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/large-scale/" data-taxonomy-term="large-scale"><span class="taxonomy-label">Large Scale</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/latest-information/" data-taxonomy-term="latest-information"><span class="taxonomy-label">Latest Information</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/latex/" data-taxonomy-term="latex"><span class="taxonomy-label">LaTeX</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/leaderboards/" data-taxonomy-term="leaderboards"><span class="taxonomy-label">Leaderboards</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/learning-analytics/" data-taxonomy-term="learning-analytics"><span class="taxonomy-label">Learning Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/learning-materials/" data-taxonomy-term="learning-materials"><span class="taxonomy-label">Learning Materials</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/learning-resources/" data-taxonomy-term="learning-resources"><span class="taxonomy-label">Learning Resources</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/library/" data-taxonomy-term="library"><span class="taxonomy-label">Library</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/licensing/" data-taxonomy-term="licensing"><span class="taxonomy-label">Licensing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/life-sciences/" data-taxonomy-term="life-sciences"><span class="taxonomy-label">Life Sciences</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/life-skills/" data-taxonomy-term="life-skills"><span class="taxonomy-label">Life Skills</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/limitations/" data-taxonomy-term="limitations"><span class="taxonomy-label">Limitations</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linear-algebra/" data-taxonomy-term="linear-algebra"><span class="taxonomy-label">Linear Algebra</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linear-regression/" data-taxonomy-term="linear-regression"><span class="taxonomy-label">Linear Regression</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linux/" data-taxonomy-term="linux"><span class="taxonomy-label">Linux</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linux-administration/" data-taxonomy-term="linux-administration"><span class="taxonomy-label">Linux Administration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linux-directory-structure/" data-taxonomy-term="linux-directory-structure"><span class="taxonomy-label">Linux Directory Structure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linux-distributions/" data-taxonomy-term="linux-distributions"><span class="taxonomy-label">Linux Distributions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/linux-tools/" data-taxonomy-term="linux-tools"><span class="taxonomy-label">Linux Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/liquid/" data-taxonomy-term="liquid"><span class="taxonomy-label">Liquid</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/literature-review/" data-taxonomy-term="literature-review"><span class="taxonomy-label">Literature Review</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm/" data-taxonomy-term="llm"><span class="taxonomy-label">LLM</span><span class="taxonomy-count">15</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-app-development/" data-taxonomy-term="llm-app-development"><span class="taxonomy-label">LLM App Development</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-architecture/" data-taxonomy-term="llm-architecture"><span class="taxonomy-label">LLM Architecture</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-benchmarks/" data-taxonomy-term="llm-benchmarks"><span class="taxonomy-label">LLM Benchmarks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-embeddings/" data-taxonomy-term="llm-embeddings"><span class="taxonomy-label">LLM Embeddings</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-evaluation-metrics/" data-taxonomy-term="llm-evaluation-metrics"><span class="taxonomy-label">LLM Evaluation Metrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-fine-tuning/" data-taxonomy-term="llm-fine-tuning"><span class="taxonomy-label">LLM Fine-Tuning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-infrastructure/" data-taxonomy-term="llm-infrastructure"><span class="taxonomy-label">LLM Infrastructure</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/llm-internal-encoding/" data-taxonomy-term="llm-internal-encoding"><span class="taxonomy-label">LLM Internal Encoding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/local/" data-taxonomy-term="local"><span class="taxonomy-label">Local</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/local-ai-processing/" data-taxonomy-term="local-ai-processing"><span class="taxonomy-label">Local AI Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/local-models/" data-taxonomy-term="local-models"><span class="taxonomy-label">Local Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/location/" data-taxonomy-term="location"><span class="taxonomy-label">Location</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/logic/" data-taxonomy-term="logic"><span class="taxonomy-label">Logic</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/logic-gates/" data-taxonomy-term="logic-gates"><span class="taxonomy-label">Logic Gates</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/loss-functions/" data-taxonomy-term="loss-functions"><span class="taxonomy-label">Loss Functions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine/" data-taxonomy-term="machine"><span class="taxonomy-label">Machine</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning/" data-taxonomy-term="machine-learning"><span class="taxonomy-label">Machine Learning</span><span class="taxonomy-count">65</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-conferences/" data-taxonomy-term="machine-learning-conferences"><span class="taxonomy-label">Machine Learning Conferences</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-framework/" data-taxonomy-term="machine-learning-framework"><span class="taxonomy-label">Machine Learning Framework</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-infrastructure/" data-taxonomy-term="machine-learning-infrastructure"><span class="taxonomy-label">Machine Learning Infrastructure</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-interpretability/" data-taxonomy-term="machine-learning-interpretability"><span class="taxonomy-label">Machine Learning Interpretability</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-interview/" data-taxonomy-term="machine-learning-interview"><span class="taxonomy-label">Machine Learning Interview</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-math/" data-taxonomy-term="machine-learning-math"><span class="taxonomy-label">Machine Learning Math</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-models/" data-taxonomy-term="machine-learning-models"><span class="taxonomy-label">Machine Learning Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-performance-metrics/" data-taxonomy-term="machine-learning-performance-metrics"><span class="taxonomy-label">Machine Learning Performance Metrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-project-ideas/" data-taxonomy-term="machine-learning-project-ideas"><span class="taxonomy-label">Machine Learning Project Ideas</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-publications/" data-taxonomy-term="machine-learning-publications"><span class="taxonomy-label">Machine Learning Publications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-reference/" data-taxonomy-term="machine-learning-reference"><span class="taxonomy-label">Machine Learning Reference</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-security/" data-taxonomy-term="machine-learning-security"><span class="taxonomy-label">Machine Learning Security</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/machine-learning-tasks/" data-taxonomy-term="machine-learning-tasks"><span class="taxonomy-label">Machine Learning Tasks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/madam-rag/" data-taxonomy-term="madam-rag"><span class="taxonomy-label">MADAM-RAG</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mahabharata/" data-taxonomy-term="mahabharata"><span class="taxonomy-label">Mahabharata</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/make.com/" data-taxonomy-term="make.com"><span class="taxonomy-label">Make.com</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/marketing-analytics/" data-taxonomy-term="marketing-analytics"><span class="taxonomy-label">Marketing Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/markov-decision-processes/" data-taxonomy-term="markov-decision-processes"><span class="taxonomy-label">Markov Decision Processes</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/math-applications/" data-taxonomy-term="math-applications"><span class="taxonomy-label">Math Applications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/math-examples/" data-taxonomy-term="math-examples"><span class="taxonomy-label">Math Examples</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/math-in-engineering/" data-taxonomy-term="math-in-engineering"><span class="taxonomy-label">Math in Engineering</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/math-in-real-life/" data-taxonomy-term="math-in-real-life"><span class="taxonomy-label">Math in Real Life</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/math-in-science/" data-taxonomy-term="math-in-science"><span class="taxonomy-label">Math in Science</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/math-prerequisites/" data-taxonomy-term="math-prerequisites"><span class="taxonomy-label">Math Prerequisites</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mathematical-concepts/" data-taxonomy-term="mathematical-concepts"><span class="taxonomy-label">Mathematical Concepts</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mathematical-foundations/" data-taxonomy-term="mathematical-foundations"><span class="taxonomy-label">Mathematical Foundations</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mathematical-optimization/" data-taxonomy-term="mathematical-optimization"><span class="taxonomy-label">Mathematical Optimization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mathematics-for-data-science/" data-taxonomy-term="mathematics-for-data-science"><span class="taxonomy-label">Mathematics for Data Science</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/maths-in-ai/" data-taxonomy-term="maths-in-ai"><span class="taxonomy-label">Maths in AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/medical-diagnosis/" data-taxonomy-term="medical-diagnosis"><span class="taxonomy-label">Medical Diagnosis</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/medical-innovation/" data-taxonomy-term="medical-innovation"><span class="taxonomy-label">Medical Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/medical-technology/" data-taxonomy-term="medical-technology"><span class="taxonomy-label">Medical Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/memory-management/" data-taxonomy-term="memory-management"><span class="taxonomy-label">Memory Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mental-models/" data-taxonomy-term="mental-models"><span class="taxonomy-label">Mental Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/meta/" data-taxonomy-term="meta"><span class="taxonomy-label">Meta</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/microcontroller/" data-taxonomy-term="microcontroller"><span class="taxonomy-label">Microcontroller</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/microprocessor/" data-taxonomy-term="microprocessor"><span class="taxonomy-label">Microprocessor</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/microprocessors/" data-taxonomy-term="microprocessors"><span class="taxonomy-label">Microprocessors</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/microsoft/" data-taxonomy-term="microsoft"><span class="taxonomy-label">Microsoft</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/microsoft-ai/" data-taxonomy-term="microsoft-ai"><span class="taxonomy-label">Microsoft AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/microsoft-azure/" data-taxonomy-term="microsoft-azure"><span class="taxonomy-label">Microsoft Azure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mind-exploration/" data-taxonomy-term="mind-exploration"><span class="taxonomy-label">Mind Exploration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mind-studies/" data-taxonomy-term="mind-studies"><span class="taxonomy-label">Mind Studies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml/" data-taxonomy-term="ml"><span class="taxonomy-label">ML</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-algorithms/" data-taxonomy-term="ml-algorithms"><span class="taxonomy-label">ML Algorithms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-applications/" data-taxonomy-term="ml-applications"><span class="taxonomy-label">ML Applications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-automation/" data-taxonomy-term="ml-automation"><span class="taxonomy-label">ML Automation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-development/" data-taxonomy-term="ml-development"><span class="taxonomy-label">ML Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-libraries/" data-taxonomy-term="ml-libraries"><span class="taxonomy-label">ML Libraries</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-metrics/" data-taxonomy-term="ml-metrics"><span class="taxonomy-label">ML Metrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-models/" data-taxonomy-term="ml-models"><span class="taxonomy-label">ML Models</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-project-directory-structure/" data-taxonomy-term="ml-project-directory-structure"><span class="taxonomy-label">ML Project Directory Structure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-projects/" data-taxonomy-term="ml-projects"><span class="taxonomy-label">ML Projects</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-researchers/" data-taxonomy-term="ml-researchers"><span class="taxonomy-label">ML Researchers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ml-tools/" data-taxonomy-term="ml-tools"><span class="taxonomy-label">ML Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/mlops/" data-taxonomy-term="mlops"><span class="taxonomy-label">MLOps</span><span class="taxonomy-count">8</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-architecture/" data-taxonomy-term="model-architecture"><span class="taxonomy-label">Model Architecture</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-assessment/" data-taxonomy-term="model-assessment"><span class="taxonomy-label">Model Assessment</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-compression/" data-taxonomy-term="model-compression"><span class="taxonomy-label">Model Compression</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-creators/" data-taxonomy-term="model-creators"><span class="taxonomy-label">Model Creators</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-deployment/" data-taxonomy-term="model-deployment"><span class="taxonomy-label">Model Deployment</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-development/" data-taxonomy-term="model-development"><span class="taxonomy-label">Model Development</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-evaluation/" data-taxonomy-term="model-evaluation"><span class="taxonomy-label">Model Evaluation</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-evaluation-metrics/" data-taxonomy-term="model-evaluation-metrics"><span class="taxonomy-label">Model Evaluation Metrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-explainability/" data-taxonomy-term="model-explainability"><span class="taxonomy-label">Model Explainability</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-fine-tuning/" data-taxonomy-term="model-fine-tuning"><span class="taxonomy-label">Model Fine-Tuning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-formats/" data-taxonomy-term="model-formats"><span class="taxonomy-label">Model Formats</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-framework/" data-taxonomy-term="model-framework"><span class="taxonomy-label">Model Framework</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-management/" data-taxonomy-term="model-management"><span class="taxonomy-label">Model Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-monitoring/" data-taxonomy-term="model-monitoring"><span class="taxonomy-label">Model Monitoring</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-optimization/" data-taxonomy-term="model-optimization"><span class="taxonomy-label">Model Optimization</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-repository/" data-taxonomy-term="model-repository"><span class="taxonomy-label">Model Repository</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-serving/" data-taxonomy-term="model-serving"><span class="taxonomy-label">Model Serving</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-training/" data-taxonomy-term="model-training"><span class="taxonomy-label">Model Training</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-tuning/" data-taxonomy-term="model-tuning"><span class="taxonomy-label">Model Tuning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/model-zoo/" data-taxonomy-term="model-zoo"><span class="taxonomy-label">Model Zoo</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/models/" data-taxonomy-term="models"><span class="taxonomy-label">Models</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/modern-life/" data-taxonomy-term="modern-life"><span class="taxonomy-label">Modern Life</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/motivation/" data-taxonomy-term="motivation"><span class="taxonomy-label">Motivation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/naive-bayes-classifier/" data-taxonomy-term="naive-bayes-classifier"><span class="taxonomy-label">Naive Bayes Classifier</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/national-language/" data-taxonomy-term="national-language"><span class="taxonomy-label">National Language</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/natural-language-processing/" data-taxonomy-term="natural-language-processing"><span class="taxonomy-label">Natural Language Processing</span><span class="taxonomy-count">21</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/neo4j/" data-taxonomy-term="neo4j"><span class="taxonomy-label">Neo4j</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/network-security/" data-taxonomy-term="network-security"><span class="taxonomy-label">Network Security</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/neural-network-types/" data-taxonomy-term="neural-network-types"><span class="taxonomy-label">Neural Network Types</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/neural-networks/" data-taxonomy-term="neural-networks"><span class="taxonomy-label">Neural Networks</span><span class="taxonomy-count">15</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/neuroscience/" data-taxonomy-term="neuroscience"><span class="taxonomy-label">Neuroscience</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp/" data-taxonomy-term="nlp"><span class="taxonomy-label">NLP</span><span class="taxonomy-count">16</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-applications/" data-taxonomy-term="nlp-applications"><span class="taxonomy-label">NLP Applications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-conferences/" data-taxonomy-term="nlp-conferences"><span class="taxonomy-label">NLP Conferences</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-model-evaluation/" data-taxonomy-term="nlp-model-evaluation"><span class="taxonomy-label">NLP Model Evaluation</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-project-ideas/" data-taxonomy-term="nlp-project-ideas"><span class="taxonomy-label">NLP Project Ideas</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-projects/" data-taxonomy-term="nlp-projects"><span class="taxonomy-label">NLP Projects</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-researchers/" data-taxonomy-term="nlp-researchers"><span class="taxonomy-label">NLP Researchers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-tasks/" data-taxonomy-term="nlp-tasks"><span class="taxonomy-label">NLP Tasks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nlp-testing/" data-taxonomy-term="nlp-testing"><span class="taxonomy-label">NLP Testing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nosql/" data-taxonomy-term="nosql"><span class="taxonomy-label">NoSQL</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nosql-databases/" data-taxonomy-term="nosql-databases"><span class="taxonomy-label">NoSQL Databases</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nvidia/" data-taxonomy-term="nvidia"><span class="taxonomy-label">NVIDIA</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nyquist/" data-taxonomy-term="nyquist"><span class="taxonomy-label">Nyquist</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/nyquist-theorem/" data-taxonomy-term="nyquist-theorem"><span class="taxonomy-label">Nyquist Theorem</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/oci/" data-taxonomy-term="oci"><span class="taxonomy-label">OCI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/offline/" data-taxonomy-term="offline"><span class="taxonomy-label">Offline</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/offline-ai-models/" data-taxonomy-term="offline-ai-models"><span class="taxonomy-label">Offline AI Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ollama/" data-taxonomy-term="ollama"><span class="taxonomy-label">Ollama</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ollama-ai-models/" data-taxonomy-term="ollama-ai-models"><span class="taxonomy-label">Ollama AI Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-business/" data-taxonomy-term="online-business"><span class="taxonomy-label">Online Business</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-ides/" data-taxonomy-term="online-ides"><span class="taxonomy-label">Online IDEs</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-learning/" data-taxonomy-term="online-learning"><span class="taxonomy-label">Online Learning</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-marketplaces/" data-taxonomy-term="online-marketplaces"><span class="taxonomy-label">Online Marketplaces</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-platforms/" data-taxonomy-term="online-platforms"><span class="taxonomy-label">Online Platforms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-tutorial/" data-taxonomy-term="online-tutorial"><span class="taxonomy-label">Online Tutorial</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/online-video/" data-taxonomy-term="online-video"><span class="taxonomy-label">Online Video</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/onnx/" data-taxonomy-term="onnx"><span class="taxonomy-label">ONNX</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/oops/" data-taxonomy-term="oops"><span class="taxonomy-label">OOPs</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/open-source/" data-taxonomy-term="open-source"><span class="taxonomy-label">Open Source</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/open-source-ai/" data-taxonomy-term="open-source-ai"><span class="taxonomy-label">Open Source AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/open-source-models/" data-taxonomy-term="open-source-models"><span class="taxonomy-label">Open Source Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/open-webui/" data-taxonomy-term="open-webui"><span class="taxonomy-label">Open WebUI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/openai/" data-taxonomy-term="openai"><span class="taxonomy-label">OpenAI</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/operating-systems/" data-taxonomy-term="operating-systems"><span class="taxonomy-label">Operating Systems</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/optimization-theory/" data-taxonomy-term="optimization-theory"><span class="taxonomy-label">Optimization Theory</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/oracle-cloud/" data-taxonomy-term="oracle-cloud"><span class="taxonomy-label">Oracle Cloud</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/oshotalks/" data-taxonomy-term="oshotalks"><span class="taxonomy-label">OshoTalks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/others/" data-taxonomy-term="others"><span class="taxonomy-label">Others</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/paatham/" data-taxonomy-term="paatham"><span class="taxonomy-label">Paatham</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/package-management/" data-taxonomy-term="package-management"><span class="taxonomy-label">Package Management</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/paper-summary/" data-taxonomy-term="paper-summary"><span class="taxonomy-label">Paper Summary</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/papers-with-code/" data-taxonomy-term="papers-with-code"><span class="taxonomy-label">Papers With Code</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/parallel-processing/" data-taxonomy-term="parallel-processing"><span class="taxonomy-label">Parallel Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/path/" data-taxonomy-term="path"><span class="taxonomy-label">Path</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pca/" data-taxonomy-term="pca"><span class="taxonomy-label">PCA</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pep-8/" data-taxonomy-term="pep-8"><span class="taxonomy-label">PEP 8</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/performance-evaluation/" data-taxonomy-term="performance-evaluation"><span class="taxonomy-label">Performance Evaluation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/performance-metrics/" data-taxonomy-term="performance-metrics"><span class="taxonomy-label">Performance Metrics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/personal-development/" data-taxonomy-term="personal-development"><span class="taxonomy-label">Personal Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/personal-growth/" data-taxonomy-term="personal-growth"><span class="taxonomy-label">Personal Growth</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/phd-journey/" data-taxonomy-term="phd-journey"><span class="taxonomy-label">PhD Journey</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/philosophy-of-mind/" data-taxonomy-term="philosophy-of-mind"><span class="taxonomy-label">Philosophy of Mind</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/php/" data-taxonomy-term="php"><span class="taxonomy-label">PHP</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pinecone/" data-taxonomy-term="pinecone"><span class="taxonomy-label">Pinecone</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pip/" data-taxonomy-term="pip"><span class="taxonomy-label">Pip</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pipeline-orchestration/" data-taxonomy-term="pipeline-orchestration"><span class="taxonomy-label">Pipeline Orchestration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/platform-vs-engine/" data-taxonomy-term="platform-vs-engine"><span class="taxonomy-label">Platform vs Engine</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pm-tools/" data-taxonomy-term="pm-tools"><span class="taxonomy-label">PM Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/power-systems/" data-taxonomy-term="power-systems"><span class="taxonomy-label">Power Systems</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/powershell/" data-taxonomy-term="powershell"><span class="taxonomy-label">PowerShell</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/prayer/" data-taxonomy-term="prayer"><span class="taxonomy-label">Prayer</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/prayers/" data-taxonomy-term="prayers"><span class="taxonomy-label">Prayers</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/precision/" data-taxonomy-term="precision"><span class="taxonomy-label">Precision</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/precision-agriculture/" data-taxonomy-term="precision-agriculture"><span class="taxonomy-label">Precision Agriculture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/predictive-analytics/" data-taxonomy-term="predictive-analytics"><span class="taxonomy-label">Predictive Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/predictive-marketing/" data-taxonomy-term="predictive-marketing"><span class="taxonomy-label">Predictive Marketing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/predictive-modeling/" data-taxonomy-term="predictive-modeling"><span class="taxonomy-label">Predictive Modeling</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pretrained-models/" data-taxonomy-term="pretrained-models"><span class="taxonomy-label">Pretrained Models</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/privacy/" data-taxonomy-term="privacy"><span class="taxonomy-label">Privacy</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/probabilistic-models/" data-taxonomy-term="probabilistic-models"><span class="taxonomy-label">Probabilistic Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/probability-theory/" data-taxonomy-term="probability-theory"><span class="taxonomy-label">Probability Theory</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/process-automation/" data-taxonomy-term="process-automation"><span class="taxonomy-label">Process Automation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/processing/" data-taxonomy-term="processing"><span class="taxonomy-label">Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/product-development/" data-taxonomy-term="product-development"><span class="taxonomy-label">Product Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/production-ai/" data-taxonomy-term="production-ai"><span class="taxonomy-label">Production AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/production-ml/" data-taxonomy-term="production-ml"><span class="taxonomy-label">Production ML</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/professional-development/" data-taxonomy-term="professional-development"><span class="taxonomy-label">Professional Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/programming/" data-taxonomy-term="programming"><span class="taxonomy-label">Programming</span><span class="taxonomy-count">13</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/programming-tools/" data-taxonomy-term="programming-tools"><span class="taxonomy-label">Programming Tools</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/project-development/" data-taxonomy-term="project-development"><span class="taxonomy-label">Project Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/project-management/" data-taxonomy-term="project-management"><span class="taxonomy-label">Project Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/project-structure/" data-taxonomy-term="project-structure"><span class="taxonomy-label">Project Structure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/prompt-engineering/" data-taxonomy-term="prompt-engineering"><span class="taxonomy-label">Prompt Engineering</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/prompt-engineering-techniques/" data-taxonomy-term="prompt-engineering-techniques"><span class="taxonomy-label">Prompt Engineering Techniques</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/proprietary-models/" data-taxonomy-term="proprietary-models"><span class="taxonomy-label">Proprietary Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/protein-science/" data-taxonomy-term="protein-science"><span class="taxonomy-label">Protein Science</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/protocols/" data-taxonomy-term="protocols"><span class="taxonomy-label">Protocols</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/psychedelics/" data-taxonomy-term="psychedelics"><span class="taxonomy-label">Psychedelics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/psychology/" data-taxonomy-term="psychology"><span class="taxonomy-label">Psychology</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/public-administration/" data-taxonomy-term="public-administration"><span class="taxonomy-label">Public Administration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/public-services/" data-taxonomy-term="public-services"><span class="taxonomy-label">Public Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python/" data-taxonomy-term="python"><span class="taxonomy-label">Python</span><span class="taxonomy-count">8</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python-dependencies/" data-taxonomy-term="python-dependencies"><span class="taxonomy-label">Python Dependencies</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python-development/" data-taxonomy-term="python-development"><span class="taxonomy-label">Python Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python-libraries/" data-taxonomy-term="python-libraries"><span class="taxonomy-label">Python Libraries</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python-programming/" data-taxonomy-term="python-programming"><span class="taxonomy-label">Python Programming</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python-resources/" data-taxonomy-term="python-resources"><span class="taxonomy-label">Python Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/python-software-development/" data-taxonomy-term="python-software-development"><span class="taxonomy-label">Python Software Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/pytorch/" data-taxonomy-term="pytorch"><span class="taxonomy-label">PyTorch</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quality-control/" data-taxonomy-term="quality-control"><span class="taxonomy-label">Quality Control</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantization/" data-taxonomy-term="quantization"><span class="taxonomy-label">Quantization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantum-computing-and-philosophy/" data-taxonomy-term="quantum-computing-and-philosophy"><span class="taxonomy-label">Quantum Computing and Philosophy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantum-entanglement-in-daily-life/" data-taxonomy-term="quantum-entanglement-in-daily-life"><span class="taxonomy-label">Quantum Entanglement in Daily Life</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantum-mechanics/" data-taxonomy-term="quantum-mechanics"><span class="taxonomy-label">Quantum Mechanics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantum-mechanics-and-spirituality/" data-taxonomy-term="quantum-mechanics-and-spirituality"><span class="taxonomy-label">Quantum Mechanics and Spirituality</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantum-physics/" data-taxonomy-term="quantum-physics"><span class="taxonomy-label">Quantum Physics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quantum-physics-and-vedanta/" data-taxonomy-term="quantum-physics-and-vedanta"><span class="taxonomy-label">Quantum Physics and Vedanta</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/query-optimization/" data-taxonomy-term="query-optimization"><span class="taxonomy-label">Query Optimization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/question-types/" data-taxonomy-term="question-types"><span class="taxonomy-label">Question Types</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/quick-reference/" data-taxonomy-term="quick-reference"><span class="taxonomy-label">Quick Reference</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/rag/" data-taxonomy-term="rag"><span class="taxonomy-label">RAG</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ramayana/" data-taxonomy-term="ramayana"><span class="taxonomy-label">Ramayana</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ramchandra/" data-taxonomy-term="ramchandra"><span class="taxonomy-label">Ramchandra</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ramcharit-manas-chandas/" data-taxonomy-term="ramcharit-manas-chandas"><span class="taxonomy-label">Ramcharit Manas Chandas</span><span class="taxonomy-count">10</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/rastra-vandan/" data-taxonomy-term="rastra-vandan"><span class="taxonomy-label">Rastra Vandan</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/rdbms/" data-taxonomy-term="rdbms"><span class="taxonomy-label">RDBMS</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/react-library/" data-taxonomy-term="react-library"><span class="taxonomy-label">React Library</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/reactjs/" data-taxonomy-term="reactjs"><span class="taxonomy-label">Reactjs</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/reading/" data-taxonomy-term="reading"><span class="taxonomy-label">Reading</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/real-life-applications-of-mathematics/" data-taxonomy-term="real-life-applications-of-mathematics"><span class="taxonomy-label">Real-Life Applications of Mathematics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/reasoning-models/" data-taxonomy-term="reasoning-models"><span class="taxonomy-label">Reasoning Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/recall/" data-taxonomy-term="recall"><span class="taxonomy-label">Recall</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/reference-material/" data-taxonomy-term="reference-material"><span class="taxonomy-label">Reference Material</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/regression-analysis/" data-taxonomy-term="regression-analysis"><span class="taxonomy-label">Regression Analysis</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/regularization/" data-taxonomy-term="regularization"><span class="taxonomy-label">Regularization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/reinforcement-learning/" data-taxonomy-term="reinforcement-learning"><span class="taxonomy-label">Reinforcement Learning</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/reinforcement-learning-algorithms/" data-taxonomy-term="reinforcement-learning-algorithms"><span class="taxonomy-label">Reinforcement Learning Algorithms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/relational-algebra/" data-taxonomy-term="relational-algebra"><span class="taxonomy-label">Relational Algebra</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/remote-development/" data-taxonomy-term="remote-development"><span class="taxonomy-label">Remote Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/renewable-energy/" data-taxonomy-term="renewable-energy"><span class="taxonomy-label">Renewable Energy</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/repository-management/" data-taxonomy-term="repository-management"><span class="taxonomy-label">Repository Management</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research/" data-taxonomy-term="research"><span class="taxonomy-label">Research</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-collection/" data-taxonomy-term="research-collection"><span class="taxonomy-label">Research Collection</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-implementation/" data-taxonomy-term="research-implementation"><span class="taxonomy-label">Research Implementation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-links/" data-taxonomy-term="research-links"><span class="taxonomy-label">Research Links</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-methodology/" data-taxonomy-term="research-methodology"><span class="taxonomy-label">Research Methodology</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-methods/" data-taxonomy-term="research-methods"><span class="taxonomy-label">Research Methods</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-paper/" data-taxonomy-term="research-paper"><span class="taxonomy-label">Research Paper</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-papers/" data-taxonomy-term="research-papers"><span class="taxonomy-label">Research Papers</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-references/" data-taxonomy-term="research-references"><span class="taxonomy-label">Research References</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-skills/" data-taxonomy-term="research-skills"><span class="taxonomy-label">Research Skills</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/research-tools/" data-taxonomy-term="research-tools"><span class="taxonomy-label">Research Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/resource-for-python-learning/" data-taxonomy-term="resource-for-python-learning"><span class="taxonomy-label">Resource for Python Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/responsible-ai/" data-taxonomy-term="responsible-ai"><span class="taxonomy-label">Responsible AI</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/responsive-design/" data-taxonomy-term="responsive-design"><span class="taxonomy-label">Responsive Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/rest-api/" data-taxonomy-term="rest-api"><span class="taxonomy-label">REST API</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/retrieval-augmented-generation/" data-taxonomy-term="retrieval-augmented-generation"><span class="taxonomy-label">Retrieval-Augmented Generation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ripple-edit/" data-taxonomy-term="ripple-edit"><span class="taxonomy-label">Ripple Edit</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/risk-management/" data-taxonomy-term="risk-management"><span class="taxonomy-label">Risk Management</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/robust-ai/" data-taxonomy-term="robust-ai"><span class="taxonomy-label">Robust AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ruby/" data-taxonomy-term="ruby"><span class="taxonomy-label">Ruby</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/runtime-vs-ide/" data-taxonomy-term="runtime-vs-ide"><span class="taxonomy-label">Runtime vs IDE</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sahasranaamam/" data-taxonomy-term="sahasranaamam"><span class="taxonomy-label">Sahasranaamam</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sampling/" data-taxonomy-term="sampling"><span class="taxonomy-label">Sampling</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sanskrit/" data-taxonomy-term="sanskrit"><span class="taxonomy-label">Sanskrit</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sanskrit-grammar/" data-taxonomy-term="sanskrit-grammar"><span class="taxonomy-label">Sanskrit Grammar</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sanskrit-language-model/" data-taxonomy-term="sanskrit-language-model"><span class="taxonomy-label">Sanskrit Language Model</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sanskrit-literature/" data-taxonomy-term="sanskrit-literature"><span class="taxonomy-label">Sanskrit Literature</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/school-innovation/" data-taxonomy-term="school-innovation"><span class="taxonomy-label">School Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/scientific-publishing/" data-taxonomy-term="scientific-publishing"><span class="taxonomy-label">Scientific Publishing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/scripting/" data-taxonomy-term="scripting"><span class="taxonomy-label">Scripting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sdk-vs-framework/" data-taxonomy-term="sdk-vs-framework"><span class="taxonomy-label">SDK vs Framework</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sdlc/" data-taxonomy-term="sdlc"><span class="taxonomy-label">SDLC</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/search-engine/" data-taxonomy-term="search-engine"><span class="taxonomy-label">Search Engine</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/search-engine-optimization/" data-taxonomy-term="search-engine-optimization"><span class="taxonomy-label">Search Engine Optimization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/secure-user-authentication/" data-taxonomy-term="secure-user-authentication"><span class="taxonomy-label">Secure User Authentication</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security/" data-taxonomy-term="security"><span class="taxonomy-label">Security</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security-algorithms/" data-taxonomy-term="security-algorithms"><span class="taxonomy-label">Security Algorithms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security-analytics/" data-taxonomy-term="security-analytics"><span class="taxonomy-label">Security Analytics</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security-automation/" data-taxonomy-term="security-automation"><span class="taxonomy-label">Security Automation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security-best-practices/" data-taxonomy-term="security-best-practices"><span class="taxonomy-label">Security Best Practices</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security-in-web-development/" data-taxonomy-term="security-in-web-development"><span class="taxonomy-label">Security in Web Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/security-protocols/" data-taxonomy-term="security-protocols"><span class="taxonomy-label">Security Protocols</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/self-help/" data-taxonomy-term="self-help"><span class="taxonomy-label">Self Help</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sensitive-data/" data-taxonomy-term="sensitive-data"><span class="taxonomy-label">Sensitive Data</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sensitivity/" data-taxonomy-term="sensitivity"><span class="taxonomy-label">Sensitivity</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/seo/" data-taxonomy-term="seo"><span class="taxonomy-label">SEO</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/seo-tools/" data-taxonomy-term="seo-tools"><span class="taxonomy-label">SEO Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/serverless/" data-taxonomy-term="serverless"><span class="taxonomy-label">Serverless</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/serverless-computing/" data-taxonomy-term="serverless-computing"><span class="taxonomy-label">Serverless Computing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/serverless-llm/" data-taxonomy-term="serverless-llm"><span class="taxonomy-label">Serverless LLM</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/shannon/" data-taxonomy-term="shannon"><span class="taxonomy-label">Shannon</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/shared-hosting/" data-taxonomy-term="shared-hosting"><span class="taxonomy-label">Shared Hosting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/shazam/" data-taxonomy-term="shazam"><span class="taxonomy-label">Shazam</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/shell-commands/" data-taxonomy-term="shell-commands"><span class="taxonomy-label">Shell Commands</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/shell-scripting/" data-taxonomy-term="shell-scripting"><span class="taxonomy-label">Shell Scripting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/shiva/" data-taxonomy-term="shiva"><span class="taxonomy-label">Shiva</span><span class="taxonomy-count">17</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/signal/" data-taxonomy-term="signal"><span class="taxonomy-label">Signal</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/signals/" data-taxonomy-term="signals"><span class="taxonomy-label">Signals</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/similarity-measures/" data-taxonomy-term="similarity-measures"><span class="taxonomy-label">Similarity Measures</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/similarity-search/" data-taxonomy-term="similarity-search"><span class="taxonomy-label">Similarity Search</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/slip-edit/" data-taxonomy-term="slip-edit"><span class="taxonomy-label">Slip Edit</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/smart-cities/" data-taxonomy-term="smart-cities"><span class="taxonomy-label">Smart Cities</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/smart-farming/" data-taxonomy-term="smart-farming"><span class="taxonomy-label">Smart Farming</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/smart-grid/" data-taxonomy-term="smart-grid"><span class="taxonomy-label">Smart Grid</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/smart-manufacturing/" data-taxonomy-term="smart-manufacturing"><span class="taxonomy-label">Smart Manufacturing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/societal-impact/" data-taxonomy-term="societal-impact"><span class="taxonomy-label">Societal Impact</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/soft-tools/" data-taxonomy-term="soft-tools"><span class="taxonomy-label">Soft Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-architecture/" data-taxonomy-term="software-architecture"><span class="taxonomy-label">Software Architecture</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-collaboration/" data-taxonomy-term="software-collaboration"><span class="taxonomy-label">Software Collaboration</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-design/" data-taxonomy-term="software-design"><span class="taxonomy-label">Software Design</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-development/" data-taxonomy-term="software-development"><span class="taxonomy-label">Software Development</span><span class="taxonomy-count">13</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-development-toolsets/" data-taxonomy-term="software-development-toolsets"><span class="taxonomy-label">Software Development Toolsets</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-distribution/" data-taxonomy-term="software-distribution"><span class="taxonomy-label">Software Distribution</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-engineering/" data-taxonomy-term="software-engineering"><span class="taxonomy-label">Software Engineering</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/software-installation/" data-taxonomy-term="software-installation"><span class="taxonomy-label">Software Installation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sorting/" data-taxonomy-term="sorting"><span class="taxonomy-label">Sorting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/space/" data-taxonomy-term="space"><span class="taxonomy-label">Space</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/speech-recognition/" data-taxonomy-term="speech-recognition"><span class="taxonomy-label">Speech Recognition</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/spiritual-science/" data-taxonomy-term="spiritual-science"><span class="taxonomy-label">Spiritual Science</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sql/" data-taxonomy-term="sql"><span class="taxonomy-label">SQL</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/stanford-alpaca/" data-taxonomy-term="stanford-alpaca"><span class="taxonomy-label">Stanford Alpaca</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/state-of-the-art-ai/" data-taxonomy-term="state-of-the-art-ai"><span class="taxonomy-label">State of the Art AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/static-site-comments/" data-taxonomy-term="static-site-comments"><span class="taxonomy-label">Static Site Comments</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/static-site-generator/" data-taxonomy-term="static-site-generator"><span class="taxonomy-label">Static Site Generator</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistical-analysis/" data-taxonomy-term="statistical-analysis"><span class="taxonomy-label">Statistical Analysis</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistical-distributions/" data-taxonomy-term="statistical-distributions"><span class="taxonomy-label">Statistical Distributions</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistical-learning/" data-taxonomy-term="statistical-learning"><span class="taxonomy-label">Statistical Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistical-methods/" data-taxonomy-term="statistical-methods"><span class="taxonomy-label">Statistical Methods</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistical-terms/" data-taxonomy-term="statistical-terms"><span class="taxonomy-label">Statistical Terms</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistics/" data-taxonomy-term="statistics"><span class="taxonomy-label">Statistics</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistics-for-data-science/" data-taxonomy-term="statistics-for-data-science"><span class="taxonomy-label">Statistics for Data Science</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistics-guide/" data-taxonomy-term="statistics-guide"><span class="taxonomy-label">Statistics Guide</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/stotram/" data-taxonomy-term="stotram"><span class="taxonomy-label">Stotram</span><span class="taxonomy-count">17</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/study-materials/" data-taxonomy-term="study-materials"><span class="taxonomy-label">Study Materials</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/suktam/" data-taxonomy-term="suktam"><span class="taxonomy-label">Suktam</span><span class="taxonomy-count">10</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sunita-william/" data-taxonomy-term="sunita-william"><span class="taxonomy-label">Sunita William</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/superposition-and-vedantic-thought/" data-taxonomy-term="superposition-and-vedantic-thought"><span class="taxonomy-label">Superposition and Vedantic Thought</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/supervised-learning/" data-taxonomy-term="supervised-learning"><span class="taxonomy-label">Supervised Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sustainability/" data-taxonomy-term="sustainability"><span class="taxonomy-label">Sustainability</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sustainable-agriculture/" data-taxonomy-term="sustainable-agriculture"><span class="taxonomy-label">Sustainable Agriculture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/sustainable-ai/" data-taxonomy-term="sustainable-ai"><span class="taxonomy-label">Sustainable AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/swami-vivekananda/" data-taxonomy-term="swami-vivekananda"><span class="taxonomy-label">Swami Vivekananda</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/synthetic-data/" data-taxonomy-term="synthetic-data"><span class="taxonomy-label">Synthetic Data</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/system-administration/" data-taxonomy-term="system-administration"><span class="taxonomy-label">System Administration</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/system-architecture/" data-taxonomy-term="system-architecture"><span class="taxonomy-label">System Architecture</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/system-design/" data-taxonomy-term="system-design"><span class="taxonomy-label">System Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/system-settings/" data-taxonomy-term="system-settings"><span class="taxonomy-label">System Settings</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/systematic-review/" data-taxonomy-term="systematic-review"><span class="taxonomy-label">Systematic Review</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/systems/" data-taxonomy-term="systems"><span class="taxonomy-label">Systems</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/t-sne/" data-taxonomy-term="t-sne"><span class="taxonomy-label">T-SNE</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/taitriya/" data-taxonomy-term="taitriya"><span class="taxonomy-label">Taitriya</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/teaching-technology/" data-taxonomy-term="teaching-technology"><span class="taxonomy-label">Teaching Technology</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tech-categories/" data-taxonomy-term="tech-categories"><span class="taxonomy-label">Tech Categories</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tech-future/" data-taxonomy-term="tech-future"><span class="taxonomy-label">Tech Future</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tech-infrastructure/" data-taxonomy-term="tech-infrastructure"><span class="taxonomy-label">Tech Infrastructure</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-documentation/" data-taxonomy-term="technical-documentation"><span class="taxonomy-label">Technical Documentation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-glossary/" data-taxonomy-term="technical-glossary"><span class="taxonomy-label">Technical Glossary</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-guide/" data-taxonomy-term="technical-guide"><span class="taxonomy-label">Technical Guide</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-interview/" data-taxonomy-term="technical-interview"><span class="taxonomy-label">Technical Interview</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-interviews/" data-taxonomy-term="technical-interviews"><span class="taxonomy-label">Technical Interviews</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-resources/" data-taxonomy-term="technical-resources"><span class="taxonomy-label">Technical Resources</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technical-writing/" data-taxonomy-term="technical-writing"><span class="taxonomy-label">Technical Writing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technological-change/" data-taxonomy-term="technological-change"><span class="taxonomy-label">Technological Change</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technological-evolution/" data-taxonomy-term="technological-evolution"><span class="taxonomy-label">Technological Evolution</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technological-progress/" data-taxonomy-term="technological-progress"><span class="taxonomy-label">Technological Progress</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-articles/" data-taxonomy-term="technology-articles"><span class="taxonomy-label">Technology Articles</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-ethics/" data-taxonomy-term="technology-ethics"><span class="taxonomy-label">Technology Ethics</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-evolution/" data-taxonomy-term="technology-evolution"><span class="taxonomy-label">Technology Evolution</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-impact/" data-taxonomy-term="technology-impact"><span class="taxonomy-label">Technology Impact</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-innovation/" data-taxonomy-term="technology-innovation"><span class="taxonomy-label">Technology Innovation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-integration/" data-taxonomy-term="technology-integration"><span class="taxonomy-label">Technology Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-overview/" data-taxonomy-term="technology-overview"><span class="taxonomy-label">Technology Overview</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-selection/" data-taxonomy-term="technology-selection"><span class="taxonomy-label">Technology Selection</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-services/" data-taxonomy-term="technology-services"><span class="taxonomy-label">Technology Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/technology-trends/" data-taxonomy-term="technology-trends"><span class="taxonomy-label">Technology Trends</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/templating/" data-taxonomy-term="templating"><span class="taxonomy-label">Templating</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/templating-language/" data-taxonomy-term="templating-language"><span class="taxonomy-label">Templating Language</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tensorflow/" data-taxonomy-term="tensorflow"><span class="taxonomy-label">TensorFlow</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/text-analysis/" data-taxonomy-term="text-analysis"><span class="taxonomy-label">Text Analysis</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/text-corpus/" data-taxonomy-term="text-corpus"><span class="taxonomy-label">Text Corpus</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/text-generation/" data-taxonomy-term="text-generation"><span class="taxonomy-label">Text Generation</span><span class="taxonomy-count">8</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/text-mining/" data-taxonomy-term="text-mining"><span class="taxonomy-label">Text Mining</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/text-processing/" data-taxonomy-term="text-processing"><span class="taxonomy-label">Text Processing</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/textured/" data-taxonomy-term="textured"><span class="taxonomy-label">Textured</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/the-art-of-prompting/" data-taxonomy-term="the-art-of-prompting"><span class="taxonomy-label">The Art of Prompting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/theoretical/" data-taxonomy-term="theoretical"><span class="taxonomy-label">Theoretical</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/thirukkural-in-hindi/" data-taxonomy-term="thirukkural-in-hindi"><span class="taxonomy-label">Thirukkural in Hindi</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/thiruvalluvar/" data-taxonomy-term="thiruvalluvar"><span class="taxonomy-label">Thiruvalluvar</span><span class="taxonomy-count">4</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/threat-detection/" data-taxonomy-term="threat-detection"><span class="taxonomy-label">Threat Detection</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/time/" data-taxonomy-term="time"><span class="taxonomy-label">Time</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/time-series-analysis/" data-taxonomy-term="time-series-analysis"><span class="taxonomy-label">Time Series Analysis</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/timeline/" data-taxonomy-term="timeline"><span class="taxonomy-label">Timeline</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tokenization/" data-taxonomy-term="tokenization"><span class="taxonomy-label">Tokenization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tools/" data-taxonomy-term="tools"><span class="taxonomy-label">Tools</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/topic-modeling/" data-taxonomy-term="topic-modeling"><span class="taxonomy-label">Topic Modeling</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/tracks/" data-taxonomy-term="tracks"><span class="taxonomy-label">Tracks</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/transfer-learning/" data-taxonomy-term="transfer-learning"><span class="taxonomy-label">Transfer Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/transformer/" data-taxonomy-term="transformer"><span class="taxonomy-label">Transformer</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/transformer-architecture/" data-taxonomy-term="transformer-architecture"><span class="taxonomy-label">Transformer Architecture</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/transformer-models/" data-taxonomy-term="transformer-models"><span class="taxonomy-label">Transformer Models</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/transformers/" data-taxonomy-term="transformers"><span class="taxonomy-label">Transformers</span><span class="taxonomy-count">5</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/trims/" data-taxonomy-term="trims"><span class="taxonomy-label">Trims</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/truthfulness-and-hallucinations/" data-taxonomy-term="truthfulness-and-hallucinations"><span class="taxonomy-label">Truthfulness and Hallucinations</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/type-of-reasoning/" data-taxonomy-term="type-of-reasoning"><span class="taxonomy-label">Type of Reasoning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/type-script/" data-taxonomy-term="type-script"><span class="taxonomy-label">Type Script</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/types-of-web-hosting/" data-taxonomy-term="types-of-web-hosting"><span class="taxonomy-label">Types of Web Hosting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/typesetting/" data-taxonomy-term="typesetting"><span class="taxonomy-label">Typesetting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ubuntu/" data-taxonomy-term="ubuntu"><span class="taxonomy-label">Ubuntu</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ui-components/" data-taxonomy-term="ui-components"><span class="taxonomy-label">UI Components</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ui-design/" data-taxonomy-term="ui-design"><span class="taxonomy-label">UI Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/unfiltered-ai-outputs/" data-taxonomy-term="unfiltered-ai-outputs"><span class="taxonomy-label">Unfiltered AI Outputs</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/unicode/" data-taxonomy-term="unicode"><span class="taxonomy-label">Unicode</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/unix/" data-taxonomy-term="unix"><span class="taxonomy-label">Unix</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/unsupervised-learning/" data-taxonomy-term="unsupervised-learning"><span class="taxonomy-label">Unsupervised Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/upanishad/" data-taxonomy-term="upanishad"><span class="taxonomy-label">Upanishad</span><span class="taxonomy-count">30</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/user-authorization/" data-taxonomy-term="user-authorization"><span class="taxonomy-label">User Authorization</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/user-profiles/" data-taxonomy-term="user-profiles"><span class="taxonomy-label">User Profiles</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/ux-design/" data-taxonomy-term="ux-design"><span class="taxonomy-label">UX Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/value-based-vs.-policy-based-methods/" data-taxonomy-term="value-based-vs.-policy-based-methods"><span class="taxonomy-label">Value-Based vs. Policy-Based Methods</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vector-database/" data-taxonomy-term="vector-database"><span class="taxonomy-label">Vector Database</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vector-databases/" data-taxonomy-term="vector-databases"><span class="taxonomy-label">Vector Databases</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vector-embedding/" data-taxonomy-term="vector-embedding"><span class="taxonomy-label">Vector Embedding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vector-representation/" data-taxonomy-term="vector-representation"><span class="taxonomy-label">Vector Representation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vector-search/" data-taxonomy-term="vector-search"><span class="taxonomy-label">Vector Search</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vedic-science/" data-taxonomy-term="vedic-science"><span class="taxonomy-label">Vedic Science</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/version-control/" data-taxonomy-term="version-control"><span class="taxonomy-label">Version Control</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vertex-ai/" data-taxonomy-term="vertex-ai"><span class="taxonomy-label">Vertex AI</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vertexai/" data-taxonomy-term="vertexai"><span class="taxonomy-label">VertexAI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/video-editing-concepts/" data-taxonomy-term="video-editing-concepts"><span class="taxonomy-label">Video Editing Concepts</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/video-processing/" data-taxonomy-term="video-processing"><span class="taxonomy-label">Video Processing</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vipasana/" data-taxonomy-term="vipasana"><span class="taxonomy-label">Vipasana</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/virtual-environment/" data-taxonomy-term="virtual-environment"><span class="taxonomy-label">Virtual Environment</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/virtual-environments/" data-taxonomy-term="virtual-environments"><span class="taxonomy-label">Virtual Environments</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/virtual-representation/" data-taxonomy-term="virtual-representation"><span class="taxonomy-label">Virtual Representation</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vishnu/" data-taxonomy-term="vishnu"><span class="taxonomy-label">Vishnu</span><span class="taxonomy-count">6</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/visual-studio-code/" data-taxonomy-term="visual-studio-code"><span class="taxonomy-label">Visual Studio Code</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/vps-hosting/" data-taxonomy-term="vps-hosting"><span class="taxonomy-label">VPS Hosting</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/weak-supervision/" data-taxonomy-term="weak-supervision"><span class="taxonomy-label">Weak Supervision</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-applications/" data-taxonomy-term="web-applications"><span class="taxonomy-label">Web Applications</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-design/" data-taxonomy-term="web-design"><span class="taxonomy-label">Web Design</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-development/" data-taxonomy-term="web-development"><span class="taxonomy-label">Web Development</span><span class="taxonomy-count">7</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-hosting-services/" data-taxonomy-term="web-hosting-services"><span class="taxonomy-label">Web Hosting Services</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-resources/" data-taxonomy-term="web-resources"><span class="taxonomy-label">Web Resources</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-servers/" data-taxonomy-term="web-servers"><span class="taxonomy-label">Web Servers</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-services/" data-taxonomy-term="web-services"><span class="taxonomy-label">Web Services</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/web-based-coding/" data-taxonomy-term="web-based-coding"><span class="taxonomy-label">Web-Based Coding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/what-is-life/" data-taxonomy-term="what-is-life"><span class="taxonomy-label">What Is Life</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/whatsapp-integration/" data-taxonomy-term="whatsapp-integration"><span class="taxonomy-label">WhatsApp Integration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/whisper/" data-taxonomy-term="whisper"><span class="taxonomy-label">Whisper</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/why-ai-project-fail/" data-taxonomy-term="why-ai-project-fail"><span class="taxonomy-label">Why AI Project Fail?</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/wi-fi/" data-taxonomy-term="wi-fi"><span class="taxonomy-label">Wi-Fi</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/windows/" data-taxonomy-term="windows"><span class="taxonomy-label">Windows</span><span class="taxonomy-count">3</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/windows-11/" data-taxonomy-term="windows-11"><span class="taxonomy-label">Windows 11</span><span class="taxonomy-count">2</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/windows-configuration/" data-taxonomy-term="windows-configuration"><span class="taxonomy-label">Windows Configuration</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/wireless-communication/" data-taxonomy-term="wireless-communication"><span class="taxonomy-label">Wireless Communication</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/word-embedding/" data-taxonomy-term="word-embedding"><span class="taxonomy-label">Word Embedding</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/word-embeddings/" data-taxonomy-term="word-embeddings"><span class="taxonomy-label">Word Embeddings</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/workflows/" data-taxonomy-term="workflows"><span class="taxonomy-label">Workflows</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/workforce-development/" data-taxonomy-term="workforce-development"><span class="taxonomy-label">Workforce Development</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/wsl/" data-taxonomy-term="wsl"><span class="taxonomy-label">WSL</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/wsl2/" data-taxonomy-term="wsl2"><span class="taxonomy-label">WSL2</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/xai/" data-taxonomy-term="xai"><span class="taxonomy-label">XAI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/youtube-channels-for-ai/" data-taxonomy-term="youtube-channels-for-ai"><span class="taxonomy-label">YouTube Channels for AI</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/youtube-channels-for-data-scientist/" data-taxonomy-term="youtube-channels-for-data-scientist"><span class="taxonomy-label">YouTube Channels for Data Scientist</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/zero-shot-learning/" data-taxonomy-term="zero-shot-learning"><span class="taxonomy-label">Zero Shot Learning</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/%E0%A4%86%E0%A4%A4%E0%A5%8D%E0%A4%AE%E0%A4%B7%E0%A4%9F%E0%A5%8D%E0%A4%95%E0%A4%AE%E0%A5%8D/" data-taxonomy-term="%E0%A4%86%E0%A4%A4%E0%A5%8D%E0%A4%AE%E0%A4%B7%E0%A4%9F%E0%A5%8D%E0%A4%95%E0%A4%AE%E0%A5%8D"><span class="taxonomy-label">आत्मषट्कम्</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/%E0%A4%A4%E0%A5%88%E0%A4%A4%E0%A5%8D%E0%A4%A4%E0%A4%BF%E0%A4%B0%E0%A5%80%E0%A4%AF%E0%A5%8B%E0%A4%AA%E0%A4%A8%E0%A4%BF%E0%A4%B7%E0%A4%A4%E0%A5%8D/" data-taxonomy-term="%E0%A4%A4%E0%A5%88%E0%A4%A4%E0%A5%8D%E0%A4%A4%E0%A4%BF%E0%A4%B0%E0%A5%80%E0%A4%AF%E0%A5%8B%E0%A4%AA%E0%A4%A8%E0%A4%BF%E0%A4%B7%E0%A4%A4%E0%A5%8D"><span class="taxonomy-label">तैत्तिरीयोपनिषत्</span><span class="taxonomy-count">1</span></a></li>
        <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/%E0%A4%A8%E0%A4%BF%E0%A4%B0%E0%A5%8D%E0%A4%B5%E0%A4%BE%E0%A4%A3%E0%A4%B7%E0%A4%9F%E0%A5%8D%E0%A4%95%E0%A4%AE%E0%A5%8D/" data-taxonomy-term="%E0%A4%A8%E0%A4%BF%E0%A4%B0%E0%A5%8D%E0%A4%B5%E0%A4%BE%E0%A4%A3%E0%A4%B7%E0%A4%9F%E0%A5%8D%E0%A4%95%E0%A4%AE%E0%A5%8D"><span class="taxonomy-label">निर्वाणषट्कम्</span><span class="taxonomy-count">1</span></a></li>
        </ul>
    </div>
  

          </aside>
          <main class="col-12 col-md-9 col-xl-8 ps-md-5" role="main">
            
  

            <nav aria-label="breadcrumb" class="td-breadcrumbs">
  <ol class="breadcrumb">
  <li class="breadcrumb-item">
    <a href="/site/docs/">Documentation</a></li>
  <li class="breadcrumb-item">
    <a href="/site/docs/dsblog/">DSBlog</a></li>
  <li class="breadcrumb-item active" aria-current="page">
    300 Important Statistical Terms</li>
  </ol>
</nav>
            
<div class="td-content">
	<h1>300 Important Statistical Terms</h1>
	
	<header class="article-meta">
		<div class="taxonomy taxonomy-terms-article taxo-categories">
  <h5 class="taxonomy-title">Categories:</h5>
  <ul class="taxonomy-terms">
    <li><a class="taxonomy-term" href="http://localhost:1313/site/categories/dsblog/" data-taxonomy-term="dsblog"><span class="taxonomy-label">Dsblog</span></a></li>
    </ul>
</div>
<div class="taxonomy taxonomy-terms-article taxo-tags">
  <h5 class="taxonomy-title">Tags:</h5>
  <ul class="taxonomy-terms">
    <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistical-terms/" data-taxonomy-term="statistical-terms"><span class="taxonomy-label">Statistical Terms</span></a></li>
    <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistics/" data-taxonomy-term="statistics"><span class="taxonomy-label">Statistics</span></a></li>
    <li><a class="taxonomy-term" href="http://localhost:1313/site/tags/statistics-for-data-science/" data-taxonomy-term="statistics-for-data-science"><span class="taxonomy-label">Statistics for Data Science</span></a></li>
    </ul>
</div>

</header>
	<h1 id="important-statistical-terms">Important Statistical Terms</h1>
<p><img src="/assets/images/dspost/dsp6011-300&#43;Important-Statitical-Terms.jpg" alt="300 Important Statistical Terms"></p>
<table>
  <thead>
      <tr>
          <th>Sno</th>
          <th>Term</th>
          <th>Definition</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1</td>
          <td>Affine transformation.</td>
          <td>See transformation.</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Affirming the antecedent.</td>
          <td>A valid logical argument that concludes from the premise A → B and the premise A that therefore, B is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the antecedent (A) in the conditional.</td>
      </tr>
      <tr>
          <td>3</td>
          <td>Affirming the consequent.</td>
          <td>A logical fallacy that argues from the premise A → B and the premise B that therefore, A is true. The name comes from the fact that the argument affirms (i.e., asserts as true) the consequent (B) in the conditional.</td>
      </tr>
      <tr>
          <td>4</td>
          <td>Alternative Hypothesis.</td>
          <td>In hypothesis testing, a null hypothesis (typically that there is no effect) is compared with an alternative hypothesis (typically that there is an effect, or that there is an effect of a particular sign). For example, in evaluating whether a new cancer remedy works, the null hypothesis typically would be that the remedy does not work, while the alternative hypothesis would be that the remedy does work. When the data are sufficiently improbable under the assumption that the null hypothesis is true, the null hypothesis is rejected in favor of the alternative hypothesis. (This does not imply that the data are probable under the assumption that the alternative hypothesis is true, nor that the null hypothesis is false, nor that the alternative hypothesis is true. Confused? Take a course in Statistics!)</td>
      </tr>
      <tr>
          <td>5</td>
          <td>Ante.</td>
          <td>The up-front cost of a bet: the money you must pay to play the game. From Latin for “before.”</td>
      </tr>
      <tr>
          <td>6</td>
          <td>Antecedent.</td>
          <td>In a conditional p → q, the antecedent is p.</td>
      </tr>
      <tr>
          <td>7</td>
          <td>Appeal to Ignorance.</td>
          <td>A logical fallacy: taking the absence of evidence to be evidence of absence. If something is not known to be false, assume that it is true; or if something is not known to be true, assume that it is false. For example, if I have no reason to think that anyone in Tajikistan wish me well, that is not evidence that nobody in Tajikistan wishes me well.</td>
      </tr>
      <tr>
          <td>8</td>
          <td>Association.</td>
          <td>Two variables are associated if some of the variability of one can be accounted for by the other. In a scatterplot of the two variables, if the scatter in the values of the variable plotted on the vertical axis is smaller in narrow ranges of the variable plotted on the horizontal axis (i.e., in vertical “slices”) than it is overall, the two variables are associated. The correlation coefficient is a measure of linear association, which is a special case of association in which large values of one variable tend to occur with large values of the other, and small values of one tend to occur with small values of the other (positive association), or in which large values of one tend to occur with small values of the other, and vice versa (negative association).</td>
      </tr>
      <tr>
          <td>9</td>
          <td>Average.</td>
          <td>An ambiguous term. It often denotes the arithmetic mean, but it can also denote the median, the mode, the geometric mean, and weighted means, among other things. Beware if something reports “the average” without making it clear which average.</td>
      </tr>
      <tr>
          <td>10</td>
          <td>Axioms of Probability.</td>
          <td>There are three axioms of probability: (1) Chances are always at least zero. (2) The chance that something happens is 100%. (3) If two events cannot both occur at the same time (if they are disjoint or mutually exclusive), the chance that either one occurs is the sum of the chances that each occurs. For example, consider an experiment that consists of tossing a coin once. The first axiom says that the chance that the coin lands heads, for instance, must be at least zero. The second axiom says that the chance that the coin either lands heads or lands tails or lands on its edge or doesn’t land at all is 100%. The third axiom says that the chance that the coin either lands heads or lands tails is the sum of the chance that the coin lands heads and the chance that the coin lands tails, because both cannot occur in the same coin toss. All other mathematical facts about probability can be derived from these three axioms. For example, it is true that the chance that an event does not occur is (100% − the chance that the event occurs). This is a consequence of the second and third axioms.</td>
      </tr>
      <tr>
          <td>11</td>
          <td>Base rate fallacy.</td>
          <td>The base rate fallacy consists of failing to take into account prior probabilities (base rates) when computing conditional probabilities from other conditional probabilities. It is related to the Prosecutor’s Fallacy. For instance, suppose that a test for the presence of some condition has a 1% chance of a false positive result (the test says the condition is present when it is not) and a 1% chance of a false negative result (the test says the condition is absent when the condition is present), so the exam is 99% accurate. What is the chance that an item that tests positive really has the condition? The intuitive answer is 99%, but that is not necessarily true: the correct answer depends on the fraction f of items in the population that have the condition (and on whether the item tested is selected at random from the population). The chance that a randomly selected item tests positive is 0.99×f/(0.99×f + 0.01×(1−f)), which could be much smaller than 99% if f is small. See Bayes’ Rule.</td>
      </tr>
      <tr>
          <td>12</td>
          <td>Bayes’ Rule.</td>
          <td>Bayes’ rule expresses the conditional probability of the event A given the event B in terms of the conditional probability of the event B given the event A and the unconditional probability of A: P(A/B) = P(B/A) ×P(A)/( P(B/A)×P(A) + P(B/Ac) ×P(Ac) ). In this expression, the unconditional probability of A is also called the prior probability of A, because it is the probability assigned to A prior to observing any data. Similarly, in this context, P(A/B) is called the posterior probability of A given B, because it is the probability of A updated to reflect (i.e., to condition on) the fact that B was observed to occur.</td>
      </tr>
      <tr>
          <td>13</td>
          <td>Bernoulli’s Inequality.</td>
          <td>The Bernoulli Inequality says that if x ≥ −1 then (1+x)n ≥ 1 + nx for every integer n ≥ 0. If n is even, the inequality holds for all x.</td>
      </tr>
      <tr>
          <td>14</td>
          <td>Bias.</td>
          <td>A measurement procedure or estimator is said to be biased if, on the average, it gives an answer that differs from the truth. The bias is the average (expected) difference between the measurement and the truth. For example, if you get on the scale with clothes on, that biases the measurement to be larger than your true weight (this would be a positive bias). The design of an experiment or of a survey can also lead to bias. Bias can be deliberate, but it is not necessarily so. See also nonresponse bias.</td>
      </tr>
      <tr>
          <td>15</td>
          <td>Bimodal.</td>
          <td>Having two modes.</td>
      </tr>
      <tr>
          <td>16</td>
          <td>Bin.</td>
          <td>See class interval.</td>
      </tr>
      <tr>
          <td>17</td>
          <td>Binomial Coefficient.</td>
          <td>See combinations.</td>
      </tr>
      <tr>
          <td>18</td>
          <td>Binomial Distribution.</td>
          <td>A random variable has a binomial distribution (with parameters n and p) if it is the number of “successes” in a fixed number n of independent random trials, all of which have the same probability p of resulting in “success.” Under these assumptions, the probability of k successes (and n−k failures) is nCk pk(1−p)n−k, where nCk is the number of combinations of n objects taken k at a time: nCk = n!/(k!(n−k)!). The expected value of a random variable with the Binomial distribution is n×p, and the standard error of a random variable with the Binomial distribution is (n×p×(1 − p))½. This page shows the probability histogram of the binomial distribution.</td>
      </tr>
      <tr>
          <td>19</td>
          <td>Binomial Theorem.</td>
          <td>The Binomial theorem says that (x+y)n = xn + nxn−1y + … + nCkxn−kyk + … + yn.</td>
      </tr>
      <tr>
          <td>20</td>
          <td>Bivariate.</td>
          <td>Having or having to do with two variables. For example, bivariate data are data where we have two measurements of each “individual.” These measurements might be the heights and weights of a group of people (an “individual” is a person), the heights of fathers and sons (an “individual” is a father-son pair), the pressure and temperature of a fixed volume of gas (an “individual” is the volume of gas under a certain set of experimental conditions), etc. Scatterplots, the correlation coefficient, and regression make sense for bivariate data but not univariate data. C.f. univariate.</td>
      </tr>
      <tr>
          <td>21</td>
          <td>Blind, Blind Experiment.</td>
          <td>In a blind experiment, the subjects do not know whether they are in the treatment group or the control group. In order to have a blind experiment with human subjects, it is usually necessary to administer a placebo to the control group.</td>
      </tr>
      <tr>
          <td>22</td>
          <td>Bootstrap estimate of Standard Error.</td>
          <td>The name for this idea comes from the idiom “to pull oneself up by one’s bootstraps,” which connotes getting out of a hole without anything to stand on. The idea of the bootstrap is to assume, for the purposes of estimating uncertainties, that the sample is the population, then use the SE for sampling from the sample to estimate the SE of sampling from the population. For sampling from a box of numbers, the SD of the sample is the bootstrap estimate of the SD of the box from which the sample is drawn. For sample percentages, this takes a particularly simple form: the SE of the sample percentage of n draws from a box, with replacement, is SD(box)/n½, where for a box that contains only zeros and ones, SD(box) = ((fraction of ones in box)×(fraction of zeros in box) )½. The bootstrap estimate of the SE of the sample percentage consists of estimating SD(box) by ((fraction of ones in sample)×(fraction of zeros in sample))½. When the sample size is large, this approximation is likely to be good.</td>
      </tr>
      <tr>
          <td>23</td>
          <td>Box model.</td>
          <td>An analogy between an experiment and drawing numbered tickets “at random” from a box with replacement. For example, suppose we are trying to evaluate a cold remedy by giving it or a placebo to a group of n individuals, randomly choosing half the individuals to receive the remedy and half to receive the placebo. Consider the median time to recovery for all the individuals (we assume everyone recovers from the cold eventually; to simplify things, we also assume that no one recovered in exactly the median time, and that n is even). By definition, half the individuals got better in less than the median time, and half in more than the median time. The individuals who received the treatment are a random sample of size n/2 from the set of n subjects, half of whom got better in less than median time, and half in longer than median time. If the remedy is ineffective, the number of subjects who received the remedy and who recovered in less than median time is like the sum of n/2 draws with replacement from a box with two tickets in it: one with a “1” on it, and one with a “0” on it. This page illustrates the sampling distribution of random draws with or without from a box of numbered tickets.</td>
      </tr>
      <tr>
          <td>24</td>
          <td>Breakdown Point.</td>
          <td>The breakdown point of an estimator is the smallest fraction of observations one must corrupt to make the estimator take any value one wants.</td>
      </tr>
      <tr>
          <td>25</td>
          <td>Categorical Variable.</td>
          <td>A variable whose value ranges over categories, such as {red, green, blue}, {male, female}, {Arizona, California, Montana, New York}, {short, tall}, {Asian, African-American, Caucasian, Hispanic, Native American, Polynesian}, {straight, curly}, etc. Some categorical variables are ordinal. The distinction between categorical variables and qualitative variables is a bit blurry. C.f. quantitative variable.</td>
      </tr>
      <tr>
          <td>26</td>
          <td>Causation, causal relation.</td>
          <td>Two variables are causally related if changes in the value of one cause the other to change. For example, if one heats a rigid container filled with a gas, that causes the pressure of the gas in the container to increase. Two variables can be associated without having any causal relation, and even if two variables have a causal relation, their correlation can be small or zero.</td>
      </tr>
      <tr>
          <td>27</td>
          <td>Central Limit Theorem.</td>
          <td>The central limit theorem states that the probability histograms of the sample mean and sample sum of n draws with replacement from a box of labeled tickets converge to a normal curve as the sample size n grows, in the following sense: As n grows, the area of the probability histogram for any range of values approaches the area under the normal curve for the same range of values, converted to standard units. See also the normal approximation.</td>
      </tr>
      <tr>
          <td>28</td>
          <td>Certain Event.</td>
          <td>An event is certain if its probability is 100%. Even if an event is certain, it might not occur. However, by the complement rule, the chance that it does not occur is 0%.</td>
      </tr>
      <tr>
          <td>29</td>
          <td>Chance variation, chance error.</td>
          <td>A random variable can be decomposed into a sum of its expected value and chance variation around its expected value. The expected value of the chance variation is zero; the standard error of the chance variation is the same as the standard error of the random variable—the size of a “typical” difference between the random variable and its expected value. See also sampling error.</td>
      </tr>
      <tr>
          <td>30</td>
          <td>Change of Units or Variables.</td>
          <td>See also transformation.</td>
      </tr>
      <tr>
          <td>31</td>
          <td>Chebychev’s Inequality.</td>
          <td>For lists: For every number k&gt;0, the fraction of elements in a list that are k SD’s or further from the arithmetic mean of the list is at most 1/k2. For random variables: For every number k&gt;0, the probability that a random variable X is k SEs or further from its expected value is at most 1/k2.</td>
      </tr>
      <tr>
          <td>32</td>
          <td>Chi-square curve.</td>
          <td>The chi-square curve is a family of curves that depend on a parameter called degrees of freedom (d.f.). The chi-square curve is an approximation to the probability histogram of the chi-square statistic for multinomial model if the expected number of outcomes in each category is large. The chi-square curve is positive, and its total area is 100%, so we can think of it as the probability histogram of a random variable. The balance point of the curve is d.f., so the expected value of the corresponding random variable would equal d.f.. The standard error of the corresponding random variable would be (2×d.f.)½. As d.f. grows, the shape of the chi-square curve approaches the shape of the normal curve. This page shows the chi-square curve.</td>
      </tr>
      <tr>
          <td>33</td>
          <td>Chi-square Statistic.</td>
          <td>The chi-square statistic is used to measure the agreement between categorical data and a multinomial model that predicts the relative frequency of outcomes in each possible category. Suppose there are n independent trials, each of which can result in one of k possible outcomes. Suppose that in each trial, the probability that outcome i occurs is pi, for i = 1, 2, … , k, and that these probabilities are the same in every trial. The expected number of times outcome 1 occurs in the n trials is n×p1; more generally, the expected number of times outcome i occurs is expectedi = n×pi. If the model be correct, we would expect the n trials to result in outcome i about n×pi times, give or take a bit. Let observedi denote the number of times an outcome of type i occurs in the n trials, for i = 1, 2, … , k. The chi-squared statistic summarizes the discrepancies between the expected number of times each outcome occurs (assuming that the model is true) and the observed number of times each outcome occurs, by summing the squares of the discrepancies, normalized by the expected numbers, over all the categories: chi-squared = (observed1 − expected1)2/expected1 + (observed2 − expected2)2/expected2 + … + (observedk − expectedk)2/expectedk. As the sample size n increases, if the model is correct, the sampling distribution of the chi-squared statistic is approximated increasingly well by the chi-squared curve with (#categories − 1) = k − 1 degrees of freedom (d.f.), in the sense that the chance that the chi-squared statistic is in any given range grows closer and closer to the area under the Chi-Squared curve over the same range. This page illustrates the sampling distribution of the chi-square statistic.</td>
      </tr>
      <tr>
          <td>34</td>
          <td>Class Boundary.</td>
          <td>A point that is the left endpoint of one class interval, and the right endpoint of another class interval.</td>
      </tr>
      <tr>
          <td>35</td>
          <td>Class Interval.</td>
          <td>In plotting a histogram, one starts by dividing the range of values into a set of non-overlapping intervals, called class intervals, in such a way that every datum is contained in some class interval. See the related entries class boundary and endpoint convention.</td>
      </tr>
      <tr>
          <td>36</td>
          <td>Cluster Sample.</td>
          <td>In a cluster sample, the sampling unit is a collection of population units, not single population units. For example, techniques for adjusting the U.S. census start with a sample of geographic blocks, then (try to) enumerate all inhabitants of the blocks in the sample to obtain a sample of people. This is an example of a cluster sample. (The blocks are chosen separately from different strata, so the overall design is a stratified cluster sample.)</td>
      </tr>
      <tr>
          <td>37</td>
          <td>Combinations.</td>
          <td>The number of combinations of n things taken k at a time is the number of ways of picking a subset of k of the n things, without replacement, and without regard to the order in which the elements of the subset are picked. The number of such combinations is nCk = n!/(k!(n−k)!), where k! (pronounced “k factorial”) is k×(k−1)×(k−2)× … × 1. The numbers nCk are also called the Binomial coefficients. From a set that has n elements one can form a total of 2n subsets of all sizes. For example, from the set {a, b, c}, which has 3 elements, one can form the 23 = 8 subsets {}, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}. Because the number of subsets with k elements one can form from a set with n elements is nCk, and the total number of subsets of a set is the sum of the numbers of possible subsets of each size, it follows that nC0+nC1+nC2+ … +nCn = 2n. The calculator has a button (nCm) that lets you compute the number of combinations of m things chosen from a set of n things. To use the button, first type the value of n, then push the nCm button, then type the value of m, then press the “=” button.</td>
      </tr>
      <tr>
          <td>38</td>
          <td>Complement.</td>
          <td>The complement of a subset of a given set is the collection of all elements of the set that are not elements of the subset.</td>
      </tr>
      <tr>
          <td>39</td>
          <td>Complement rule.</td>
          <td>The probability of the complement of an event is 100% minus the probability of the event: P(Ac) = 100% − P(A).</td>
      </tr>
      <tr>
          <td>40</td>
          <td>Compound proposition.</td>
          <td>A logical proposition formed from other propositions using logical operations such as !, /, XOR, &amp;, → and ↔.</td>
      </tr>
      <tr>
          <td>41</td>
          <td>Conditional Probability.</td>
          <td>Suppose we are interested in the probability that some event A occurs, and we learn that the event B occurred. How should we update the probability of A to reflect this new knowledge? This is what the conditional probability does: it says how the additional knowledge that B occurred should affect the probability that A occurred quantitatively. For example, suppose that A and B are mutually exclusive. Then if B occurred, A did not, so the conditional probability that A occurred given that B occurred is zero. At the other extreme, suppose that B is a subset of A, so that A must occur whenever B does. Then if we learn that B occurred, A must have occurred too, so the conditional probability that A occurred given that B occurred is 100%. For in-between cases, where A and B intersect, but B is not a subset of A, the conditional probability of A given B is a number between zero and 100%. Basically, one “restricts” the outcome space S to consider only the part of S that is in B, because we know that B occurred. For A to have happened given that B happened requires that AB happened, so we are interested in the event AB. To have a legitimate probability requires that P(S) = 100%, so if we are restricting the outcome space to B, we need to divide by the probability of B to make the probability of this new S be 100%. On this scale, the probability that AB happened is P(AB)/P(B). This is the definition of the conditional probability of A given B, provided P(B) is not zero (division by zero is undefined). Note that the special cases AB = {} (A and B are mutually exclusive) and AB = B (B is a subset of A) agree with our intuition as described at the top of this paragraph. Conditional probabilities satisfy the axioms of probability, just as ordinary probabilities do.</td>
      </tr>
      <tr>
          <td>42</td>
          <td>Confidence Interval.</td>
          <td>A confidence interval for a parameter is a random interval constructed from data in such a way that the probability that the interval contains the true value of the parameter can be specified before the data are collected. Confidence intervals are demonstrated in this page.</td>
      </tr>
      <tr>
          <td>43</td>
          <td>Confidence Level.</td>
          <td>The confidence level of a confidence interval is the chance that the interval that will result once data are collected will contain the corresponding parameter. If one computes confidence intervals again and again from independent data, the long-term limit of the fraction of intervals that contain the parameter is the confidence level.</td>
      </tr>
      <tr>
          <td>44</td>
          <td>Confounding.</td>
          <td>When the differences between the treatment and control groups other than the treatment produce differences in response that are not distinguishable from the effect of the treatment, those differences between the groups are said to be confounded with the effect of the treatment (if any). For example, prominent statisticians questioned whether differences between individuals that led some to smoke and others not to (rather than the act of smoking itself) were responsible for the observed difference in the frequencies with which smokers and non-smokers contract various illnesses. If that were the case, those factors would be confounded with the effect of smoking. Confounding is quite likely to affect observational studies and experiments that are not randomized. Confounding tends to be decreased by randomization. See also Simpson’s Paradox.</td>
      </tr>
      <tr>
          <td>45</td>
          <td>Continuity Correction.</td>
          <td>In using the normal approximation to the binomial probability histogram, one can get more accurate answers by finding the area under the normal curve corresponding to half-integers, transformed to standard units. This is clearest if we are seeking the chance of a particular number of successes. For example, suppose we seek to approximate the chance of 10 successes in 25 independent trials, each with probability p = 40% of success. The number of successes in this scenario has a binomial distribution with parameters n = 25 and p = 40%. The expected number of successes is np = 10, and the standard error is (np(1−p))½ = 6½ = 2.45. If we consider the area under the normal curve at the point 10 successes, transformed to standard units, we get zero: the area under a point is always zero. We get a better approximation by considering 10 successes to be the range from 9 1/2 to 10 1/2 successes. The only possible number of successes between 9 1/2 and 10 1/2 is 10, so this is exactly right for the binomial distribution. Because the normal curve is continuous and a binomial random variable is discrete, we need to “smear out” the binomial probability over an appropriate range. The lower endpoint of the range, 9 1/2 successes, is (9.5 − 10)/2.45 = −0.20 standard units. The upper endpoint of the range, 10 1/2 successes, is (10.5 − 10)/2.45 = +0.20 standard units. The area under the normal curve between −0.20 and +0.20 is about 15.8%. The true binomial probability is 25C10×(0.4)10×(0.6)15 = 16%. In a similar way, if we seek the normal approximation to the probability that a binomial random variable is in the range from i successes to k successes, inclusive, we should find the area under the normal curve from i−1/2 to k+1/2 successes, transformed to standard units. If we seek the probability of more than i successes and fewer than k successes, we should find the area under the normal curve corresponding to the range i+1/2 to k−1/2 successes, transformed to standard units. If we seek the probability of more than i but no more than k successes, we should find the area under the normal curve corresponding to the range i+1/2 to k+1/2 successes, transformed to standard units. If we seek the probability of at least i but fewer than k successes, we should find the area under the normal curve corresponding to the range i−1/2 to k−1/2 successes, transformed to standard units. Including or excluding the half-integer ranges at the ends of the interval in this manner is called the continuity correction.</td>
      </tr>
      <tr>
          <td>46</td>
          <td>Consequent.</td>
          <td>In a conditional p → q, the consequent is q.</td>
      </tr>
      <tr>
          <td>47</td>
          <td>Continuous Variable.</td>
          <td>A quantitative variable is continuous if its set of possible values is uncountable. Examples include temperature, exact height, exact age (including parts of a second). In practice, one can never measure a continuous variable to infinite precision, so continuous variables are sometimes approximated by discrete variables. A random variable X is also called continuous if its set of possible values is uncountable, and the chance that it takes any particular value is zero (in symbols, if P(X = x) = 0 for every real number x). A random variable is continuous if and only if its cumulative probability distribution function is a continuous function (a function with no jumps).</td>
      </tr>
      <tr>
          <td>48</td>
          <td>Contrapositive.</td>
          <td>If p and q are two logical propositions, then the contrapositive of the proposition (p → q) is the proposition ((! q) → (!p) ). The contrapositive is logically equivalent to the original proposition.</td>
      </tr>
      <tr>
          <td>49</td>
          <td>Control.</td>
          <td>There are at least three senses of “control” in statistics: a member of the control group, to whom no treatment is given; a controlled experiment, and to control for a possible confounding variable.</td>
      </tr>
      <tr>
          <td>50</td>
          <td>Controlled experiment.</td>
          <td>An experiment that uses the method of comparison to evaluate the effect of a treatment by comparing treated subjects with a control group, who do not receive the treatment.</td>
      </tr>
      <tr>
          <td>51</td>
          <td>Controlled, randomized experiment.</td>
          <td>A controlled experiment in which the assignment of subjects to the treatment group or control group is done at random, for example, by tossing a coin.</td>
      </tr>
      <tr>
          <td>52</td>
          <td>Control for a variable.</td>
          <td>To control for a variable is to try to separate its effect from the treatment effect, so it will not confound with the treatment. There are many methods that try to control for variables. Some are based on matching individuals between treatment and control; others use assumptions about the nature of the effects of the variables to try to model the effect mathematically, for example, using regression.</td>
      </tr>
      <tr>
          <td>53</td>
          <td>Control group.</td>
          <td>The subjects in a controlled experiment who do not receive the treatment.</td>
      </tr>
      <tr>
          <td>54</td>
          <td>Convenience Sample.</td>
          <td>A sample drawn because of its convenience; it is not a probability sample. For example, I might take a sample of opinions in Berkeley (where I live) by just asking my 10 nearest neighbors. That would be a sample of convenience, and would be unlikely to be representative of all of Berkeley. Samples of convenience are not typically representative, and it is not possible to quantify how unrepresentative results based on samples of convenience are likely to be. Convenience samples are to be avoided, and results based on convenience samples are to be viewed with suspicion. See also quota sample.</td>
      </tr>
      <tr>
          <td>55</td>
          <td>Converge, convergence.</td>
          <td>A sequence of numbers x1, x2, x3 … converges if there is a number x such that for any number E&gt;0, there is a number k (which can depend on E) such that /xj − x/ &lt; E whenever j &gt; k. If such a number x exists, it is called the limit of the sequence x1, x2, x3 … .</td>
      </tr>
      <tr>
          <td>56</td>
          <td>Convergence in probability.</td>
          <td>A sequence of random variables X1, X2, X3 … converges in probability if there is a random variable X such that for any number E&gt;0, the sequence of numbers P(/X1 − X/ &lt; e), P(/X2 − X/ &lt; e), P(/X3 − X/ &lt; e), … converges to 100%.</td>
      </tr>
      <tr>
          <td>57</td>
          <td>Converse.</td>
          <td>If p and q are two logical propositions, then the converse of the proposition (p → q) is the proposition (q → p).</td>
      </tr>
      <tr>
          <td>58</td>
          <td>Correlation.</td>
          <td>A measure of linear association between two (ordered) lists. Two variables can be strongly correlated without having any causal relationship, and two variables can have a causal relationship and yet be uncorrelated.</td>
      </tr>
      <tr>
          <td>59</td>
          <td>Correlation coefficient.</td>
          <td>The correlation coefficient r is a measure of how nearly a scatterplot falls on a straight line. The correlation coefficient is always between −1 and +1. To compute the correlation coefficient of a list of pairs of measurements (X,Y), first transform X and Y individually into standard units. Multiply corresponding elements of the transformed pairs to get a single list of numbers. The correlation coefficient is the mean of that list of products. This page contains a tool that lets you generate bivariate data with any correlation coefficient you want.</td>
      </tr>
      <tr>
          <td>60</td>
          <td>Counting.</td>
          <td>To count a set of things is to put it in one to one correspondence with a consecutive subset of the positive integers (counting numbers).</td>
      </tr>
      <tr>
          <td>61</td>
          <td>Counting numbers, natural numbers.</td>
          <td>The counting numbers are the strictly positive integers ({1, 2, 3, … }). (Some authorities include (0) among the counting numbers.)</td>
      </tr>
      <tr>
          <td>62</td>
          <td>Countable Set.</td>
          <td>A set is countable if its elements can be put in one-to-one correspondence with a subset of the counting numbers. For example, the sets {0, 1, 7, −3}, {red, green, blue}, {…,−2, −1, 0, 1, 2, …}, {straight, curly}, and the set of all fractions, are countable. If a set is not countable, it is uncountable. The set of all real numbers is uncountable.</td>
      </tr>
      <tr>
          <td>63</td>
          <td>Cover.</td>
          <td>A confidence interval is said to cover if the interval contains the true value of the parameter. Before the data are collected, the chance that the confidence interval will contain the parameter value is the coverage probability, which equals the confidence level after the data are collected and the confidence interval is actually computed.</td>
      </tr>
      <tr>
          <td>64</td>
          <td>Coverage probability.</td>
          <td>The coverage probability of a procedure for making confidence intervals is the chance that the procedure produces an interval that covers the truth.</td>
      </tr>
      <tr>
          <td>65</td>
          <td>Critical value</td>
          <td>The critical value in an hypothesis test is the value of the test statistic beyond which we would reject the null hypothesis. The critical value is set so that the probability that the test statistic is beyond the critical value is at most equal to the significance level if the null hypothesis be true.</td>
      </tr>
      <tr>
          <td>66</td>
          <td>Cross-sectional study.</td>
          <td>A cross-sectional study compares different individuals to each other at the same time—it looks at a cross-section of a population. The differences between those individuals can confound with the effect being explored. For example, in trying to determine the effect of age on sexual promiscuity, a cross-sectional study would be likely to confound the effect of age with the effect of the mores the subjects were taught as children: the older individuals were probably raised with a very different attitude towards promiscuity than the younger subjects. Thus it would be imprudent to attribute differences in promiscuity to the aging process. C.f. longitudinal study.</td>
      </tr>
      <tr>
          <td>67</td>
          <td>Cumulative Probability Distribution Function (cdf).</td>
          <td>The cumulative distribution function of a random variable is the chance that the random variable is less than or equal to x, as a function of x. In symbols, if F is the cdf of the random variable X, then F(x) = P( X ≤ x). The cumulative distribution function must tend to zero as x approaches minus infinity, and must tend to unity as x approaches infinity. It is a positive function, and increases monotonically: if y &gt; x, then F(y) ≥ F(x). The cumulative distribution function completely characterizes the probability distribution of a random variable.</td>
      </tr>
      <tr>
          <td>68</td>
          <td>de Morgan’s Laws</td>
          <td>de Morgan’s Laws are identities involving logical operations: the negation of a conjunction is logically equivalent to the disjunction of the negations, and the negation of a disjunction is logically equivalent to the conjunction of the negations. In symbols, !(p &amp; q) = !p / !q and !(p / q) = !p &amp; !q.</td>
      </tr>
      <tr>
          <td>69</td>
          <td>Deck of Cards.</td>
          <td>A standard deck of playing cards contains 52 cards, 13 each of four suits: spades, hearts, diamonds, and clubs. The thirteen cards of each suit are {ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, jack, queen, king}. The face cards are {jack, queen, king}. It is typically assumed that if a deck of cards is shuffled well, it is equally likely to be in each possible ordering. There are 52! (52 factorial) possible orderings.</td>
      </tr>
      <tr>
          <td>70</td>
          <td>Dependent Events, Dependent Random Variables.</td>
          <td>Two events or random variables are dependent if they are not independent.</td>
      </tr>
      <tr>
          <td>71</td>
          <td>Dependent Variable.</td>
          <td>In regression, the variable whose values are supposed to be explained by changes in the other variable (the the independent or explanatory variable). Usually one regresses the dependent variable on the independent variable.</td>
      </tr>
      <tr>
          <td>72</td>
          <td>Density, Density Scale.</td>
          <td>The vertical axis of a histogram has units of percent per unit of the horizontal axis. This is called a density scale; it measures how “dense” the observations are in each bin. See also probability density.</td>
      </tr>
      <tr>
          <td>73</td>
          <td>Denying the antecedent.</td>
          <td>A logical fallacy that argues from the premise A → B and the premise !A that therefore, !B. The name comes from the fact that the operation denies (i.e., asserts the negation of) the antecedent (A) in the conditional.</td>
      </tr>
      <tr>
          <td>74</td>
          <td>Denying the consequent.</td>
          <td>A valid logical argument that concludes from the premise A → B and the premise !B that therefore, !A. The name comes from the fact that the operation denies (i.e., asserts the logical negation) the consequent (B) in the conditional.</td>
      </tr>
      <tr>
          <td>75</td>
          <td>Deviation.</td>
          <td>A deviation is the difference between a datum and some reference value, typically the mean of the data. In computing the SD, one finds the rms of the deviations from the mean, the differences between the individual data and the mean of the data.</td>
      </tr>
      <tr>
          <td>76</td>
          <td>Discrete Variable.</td>
          <td>A quantitative variable whose set of possible values is countable. Typical examples of discrete variables are variables whose possible values are a subset of the integers, such as Social Security numbers, the number of people in a family, ages rounded to the nearest year, etc. Discrete variables are “chunky.” C.f. continuous variable. A discrete random variable is one whose set of possible values is countable. A random variable is discrete if and only if its cumulative probability distribution function is a stair-step function; i.e., if it is piecewise constant and only increases by jumps.</td>
      </tr>
      <tr>
          <td>77</td>
          <td>Disjoint or Mutually Exclusive Events.</td>
          <td>Two events are disjoint or mutually exclusive if the occurrence of one is incompatible with the occurrence of the other; that is, if they can’t both happen at once (if they have no outcome in common). Equivalently, two events are disjoint if their intersection is the empty set.</td>
      </tr>
      <tr>
          <td>78</td>
          <td>Disjoint or Mutually Exclusive Sets.</td>
          <td>Two sets are disjoint or mutually exclusive if they have no element in common. Equivalently, two sets are disjoint if their intersection is the empty set.</td>
      </tr>
      <tr>
          <td>79</td>
          <td>Distribution.</td>
          <td>The distribution of a set of numerical data is how their values are distributed over the real numbers. It is completely characterized by the empirical distribution function. Similarly, the probability distribution of a random variable is completely characterized by its probability distribution function. Sometimes the word “distribution” is used as a synonym for the empirical distribution function or the probability distribution function. If two or more random variables are defined for the same experiment, they have a joint probability distribution.</td>
      </tr>
      <tr>
          <td>80</td>
          <td>Distribution Function, Empirical.</td>
          <td>The empirical (cumulative) distribution function of a set of numerical data is, for each real value of x, the fraction of observations that are less than or equal to x. A plot of the empirical distribution function is an uneven set of stairs. The width of the stairs is the spacing between adjacent data; the height of the stairs depends on how many data have exactly the same value. The distribution function is zero for small enough (negative) values of x, and is unity for large enough values of x. It increases monotonically: if y &gt; x, the empirical distribution function evaluated at y is at least as large as the empirical distribution function evaluated at x.</td>
      </tr>
      <tr>
          <td>81</td>
          <td>Double-Blind, Double-Blind Experiment.</td>
          <td>In a double-blind experiment, neither the subjects nor the people evaluating the subjects knows who is in the treatment group and who is in the control group. This mitigates the placebo effect and guards against conscious and unconscious prejudice for or against the treatment on the part of the evaluators.</td>
      </tr>
      <tr>
          <td>82</td>
          <td>Ecological Correlation.</td>
          <td>The correlation between averages of groups of individuals, instead of individuals. Ecological correlation can be misleading about the association of individuals.</td>
      </tr>
      <tr>
          <td>83</td>
          <td>Element of a Set.</td>
          <td>See member.</td>
      </tr>
      <tr>
          <td>84</td>
          <td>Empirical Law of Averages.</td>
          <td>The Empirical Law of Averages lies at the base of the frequency theory of probability. This law, which is, in fact, an assumption about how the world works, rather than a mathematical or physical law, states that if one repeats a random experiment over and over, independently and under “identical” conditions, the fraction of trials that result in a given outcome converges to a limit as the number of trials grows without bound.</td>
      </tr>
      <tr>
          <td>85</td>
          <td>Empty Set.</td>
          <td>The empty set, denoted {} or ∅, is the set that has no members.</td>
      </tr>
      <tr>
          <td>86</td>
          <td>Endpoint Convention.</td>
          <td>In plotting a histogram, one must decide whether to include a datum that lies at a class boundary with the class interval to the left or the right of the boundary. The rule for making this assignment is called an endpoint convention. The two standard endpoint conventions are (1) to include the left endpoint of all class intervals and exclude the right, except for the rightmost class interval, which includes both of its endpoints, and (2) to include the right endpoint of all class intervals and exclude the left, except for the leftmost interval, which includes both of its endpoints.</td>
      </tr>
      <tr>
          <td>87</td>
          <td>Equally Likely Outcomes.</td>
          <td>According to the equally likely outcome Theory of Probability, if an experiment has a finite number possible outcomes and there is no reason Nature should prefer any of those outcomes over any other (e.g., because the outcome is the result of rolling a symmetric die or tossing a perfectly balanced coin or thoroughly shuffling a deck of cards), then each of those possible outcomes has the same probability. See also Laplace’s Principle of Insufficient Reason.</td>
      </tr>
      <tr>
          <td>88</td>
          <td>Estimator.</td>
          <td>An estimator is a rule for “guessing” the value of a population parameter based on a random sample from the population. An estimator is a random variable, because its value depends on which particular sample is obtained, which is random. A canonical example of an estimator is the sample mean, which is an estimator of the population mean.</td>
      </tr>
      <tr>
          <td>89</td>
          <td>Event.</td>
          <td>An event is a subset of outcome space. An event determined by a random variable is an event of the form A=(X is in A). When the random variable X is observed, that determines whether or not A occurs: if the value of X happens to be in A, A occurs; if not, A does not occur.</td>
      </tr>
      <tr>
          <td>90</td>
          <td>Exhaustive.</td>
          <td>A collection of events {A1, A2, A3, … } exhausts the set A if, for the event A to occur, at least one of those sets must also occur; that is, if S ⊂ A1 ∪ A2 ∪ A3 ∪ … If the event A is not specified, it is assumed to be the entire outcome space S.</td>
      </tr>
      <tr>
          <td>91</td>
          <td>Expectation, Expected Value.</td>
          <td>The expected value of a random variable is the long-term limiting average of its values in independent repeated experiments. The expected value of the random variable X is denoted EX or E(X). For a discrete random variable (one that has a countable number of possible values) the expected value is the weighted average of its possible values, where the weight assigned to each possible value is the chance that the random variable takes that value. One can think of the expected value of a random variable as the point at which its probability histogram would balance, if it were cut out of a uniform material. Taking the expected value is a linear operation: if X and Y are two random variables, the expected value of their sum is the sum of their expected values (E(X+Y) = E(X) + E(Y)), and the expected value of a constant a times a random variable X is the constant times the expected value of X (E(a×X ) = a× E(X)).</td>
      </tr>
      <tr>
          <td>92</td>
          <td>Experiment.</td>
          <td>What distinguishes an experiment from an observational study is that in an experiment, the experimenter chooses who receives the treatment.</td>
      </tr>
      <tr>
          <td>93</td>
          <td>Explanatory Variable.</td>
          <td>In regression, the explanatory or independent variable is the one that is supposed to “explain” the other. For example, in examining crop yield versus quantity of fertilizer applied, the quantity of fertilizer would be the explanatory or independent variable, and the crop yield would be the dependent variable. In experiments, the explanatory variable is the one that is manipulated; the one that is observed is the dependent variable.</td>
      </tr>
      <tr>
          <td>94</td>
          <td>Extrapolation.</td>
          <td>See interpolation.</td>
      </tr>
      <tr>
          <td>95</td>
          <td>Factorial.</td>
          <td>For an integer k that is greater than or equal to 1, k! (pronounced “k factorial”) is k×(k−1)×(k−2)× …×1. By convention, 0! = 1. There are k! ways of ordering k distinct objects. For example, 9! is the number of batting orders of 9 baseball players, and 52! is the number of different ways a standard deck of playing cards can be ordered. The calculator above has a button to compute the factorial of a number. To compute k!, first type the value of k, then press the button labeled “!”.</td>
      </tr>
      <tr>
          <td>96</td>
          <td>Fair Bet.</td>
          <td>A fair bet is one for which the expected value of the payoff is zero, after accounting for the cost of the bet. For example, suppose I offer to pay you $2 if a fair coin lands heads, but you must ante up $1 to play. Your expected payoff is −$1+ $0×P(tails) + $2×P(heads) = −$1 + $2×50% = $0. This is a fair bet—in the long run, if you made this bet over and over again, you would expect to break even.</td>
      </tr>
      <tr>
          <td>97</td>
          <td>False Discovery Rate.</td>
          <td>In testing a collection of hypotheses, the false discovery rate is the fraction of rejected null hypotheses that are rejected erroneously (the number of Type I errors divided by the number of rejected null hypotheses), with the convention that if no hypothesis is rejected, the false discovery rate is zero.</td>
      </tr>
      <tr>
          <td>98</td>
          <td>Finite, finite set.</td>
          <td>A set is finite if it has a finite number of elements, that is, if for some natural number n, the elements can be put in one-to-one correspondence with the set {1, 2, … n}.</td>
      </tr>
      <tr>
          <td>99</td>
          <td>Finite Population Correction.</td>
          <td>When sampling without replacement, as in a simple random sample, the SE of sample sums and sample means depends on the fraction of the population that is in the sample: the greater the fraction, the smaller the SE. Sampling with replacement is like sampling from an infinitely large population. The adjustment to the SE for sampling without replacement is called the finite population correction. The SE for sampling without replacement is smaller than the SE for sampling with replacement by the finite population correction factor ((N −n)/(N − 1))½. Note that for sample size n=1, there is no difference between sampling with and without replacement; the finite population correction is then unity. If the sample size is the entire population of N units, there is no variability in the result of sampling without replacement (every member of the population is in the sample exactly once), and the SE should be zero. This is indeed what the finite population correction gives (the numerator vanishes).</td>
      </tr>
      <tr>
          <td>100</td>
          <td>Fisher’s exact test (for the equality of two percentages)</td>
          <td>Consider two populations of zeros and ones. Let p1 be the proportion of ones in the first population, and let p2 be the proportion of ones in the second population. We would like to test the null hypothesis that p1 = p2 on the basis of a simple random sample from each population. Let n1 be the size of the sample from population 1, and let n2 be the size of the sample from population 2. Let G be the total number of ones in both samples. If the null hypothesis be true, the two samples are like one larger sample from a single population of zeros and ones. The allocation of ones between the two samples would be expected to be proportional to the relative sizes of the samples, but would have some chance variability. Conditional on G and the two sample sizes, under the null hypothesis, the tickets in the first sample are like a random sample of size n1 without replacement from a collection of N = n1 + n2 units of which G are labeled with ones. Thus, under the null hypothesis, the number of tickets labeled with ones in the first sample has (conditional on G) an hypergeometric distribution with parameters N, G, and n1. Fisher’s exact test uses this distribution to set the ranges of observed values of the number of ones in the first sample for which we would reject the null hypothesis.</td>
      </tr>
      <tr>
          <td>101</td>
          <td>Football-Shaped Scatterplot.</td>
          <td>In a football-shaped scatterplot, most of the points lie within a tilted oval, shaped more-or-less like a football. A football-shaped scatterplot is one in which the data are homoscedastically scattered about a straight line.</td>
      </tr>
      <tr>
          <td>102</td>
          <td>Frame, sampling frame.</td>
          <td>A sampling frame is a collection of units from which a sample will be drawn. Ideally, the frame is identical to the population we want to learn about; more typically, the frame is only a subset of the population of interest. The difference between the frame and the population can be a source of bias in sampling design, if the parameter of interest has a different value for the frame than it does for the population. For example, one might desire to estimate the current annual average income of 1998 graduates of the University of California at Berkeley. I propose to use the sample mean income of a sample of graduates drawn at random. To facilitate taking the sample and contacting the graduates to obtain income information from them, I might draw names at random from the list of 1998 graduates for whom the alumni association has an accurate current address. The population is the collection of 1998 graduates; the frame is those graduates who have current addresses on file with the alumni association. If there is a tendency for graduates with higher incomes to have up-to-date addresses on file with the alumni association, that would introduce a positive bias into the annual average income estimated from the sample by the sample mean.</td>
      </tr>
      <tr>
          <td>103</td>
          <td>FPP.</td>
          <td>Statistics, third edition, by Freedman, Pisani, and Purves, published by W.W. Norton, 1997.</td>
      </tr>
      <tr>
          <td>104</td>
          <td>Frequency theory of probability.</td>
          <td>See Probability, Theories of.</td>
      </tr>
      <tr>
          <td>105</td>
          <td>Frequency table.</td>
          <td>A table listing the frequency (number) or relative frequency (fraction or percentage) of observations in different ranges, called class intervals.</td>
      </tr>
      <tr>
          <td>106</td>
          <td>Fundamental Rule of Counting.</td>
          <td>If a sequence of experiments or trials T1, T2, T3, …, Tk could result, respectively, in n1, n2 n3, …, nk possible outcomes, and the numbers n1, n2 n3, …, nk do not depend on which outcomes actually occurred, the entire sequence of k experiments has n1× n2 × n3× …× nk possible outcomes.</td>
      </tr>
      <tr>
          <td>107</td>
          <td>Game Theory.</td>
          <td>A field of study that bridges mathematics, statistics, economics, and psychology. It is used to study economic behavior, and to model conflict between nations, for example, “nuclear stalemate” during the Cold War.</td>
      </tr>
      <tr>
          <td>108</td>
          <td>Geometric Distribution.</td>
          <td>The geometric distribution describes the number of trials up to and including the first success, in independent trials with the same probability of success. The geometric distribution depends only on the single parameter p, the probability of success in each trial. For example, the number of times one must toss a fair coin until the first time the coin lands heads has a geometric distribution with parameter p = 50%. The geometric distribution assigns probability p×(1 − p)k−1to the event that it takes k trials to the first success. The expected value of the geometric distribution is 1/p, and its SE is (1−p)½/p.</td>
      </tr>
      <tr>
          <td>109</td>
          <td>Geometric Mean.</td>
          <td>The geometric mean of n numbers {x1, x2, x3, …, xn} is the nth root of their product: (x1×x2×x3× … ×xn)1/n.</td>
      </tr>
      <tr>
          <td>110</td>
          <td>Graph of Averages.</td>
          <td>For bivariate data, a graph of averages is a plot of the average values of one variable (say y) for small ranges of values of the other variable (say x), against the value of the second variable (x) at the midpoints of the ranges.</td>
      </tr>
      <tr>
          <td>111</td>
          <td>Heteroscedasticity.</td>
          <td>“Mixed scatter.” A scatterplot or residual plot shows heteroscedasticity if the scatter in vertical slices through the plot depends on where you take the slice. Linear regression is not usually a good idea if the data are heteroscedastic.</td>
      </tr>
      <tr>
          <td>112</td>
          <td>Histogram.</td>
          <td>A histogram is a kind of plot that summarizes how data are distributed. Starting with a set of class intervals, the histogram is a set of rectangles (“bins”) sitting on the horizontal axis. The bases of the rectangles are the class intervals, and their heights are such that their areas are proportional to the fraction of observations in the corresponding class intervals. That is, the height of a given rectangle is the fraction of observations in the corresponding class interval, divided by the length of the corresponding class interval. A histogram does not need a vertical scale, because the total area of the histogram must equal 100%. The units of the vertical axis are percent per unit of the horizontal axis. This is called the density scale. The horizontal axis of a histogram needs a scale. If any observations coincide with the endpoints of class intervals, the endpoint convention is important. This page contains a histogram tool, with controls to highlight ranges of values and read their areas.</td>
      </tr>
      <tr>
          <td>113</td>
          <td>Historical Controls.</td>
          <td>Sometimes, the a treatment group is compared with individuals from another epoch who did not receive the treatment; for example, in studying the possible effect of fluoridated water on childhood cancer, we might compare cancer rates in a community before and after fluorine was added to the water supply. Those individuals who were children before fluoridation started would comprise an historical control group. Experiments and studies with historical controls tend to be more susceptible to confounding than those with contemporary controls, because many factors that might affect the outcome other than the treatment tend to change over time as well. (In this example, the level of other potential carcinogens in the environment also could have changed.)</td>
      </tr>
      <tr>
          <td>114</td>
          <td>Homoscedasticity.</td>
          <td>“Same scatter.” A scatterplot or residual plot shows homoscedasticity if the scatter in vertical slices through the plot does not depend much on where you take the slice. C.f. heteroscedasticity.</td>
      </tr>
      <tr>
          <td>115</td>
          <td>House Edge.</td>
          <td>In casino games, the expected payoff to the bettor is negative: the house (casino) tends to win money in the long run. The amount of money the house would expect to win for each $1 wagered on a particular bet (such as a bet on “red” in roulette) is called the house edge for that bet.</td>
      </tr>
      <tr>
          <td>116</td>
          <td>HTLWS.</td>
          <td>The book How to lie with Statistics by D. Huff.</td>
      </tr>
      <tr>
          <td>117</td>
          <td>Hypergeometric Distribution.</td>
          <td>The hypergeometric distribution with parameters N, G and n is the distribution of the number of “good” objects in a simple random sample of size n (i.e., a random sample without replacement in which every subset of size n has the same chance of occurring) from a population of N objects of which G are “good.” The chance of getting exactly g good objects in such a sample is GCg × N−GCn−g/NCn, provided g ≤ n, g ≤ G, and n − g ≤ N − G. (The probability is zero otherwise.) The expected value of the hypergeometric distribution is n×G/N, and its standard error is ((N−n)/(N−1))½ × (n × G/N × (1−G/N) )½.</td>
      </tr>
      <tr>
          <td>118</td>
          <td>Hypothesis testing.</td>
          <td>Statistical hypothesis testing is formalized as making a decision between rejecting or not rejecting a null hypothesis, on the basis of a set of observations. Two types of errors can result from any decision rule (test): rejecting the null hypothesis when it is true (a Type I error), and failing to reject the null hypothesis when it is false (a Type II error). For any hypothesis, it is possible to develop many different decision rules (tests). Typically, one specifies ahead of time the chance of a Type I error one is willing to allow. That chance is called the significance level of the test or decision rule. For a given significance level, one way of deciding which decision rule is best is to pick the one that has the smallest chance of a Type II error when a given alternative hypothesis is true. The chance of correctly rejecting the null hypothesis when a given alternative hypothesis is true is called the power of the test against that alternative.</td>
      </tr>
      <tr>
          <td>119</td>
          <td>iff, if and only if, ↔</td>
          <td>If p and q are two logical propositions, then(p ↔ q) is a proposition that is true when both p and q are true, and when both p and q are false. It is logically equivalent to the proposition ( (p → q) &amp; (q → p) ) and to the proposition ( (p &amp; q)</td>
      </tr>
      <tr>
          <td>120</td>
          <td>Implies, logical implication, → , conditional, if-then</td>
          <td>Logical implication is an operation on two logical propositions. If p and q are two logical propositions, (p → q), pronounced “p implies q” or “if p then q” is a logical proposition that is true if p is false, or if both p and q are true. The proposition (p → q) is logically equivalent to the proposition ((!p) / q). In the conditional p → q, the antecedent is p and the consequent is q.</td>
      </tr>
      <tr>
          <td>121</td>
          <td>Independent, independence.</td>
          <td>Two events A and B are (statistically) independent if the chance that they both happen simultaneously is the product of the chances that each occurs individually; i.e., if P(AB) = P(A)P(B). This is essentially equivalent to saying that learning that one event occurs does not give any information about whether the other event occurred too: the conditional probability of A given B is the same as the unconditional probability of A, i.e., P(A/B) = P(A). Two random variables X and Y are independent if all events they determine are independent, for example, if the event {a &lt; X ≤ b} is independent of the event {c &lt; Y ≤ d} for all choices of a, b, c, and d. A collection of more than two random variables is independent if for every proper subset of the variables, every event determined by that subset of the variables is independent of every event determined by the variables in the complement of the subset. For example, the three random variables X, Y, and Z are independent if every event determined by X is independent of every event determined by Y and every event determined by X is independent of every event determined by Y and Z and every event determined by Y is independent of every event determined by X and Z and every event determined by Z is independent of every event determined by X and Y.</td>
      </tr>
      <tr>
          <td>122</td>
          <td>Independent and identically distributed (iid).</td>
          <td>A collection of two or more random variables {X1, X2, … , } is independent and identically distributed if the variables have the same probability distribution, and are independent.</td>
      </tr>
      <tr>
          <td>123</td>
          <td>Independent Variable.</td>
          <td>In regression, the independent variable is the one that is supposed to explain the other; the term is a synonym for “explanatory variable.” Usually, one regresses the “dependent variable” on the “independent variable.” There is not always a clear choice of the independent variable. The independent variable is usually plotted on the horizontal axis. Independent in this context does not mean the same thing as statistically independent.</td>
      </tr>
      <tr>
          <td>124</td>
          <td>Indicator Random Variable.</td>
          <td>The indicator [random variable] of the event A, often written 1A, is the random variable that equals unity if A occurs, and zero if A does not occur. The expected value of the indicator of A is the probability of A, P(A), and the standard error of the indicator of A is (P(A)×(1−P(A))½. The sum 1A + 1B + 1C + … of the indicators of a collection of events {A, B, C, …} counts how many of the events {A, B, C, …} occur in a given trial. The product of the indicators of a collection of events is the indicator of the intersection of the events (the product equals one if and only if all of indicators equal one). The maximum of the indicators of a collection of events is the indicator of the union of the events (the maximum equals one if any of the indicators equals one).</td>
      </tr>
      <tr>
          <td>125</td>
          <td>Inter-quartile Range (IQR).</td>
          <td>The inter-quartile range of a list of numbers is the upper quartile minus the lower quartile.</td>
      </tr>
      <tr>
          <td>126</td>
          <td>Interpolation.</td>
          <td>Given a set of bivariate data (x, y), to impute a value of y corresponding to some value of x at which there is no measurement of y is called interpolation, if the value of x is within the range of the measured values of x. If the value of x is outside the range of measured values, imputing a corresponding value of y is called extrapolation.</td>
      </tr>
      <tr>
          <td>127</td>
          <td>Intersection.</td>
          <td>The intersection of two or more sets is the set of elements that all the sets have in common; the elements contained in every one of the sets. The intersection of the events A and B is written “A∩B,” “A and B,” and “AB.” C.f. union. See also Venn diagrams.</td>
      </tr>
      <tr>
          <td>128</td>
          <td>Invalid (logical) argument.</td>
          <td>An invalid logical argument is one in which the truth of the premises does not guarantee the truth of the conclusion. For example, the following logical argument is invaldraft: false</td>
      </tr>
      <tr>
          <td>id: If the forecast calls for rain, I will not wear sandals. The forecast does not call for rain. Therefore, I will wear sandals. See also valid argument.</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>129</td>
          <td>Joint Probability Distribution.</td>
          <td>If X1, X2, … , Xk are random variables defined for the same experiment, their joint probability distribution gives the probability of events determined by the collection of random variables: for any collection of sets of numbers {A1, … , Ak}, the joint probability distribution determines P( (X1 is in A1) and (X2 is in A2) and … and (Xk is in Ak) ). For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. If a collection of random variables is independent, their joint probability distribution is the product of their marginal probability distributions, their individual probability distributions without regard for the value of the other variables. In this example, the marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6, and the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Note that P(X1 = 1, X2 = 10) = 0, while P(X1 = 1)×P(X2 = 10) = (1/6)(3/36) = 1/72. The joint probability is not equal to the product of the marginal probabilities: X1 and X2 are dependent random variables.</td>
      </tr>
      <tr>
          <td>130</td>
          <td>Law of Averages.</td>
          <td>The Law of Averages says that the average of independent observations of random variables that have the same probability distribution is increasingly likely to be close to the expected value of the random variables as the number of observations grows. More precisely, if X1, X2, X3, …, are independent random variables with the same probability distribution, and E(X) is their common expected value, then for every number ε &gt; 0, P{/(X1 + X2 + … + Xn)/n − E(X) / &lt; ε} converges to 100% as n grows. This is equivalent to saying that the sequence of sample means X1, (X1+X2)/2, (X1+X2+X3)/3, … converges in probability to E(X).</td>
      </tr>
      <tr>
          <td>131</td>
          <td>Law of Large Numbers.</td>
          <td>The Law of Large Numbers says that in repeated, independent trials with the same probability p of success in each trial, the percentage of successes is increasingly likely to be close to the chance of success as the number of trials increases. More precisely, the chance that the percentage of successes differs from the probability p by more than a fixed positive amount, e &gt; 0, converges to zero as the number of trials n goes to infinity, for every number e &gt; 0. Note that in contrast to the difference between the percentage of successes and the probability of success, the difference between the number of successes and the expected number of successes, n×p, tends to grow as n grows. The following tool illustrates the law of large numbers; the button toggles between displaying the difference between the number of successes and the expected number of successes, and the difference between the percentage of successes and the expected percentage of successes. The tool on this page illustrates the law of large numbers.</td>
      </tr>
      <tr>
          <td>132</td>
          <td>Limit.</td>
          <td>See converge.</td>
      </tr>
      <tr>
          <td>133</td>
          <td>Linear Operation.</td>
          <td>Suppose f is a function or operation that acts on things we shall denote generically by the lower-case Roman letters x and y. Suppose it makes sense to multiply x and y by numbers (which we denote by a), and that it makes sense to add things like x and y together. We say that f is linear if for every number a and every value of x and y for which f(x) and f(y) are defined, (i) f( a×x ) is defined and equals a×f(x), and (ii) f( x + y ) is defined and equals f(x) + f(y). C.f. affine.</td>
      </tr>
      <tr>
          <td>134</td>
          <td>Linear association.</td>
          <td>Two variables are linearly associated if a change in one is associated with a proportional change in the other, with the same constant of proportionality throughout the range of measurement. The correlation coefficient measures the degree of linear association on a scale of −1 to 1.</td>
      </tr>
      <tr>
          <td>135</td>
          <td>List.</td>
          <td>I use the term list to mean two things: either a multiset or (more often) an tuple. Lists are countable collections (multisets) in some order (like a tuple). That is, it makes sense to talk about the 1st (or 7th, or nth) element of a list, and the nth and mth elements of a list can be equal, even if n ≠ m (the elements of a list need not be distinct).</td>
      </tr>
      <tr>
          <td>136</td>
          <td>Location, Measure of.</td>
          <td>A measure of location is a way of summarizing what a “typical” element of a list is—it is a one-number summary of a distribution. See also arithmetic mean, median, and mode.</td>
      </tr>
      <tr>
          <td>137</td>
          <td>Logical argument.</td>
          <td>A logical argument consists of one or more premises, propositions that are assumed to be true, and a conclusion, a proposition that is supposed to be guaranteed to be true (as a matter of pure logic) if the premises are true. For example, the following is a logical argument: p → q Therefore, q. This argument has two premises: p → q, and p. The conclusion of the argument is q. If a logical argument is valid if the truth of the premises guarantees the truth of the conclusion; otherwise, the argument is invalid. That is, an argument with premises p1, p1, … pn and conclusion q is valid if the compound proposition (p1 &amp; p2 &amp; … &amp; pn) → q is logically equivalent to TRUE. The argument given above is valid because if it is true that p → q and that p is true (the two premises), then q (the conclusion of the argument) must also be true.</td>
      </tr>
      <tr>
          <td>138</td>
          <td>Logically equivalent, logical equivalence.</td>
          <td>Two propositions are logically equivalent if they always have the same truth value. That is, the propositions p and q are logically equivalent if p is true whenever q is true and p is false whenever q is false. The proposition (p ↔ q) is always true if and only if p and q are logically equivalent. For example, p is logically equivalent to p, to (p &amp; p), and to (p / p); (p / (!p)) is logically equivalent to TRUE; (p &amp; !p) is logically equivalent to FALSE; (p ↔ p) is logically equivalent to TRUE; and (p → q) is logically equivalent to (!p / q).</td>
      </tr>
      <tr>
          <td>139</td>
          <td>Longitudinal study.</td>
          <td>A study in which individuals are followed over time, and compared with themselves at different times, to determine, for example, the effect of aging on some measured variable. Longitudinal studies provide much more persuasive evidence about the effect of aging than do cross-sectional studies.</td>
      </tr>
      <tr>
          <td>140</td>
          <td>Lower Quartile (LQ).</td>
          <td>See quartiles.</td>
      </tr>
      <tr>
          <td>141</td>
          <td>Margin of error.</td>
          <td>A measure of the uncertainty in an estimate of a parameter; unfortunately, not everyone agrees what it should mean. The margin of error of an estimate is typically one or two times the estimated standard error of the estimate.</td>
      </tr>
      <tr>
          <td>142</td>
          <td>Marginal probability distribution.</td>
          <td>The marginal probability distribution of a random variable that has a joint probability distribution with some other random variables is the probability distribution of that random variable without regard for the values that the other random variables take. The marginal distribution of a discrete random variable X1 that has a joint distribution with other discrete random variables can be found from the joint distribution by summing over all possible values of the other variables. For example, suppose we roll two fair dice independently. Let X1 be the number of spots that show on the first die, and let X2 be the total number of spots that show on both dice. Then the joint distribution of X1 and X2 is as follows: P(X1 = 1, X2 = 2) = P(X1 = 1, X2 = 3) = P(X1 = 1, X2 = 4) = P(X1 = 1, X2 = 5) = P(X1 = 1, X2 = 6) = P(X1 = 1, X2 = 7) = P(X1 = 2, X2 = 3) = P(X1 = 2, X2 = 4) = P(X1 = 2, X2 = 5) = P(X1 = 2, X2 = 6) = P(X1 = 2, X2 = 7) = P(X1 = 2, X2 = 8) = … … P(X1 = 6, X2 = 7) = P(X1 = 6, X2 = 8) = P(X1 = 6, X2 = 9) = P(X1 = 6, X2 = 10) = P(X1 = 6, X2 = 11) = P(X1 = 6, X2 = 12) = 1/36. The marginal probability distribution of X1 is P(X1 = 1) = P(X1 = 2) = P(X1 = 3) = P(X1 = 4) = P(X1 = 5) = P(X1 = 6) = 1/6. We can verify that the marginal probability that X1 = 1 is indeed the sum of the joint probability distribution over all possible values of X2 for which X1 = 1: P(X1 = 1) = P(X1 = 1, X2 = 2) + P(X1 = 1, X2 = 3) + P(X1 = 1, X2 = 4) + P(X1 = 1, X2 = 5) + P(X1 = 1, X2 = 6) + P(X1 = 1, X2 = 7) = 6/36 = 1/6. Similarly, the marginal probability distribution of X2 is P(X2 = 2) = P(X2 = 12) = 1/36 P(X2 = 3) = P(X2 = 11) = 1/18 P(X2 = 4) = P(X2 = 10) = 3/36 P(X2 = 5) = P(X2 = 9) = 1/9 P(X2 = 6) = P(X2 = 8) = 5/36 P(X2 = 7) = 1/6. Again, we can verify that the marginal probability that X2 = 4 is 3/36 by adding the joint probabilities for all possible values of X1 for which X2 = 4: P(X2 = 4) = P(X1 = 1, X2 = 4) + P(X1 = 2, X2 = 4) + P(X1 = 3, X2 = 4) = 3/36.</td>
      </tr>
      <tr>
          <td>143</td>
          <td>Markov’s Inequality.</td>
          <td>For lists: If a list contains no negative numbers, the fraction of numbers in the list at least as large as any given constant a&gt;0 is no larger than the arithmetic mean of the list, divided by a. For random variables: if a random variable X must be nonnegative, the chance that X exceeds any given constant a&gt;0 is no larger than the expected value of X, divided by a.</td>
      </tr>
      <tr>
          <td>144</td>
          <td>Maximum Likelihood Estimate (MLE).</td>
          <td>The maximum likelihood estimate of a parameter from data is the possible value of the parameter for which the chance of observing the data largest. That is, suppose that the parameter is p, and that we observe data x. Then the maximum likelihood estimate of p is estimate p by the value q that makes P(observing x when the value of p is q) as large as possible. For example, suppose we are trying to estimate the chance that a (possibly biased) coin lands heads when it is tossed. Our data will be the number of times x the coin lands heads in n independent tosses of the coin. The distribution of the number of times the coin lands heads is binomial with parameters n (known) and p (unknown). The chance of observing x heads in n trials if the chance of heads in a given trial is q is nCx qx(1−q)n−x. The maximum likelihood estimate of p would be the value of q that makes that chance largest. We can find that value of q explicitly using calculus; it turns out to be q = x/n, the fraction of times the coin is observed to land heads in the n tosses. Thus the maximum likelihood estimate of the chance of heads from the number of heads in n independent tosses of the coin is the observed fraction of tosses in which the coin lands heads.</td>
      </tr>
      <tr>
          <td>145</td>
          <td>Mean, Arithmetic mean.</td>
          <td>The sum of a list of numbers, divided by the number of elements in the list. See also average.</td>
      </tr>
      <tr>
          <td>146</td>
          <td>Mean Squared Error (MSE).</td>
          <td>The mean squared error of an estimator of a parameter is the expected value of the square of the difference between the estimator and the parameter. In symbols, if X is an estimator of the parameter t, then MSE(X) = E( (X−t)2 ). The MSE measures how far the estimator is off from what it is trying to estimate, on the average in repeated experiments. It is a summary measure of the accuracy of the estimator. It combines any tendency of the estimator to overshoot or undershoot the truth (bias), and the variability of the estimator (SE). The MSE can be written in terms of the bias and SE of the estimator: MSE(X) = (bias(X))2 + (SE(X))2.</td>
      </tr>
      <tr>
          <td>147</td>
          <td>Median.</td>
          <td>“Middle value” of a list. The smallest number such that at least half the numbers in the list are no greater than it. If the list has an odd number of entries, the median is the middle entry in the list after sorting the list into increasing order. If the list has an even number of entries, the median is the smaller of the two middle numbers after sorting. The median can be estimated from a histogram by finding the smallest number such that the area under the histogram to the left of that number is 50%.</td>
      </tr>
      <tr>
          <td>148</td>
          <td>Member of a set.</td>
          <td>Something is a member (or element) of a set if it is one of the things in the set.</td>
      </tr>
      <tr>
          <td>149</td>
          <td>Method of Comparison.</td>
          <td>The most basic and important method of determining whether a treatment has an effect: compare what happens to individuals who are treated (the treatment group) with what happens to individuals who are not treated (the control group).</td>
      </tr>
      <tr>
          <td>150</td>
          <td>Minimax Strategy.</td>
          <td>In game theory, a minimax strategy is one that minimizes one’s maximum loss, whatever the opponent might do (whatever strategy the opponent might choose).</td>
      </tr>
      <tr>
          <td>151</td>
          <td>Mode.</td>
          <td>For lists, the mode is a most common (frequent) value. A list can have more than one mode. For histograms, a mode is a relative maximum (“bump”).</td>
      </tr>
      <tr>
          <td>152</td>
          <td>Moment.</td>
          <td>The kth moment of a list is the average value of the elements raised to the kth power; that is, if the list consists of the N elements x1, x2, … , xN, the kth moment of the list is ( x1k + x2k + xNk )/N. The kth moment of a random variable X is the expected value of Xk, E(Xk).</td>
      </tr>
      <tr>
          <td>153</td>
          <td>Monotone, monotonic function.</td>
          <td>A function is monotone if it only increases or only decreases: f increases monotonically (is monotonic increasing) if x &gt; y, implies thatf(x) ≥ f(y). A function f decreases monotonically (is monotonic decreasing) if x &gt; y, implies thatf(x) ≤ f(y). A function f is strictly monotonically increasing if x &gt; y, implies thatf(x) &gt; f(y), and strictly monotonically decreasing if if x &gt; y, implies thatf(x) &lt; f(y).</td>
      </tr>
      <tr>
          <td>154</td>
          <td>Multimodal Distribution.</td>
          <td>A distribution with more than one mode. The histogram of a multimodal distribution has more than one “bump.”</td>
      </tr>
      <tr>
          <td>155</td>
          <td>Multinomial Distribution</td>
          <td>Consider a sequence of n independent trials, each of which can result in an outcome in any of k categories. Let pj be the probability that each trial results in an outcome in category j, j = 1, 2, … , k, so p1 + p2 + … + pk = 100%. The number of outcomes of each type has a multinomial distribution. In particular, the probability that the n trials result in n1 outcomes of type 1, n2 outcomes of type 2, … , and nk outcomes of type k is n!/(n1! × n2! × … × nk!) × p1n1 × p2n2 × … × pknk, if n1, … , nk are nonnegative integers that sum to n; the chance is zero otherwise.</td>
      </tr>
      <tr>
          <td>156</td>
          <td>Multiplication rule.</td>
          <td>The chance that events A and B both occur (i.e., that event AB occurs), is the conditional probability that A occurs given that B occurs, times the unconditional probability that B occurs.</td>
      </tr>
      <tr>
          <td>157</td>
          <td>Multiplicity in hypothesis tests.</td>
          <td>In hypothesis testing, if more than one hypothesis is tested, the actual significance level of the combined tests is not equal to the nominal significance level of the individual tests. See also false discovery rate.</td>
      </tr>
      <tr>
          <td>158</td>
          <td>Multivariate Data.</td>
          <td>A set of measurements of two or more variables per individual. See bivariate.</td>
      </tr>
      <tr>
          <td>159</td>
          <td>Multiset.</td>
          <td>A multiset, also known as a bag is a collection of things, but—unlike a set, which is also a collection of things—the same object can occur in a multiset more than once. For instance, the sets {1, 2}, {1, 2, 2}, and {1, 1, 1, 1, 1, 2, 2} are all equal, while the multisets [1, 2], [1, 2, 2], and [1, 1, 1, 1, 1, 2, 2] are all different. However, order does not matter for sets or for multisets, so, for instance {1, 2} = {2, 1} and [1, 1, 1, 1, 1, 2, 2] = [2, 1, 1, 2, 1, 1, 1].</td>
      </tr>
      <tr>
          <td>160</td>
          <td>Mutually Exclusive.</td>
          <td>See disjoint events or disjoint sets.</td>
      </tr>
      <tr>
          <td>161</td>
          <td>Nearly normal distribution.</td>
          <td>A population of numbers (a list of numbers) is said to have a nearly normal distribution if the histogram of its values in standard units nearly follows a normal curve. More precisely, suppose that the mean of the list is μ and the standard deviation of the list is SD. Then the list is nearly normally distributed if, for every two numbers a &lt; b, the fraction of numbers in the list that are between a and b is approximately equal to the area under the normal curve between (a − μ)/SD and (a − μ)/SD.</td>
      </tr>
      <tr>
          <td>162</td>
          <td>Negative Binomial Distribution.</td>
          <td>Consider a sequence of independent trials with the same probability p of success in each trial. The number of trials up to and including the rth success has the negative Binomial distribution with parameters n and r. If the random variable N has the negative binomial distribution with parameters n and r, then P(N=k) = k−1Cr−1 × pr × (1−p)k−r, for k = r, r+1, r+2, …, and zero for k &lt; r, because there must be at least r trials to have r successes. The negative binomial distribution is derived as follows: for the rth success to occur on the kth trial, there must have been r−1 successes in the first k−1 trials, and the kth trial must result in success. The chance of the former is the chance of r−1 successes in k−1 independent trials with the same probability of success in each trial, which, according to the Binomial distribution with parameters n=k−1 and p, has probability k−1Cr−1 × pr−1 × (1−p)k−r. The chance of the latter event is p, by assumption. Because the trials are independent, we can find the chance that both events occur by multiplying their chances together, which gives the expression for P(N=k) above.</td>
      </tr>
      <tr>
          <td>163</td>
          <td>No causation without manipulation.</td>
          <td>A slogan attributed to Paul Holland. If the conditions were not deliberately manipulated (for example, if the situation is an observational study rather than an experiment), it is unwise to conclude that there is any causal relationship between the outcome and the conditions. See post hoc ergo propter hoc and cum hoc ergo propter hoc.</td>
      </tr>
      <tr>
          <td>164</td>
          <td>Nonlinear Association.</td>
          <td>The relationship between two variables is nonlinear if a change in one is associated with a change in the other that is depends on the value of the first; that is, if the change in the second is not simply proportional to the change in the first, independent of the value of the first variable.</td>
      </tr>
      <tr>
          <td>165</td>
          <td>Nonresponse.</td>
          <td>In surveys, it is rare that everyone who is “invited” to participate (everyone whose phone number is called, everyone who is mailed a questionnaire, everyone an interviewer tries to stop on the street…) in fact responds. The difference between the “invited” sample sought, and that obtained, is the nonresponse.</td>
      </tr>
      <tr>
          <td>166</td>
          <td>Nonresponse bias.</td>
          <td>In a survey, those who respond may differ from those who do not, in ways that are related to the effect one is trying to measure. For example, a telephone survey of how many hours people work is likely to miss people who are working late, and are therefore not at home to answer the phone. When that happens, the survey may suffer from nonresponse bias. Nonresponse bias makes the result of a survey differ systematically from the truth.</td>
      </tr>
      <tr>
          <td>167</td>
          <td>Nonresponse rate.</td>
          <td>The fraction of nonresponders in a survey: the number of nonresponders divided by the number of people invited to participate (the number sent questionnaires, the number of interview attempts, etc.) If the nonresponse rate is appreciable, the survey suffer from large nonresponse bias.</td>
      </tr>
      <tr>
          <td>168</td>
          <td>Normal approximation.</td>
          <td>The normal approximation to data is to approximate areas under the histogram of data, transformed into standard units, by the corresponding areas under the normal curve. Many probability distributions can be approximated by a normal distribution, in the sense that the area under the probability histogram is close to the area under a corresponding part of the normal curve. To find the corresponding part of the normal curve, the range must be converted to standard units, by subtracting the expected value and dividing by the standard error. For example, the area under the binomial probability histogram for n = 50 and p = 30% between 9.5 and 17.5 is 74.2%. To use the normal approximation, we transform the endpoints to standard units, by subtracting the expected value (for the Binomial random variable, n×p = 15 for these values of n and p) and dividing the result by the standard error (for a Binomial, (n × p × (1−p))1/2 = 3.24 for these values of n and p). The area normal approximation is the area under the normal curve between (9.5 − 15)/3.24 = −1.697 and (17.5 − 15)/3.24 = 0.772; that area is 73.5%, slightly smaller than the corresponding area under the binomial histogram. See also the continuity correction. The tool on this page illustrates the normal approximation to the binomial probability histogram. Note that the approximation gets worse when p gets close to 0 or 1, and that the approximation improves as n increases.</td>
      </tr>
      <tr>
          <td>169</td>
          <td>Normal curve.</td>
          <td>The normal curve is the familiar “bell curve:,” illustrated on this page. The mathematical expression for the normal curve is y = (2×pi)−½e−x2/2, where pi is the ratio of the circumference of a circle to its diameter (3.14159265…), and e is the base of the natural logarithm (2.71828…). The normal curve is symmetric around the point x=0, and positive for every value of x. The area under the normal curve is unity, and the SD of the normal curve, suitably defined, is also unity. Many (but not most) histograms, converted into standard units, approximately follow the normal curve.</td>
      </tr>
      <tr>
          <td>170</td>
          <td>Normal distribution.</td>
          <td>A random variable X has a normal distribution with mean m and standard error s if for every pair of numbers a ≤ b, the chance that a &lt; (X−m)/s &lt; b is P(a &lt; (X−m)/s &lt; b) = area under the normal curve between a and b. If there are numbers m and s such that X has a normal distribution with mean m and standard error s, then X is said to have a normal distribution or to be normally distributed. If X has a normal distribution with mean m=0 and standard error s=1, then X is said to have a standard normal distribution. The notation X<del>N(m,s2) means that X has a normal distribution with mean m and standard error s; for example, X</del>N(0,1), means X has a standard normal distribution.</td>
      </tr>
      <tr>
          <td>171</td>
          <td>NOT, !, Negation, Logical Negation.</td>
          <td>The negation of a logical proposition p, !p, is a proposition that is the logical opposite of p. That is, if p is true, !p is false, and if p is false, !p is true. Negation takes precedence over other logical operations. Other common symbols for the negation operator include ¬, − and ˜.</td>
      </tr>
      <tr>
          <td>172</td>
          <td>Null hypothesis.</td>
          <td>In hypothesis testing, the hypothesis we wish to falsify on the basis of the data. The null hypothesis is typically that something is not present, that there is no effect, or that there is no difference between treatment and control.</td>
      </tr>
      <tr>
          <td>173</td>
          <td>Observational Study.</td>
          <td>Controlled experiment.</td>
      </tr>
      <tr>
          <td>174</td>
          <td>Odds.</td>
          <td>The odds in favor of an event is the ratio of the probability that the event occurs to the probability that the event does not occur. For example, suppose an experiment can result in any of n possible outcomes, all equally likely, and that k of the outcomes result in a “win” and n−k result in a “loss.” Then the chance of winning is k/n; the chance of not winning is (n−k)/n; and the odds in favor of winning are (k/n)/((n−k)/n) = k/(n−k), which is the number of favorable outcomes divided by the number of unfavorable outcomes. Note that odds are not synonymous with probability, but the two can be converted back and forth. If the odds in favor of an event are q, then the probability of the event is q/(1+q). If the probability of an event is p, the odds in favor of the event are p/(1−p) and the odds against the event are (1−p)/p.</td>
      </tr>
      <tr>
          <td>175</td>
          <td>One-sided Test.</td>
          <td>C.f. two-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against either the alternative hypothesis that μ &lt; μ0 or the alternative μ &gt; μ0 (but not both). For example, a significance level 5%, one-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero, would reject the null hypothesis for values of</td>
      </tr>
      <tr>
          <td>176</td>
          <td>or, /, Disjunction, Logical Disjunction, ∨</td>
          <td>An operation on two logical propositions. If p and q are two propositions, (p / q) is a proposition that is true if p is true or if q is true (or both); otherwise, it is false. That is, (p / q) is true unless both p and q are false. The operation / is sometimes represented by the symbol ∨ and sometimes by the word or. C.f. exclusive disjunction, XOR.</td>
      </tr>
      <tr>
          <td>177</td>
          <td>Ordinal Variable.</td>
          <td>A variable whose possible values have a natural order, such as {short, medium, long}, {cold, warm, hot}, or {0, 1, 2, 3, …}. In contrast, a variable whose possible values are {straight, curly} or {Arizona, California, Montana, New York} would not naturally be ordinal. Arithmetic with the possible values of an ordinal variable does not necessarily make sense, but it does make sense to say that one possible value is larger than another.</td>
      </tr>
      <tr>
          <td>178</td>
          <td>Outcome Space.</td>
          <td>The outcome space is the set of all possible outcomes of a given random experiment. The outcome space is often denoted by the capital letter S.</td>
      </tr>
      <tr>
          <td>179</td>
          <td>Outlier.</td>
          <td>An outlier is an observation that is many SD’s from the mean. It is sometimes tempting to discard outliers, but this is imprudent unless the cause of the outlier can be identified, and the outlier is determined to be spurious. Otherwise, discarding outliers can cause one to underestimate the true variability of the measurement process.</td>
      </tr>
      <tr>
          <td>180</td>
          <td>P-value.</td>
          <td>Suppose we have a family of hypothesis tests of a null hypothesis that let us test the hypothesis at any significance level p between 0 and 100% we choose. The P value of the null hypothesis given the data is the smallest significance level p for which any of the tests would have rejected the null hypothesis. For example, let X be a test statistic, and for p between 0 and 100%, let xp be the smallest number such that, under the null hypothesis, P( X ≤ x ) ≥ p. Then for any p between 0 and 100%, the rule reject the null hypothesis if X &lt; xp tests the null hypothesis at significance level p. If we observed X = x, the P-value of the null hypothesis given the data would be the smallest p such that x &lt; xp.</td>
      </tr>
      <tr>
          <td>181</td>
          <td>Parameter.</td>
          <td>A numerical property of a population, such as its mean.</td>
      </tr>
      <tr>
          <td>182</td>
          <td>Partition.</td>
          <td>A partition of an event A is a collection of events {A1, A2, A3, … } such that the events in the collection are disjoint, and their union is A. That is, AjAk = {} unless j = k, and A = A1 ∪ A2 ∪ A3 ∪ … . If the event A is not specified, it is assumed to be the entire outcome space S.</td>
      </tr>
      <tr>
          <td>183</td>
          <td>Payoff Matrix.</td>
          <td>A way of representing what each player in a game wins or loses, as a function of his and his opponent’s strategies.</td>
      </tr>
      <tr>
          <td>184</td>
          <td>Percentile.</td>
          <td>The pth percentile of a list is the smallest number such that at least p% of the numbers in the list are no larger than it. The pth percentile of a random variable is the smallest number such that the chance that the random variable is no larger than it is at least p%. C.f. quantile.</td>
      </tr>
      <tr>
          <td>185</td>
          <td>Permutation.</td>
          <td>A permutation of a set is an arrangement of the elements of the set in some order. If the set has n things in it, there are n! different orderings of its elements. For the first element in an ordering, there are n possible choices, for the second, there remain n−1 possible choices, for the third, there are n−2, etc., and for the nth element of the ordering, there is a single choice remaining. By the fundamental rule of counting, the total number of sequences is thus n×(n−1)×(n−2)×…×1. Similarly, the number of orderings of length k one can form from n≥k things is n×(n−1)×(n−2)×…×(n−k+1) = n!/(n−k)!. This is denoted nPk, the number of permutations of n things taken k at a time. C.f. combinations.</td>
      </tr>
      <tr>
          <td>186</td>
          <td>Placebo.</td>
          <td>A “dummy” treatment that has no pharmacological effect; e.g., a sugar pill.</td>
      </tr>
      <tr>
          <td>187</td>
          <td>Placebo effect.</td>
          <td>The belief or knowledge that one is being treated can itself have an effect that confounds with the real effect of the treatment. Subjects given a placebo as a pain-killer report statistically significant reductions in pain in randomized experiments that compare them with subjects who receive no treatment at all. This very real psychological effect of a placebo, which has no direct biochemical effect, is called the placebo effect. Administering a placebo to the control group is thus important in experiments with human subjects; this is the essence of a blind experiment.</td>
      </tr>
      <tr>
          <td>188</td>
          <td>Point of Averages.</td>
          <td>In a scatterplot, the point whose coordinates are the arithmetic means of the corresponding variables. For example, if the variable X is plotted on the horizontal axis and the variable Y is plotted on the vertical axis, the point of averages has coordinates (mean of X, mean of Y).</td>
      </tr>
      <tr>
          <td>189</td>
          <td>Poisson Distribution.</td>
          <td>The Poisson distribution is a discrete probability distribution that depends on one parameter, m. If X is a random variable with the Poisson distribution with parameter m, then the probability that X = k is E−m × mk/k!, k = 0, 1, 2, … , where E is the base of the natural logarithm and ! is the factorial function. For all other values of k, the probability is zero. The expected value the Poisson distribution with parameter m is m, and the standard error of the Poisson distribution with parameter m is m½.</td>
      </tr>
      <tr>
          <td>190</td>
          <td>Population.</td>
          <td>A collection of units being studied. Units can be people, places, objects, epochs, drugs, procedures, or many other things. Much of statistics is concerned with estimating numerical properties (parameters) of an entire population from a random sample of units from the population.</td>
      </tr>
      <tr>
          <td>191</td>
          <td>Population Mean.</td>
          <td>The mean of the numbers in a numerical population. For example, the population mean of a box of numbered tickets is the mean of the list comprised of all the numbers on all the tickets. The population mean is a parameter. C.f. sample mean.</td>
      </tr>
      <tr>
          <td>192</td>
          <td>Population Percentage.</td>
          <td>The percentage of units in a population that possess a specified property. For example, the percentage of a given collection of registered voters who are registered as Republicans. If each unit that possesses the property is labeled with “1,” and each unit that does not possess the property is labeled with “0,” the population percentage is the same as the mean of that list of zeros and ones; that is, the population percentage is the population mean for a population of zeros and ones. The population percentage is a parameter. C.f. sample percentage.</td>
      </tr>
      <tr>
          <td>193</td>
          <td>Population Standard Deviation.</td>
          <td>The standard deviation of the values of a variable for a population. This is a parameter, not a statistic. C.f. sample standard deviation.</td>
      </tr>
      <tr>
          <td>194</td>
          <td>Post hoc ergo propter hoc.</td>
          <td>“After this, therefore because of this.” A fallacy of logic known since classical times: inferring a causal relation from correlation. Don’t do this at home!</td>
      </tr>
      <tr>
          <td>195</td>
          <td>Power.</td>
          <td>Refers to an hypothesis test. The power of a test against a specific alternative hypothesis is the chance that the test correctly rejects the null hypothesis when the alternative hypothesis is true.</td>
      </tr>
      <tr>
          <td>196</td>
          <td>Premise, logical premise.</td>
          <td>A premise is a proposition that is assumed to be true as part of a logical argument.</td>
      </tr>
      <tr>
          <td>197</td>
          <td>Prima facie.</td>
          <td>Latin for “at first glance.” “On the face of it.” Prima facie evidence for something is information that at first glance supports the conclusion. On closer examination, that might not be true; there could be another explanation for the evidence.</td>
      </tr>
      <tr>
          <td>198</td>
          <td>Principle of insufficient reason (Laplace)</td>
          <td>Laplace’s principle of insufficient reason says that if there is no reason to believe that the possible outcomes of an experiment are not equally likely, one should assume that the outcomes are equally likely. This is an example of a fallacy called appeal to ignorance.</td>
      </tr>
      <tr>
          <td>199</td>
          <td>Probability.</td>
          <td>The probability of an event is a number between zero and 100%. The meaning (interpretation) of probability is the subject of theories of probability, which differ in their interpretations. However, any rule for assigning probabilities to events has to satisfy the axioms of probability.</td>
      </tr>
      <tr>
          <td>200</td>
          <td>Probability density function.</td>
          <td>The chance that a continuous random variable is in any range of values can be calculated as the area under a curve over that range of values. The curve is the probability density function of the random variable. That is, if X is a continuous random variable, there is a function f(x) such that for every pair of numbers a≤b, P(a≤ X ≤b) = (area under f between a and b); f is the probability density function of X. For example, the probability density function of a random variable with a standard normal distribution is the normal curve. Only continuous random variables have probability density functions.</td>
      </tr>
      <tr>
          <td>201</td>
          <td>Probability Distribution.</td>
          <td>The probability distribution of a random variable specifies the chance that the variable takes a value in any subset of the real numbers. (The subsets have to satisfy some technical conditions that are not important for this course.) The probability distribution of a random variable is completely characterized by the cumulative probability distribution function; the terms sometimes are used synonymously. The probability distribution of a discrete random variable can be characterized by the chance that the random variable takes each of its possible values. For example, the probability distribution of the total number of spots S showing on the roll of two fair dice can be written as a table: The probability distribution of a continuous random variable can be characterized by its probability density function.</td>
      </tr>
      <tr>
          <td>202</td>
          <td>Probability Histogram.</td>
          <td>A probability histogram for a random variable is analogous to a histogram of data, but instead of plotting the area of the bins proportional to the relative frequency of observations in the class interval, one plots the area of the bins proportional to the probability that the random variable is in the class interval.</td>
      </tr>
      <tr>
          <td>203</td>
          <td>Probability Sample.</td>
          <td>A sample drawn from a population using a random mechanism so that every element of the population has a known chance of ending up in the sample.</td>
      </tr>
      <tr>
          <td>204</td>
          <td>Probability, Theories of.</td>
          <td>A theory of probability is a way of assigning meaning to probability statements such as “the chance that a thumbtack lands point-up is 2/3.” That is, a theory of probability connects the mathematics of probability, which is the set of consequences of the axioms of probability, with the real world of observation and experiment. There are several common theories of probability. According to the frequency theory of probability, the probability of an event is the limit of the percentage of times that the event occurs in repeated, independent trials under essentially the same circumstances. According to the subjective theory of probability, a probability is a number that measures how strongly we believe an event will occur. The number is on a scale of 0% to 100%, with 0% indicating that we are completely sure it won’t occur, and 100% indicating that we are completely sure that it will occur. According to the theory of equally likely outcomes, if an experiment has n possible outcomes, and (for example, by symmetry) there is no reason that any of the n possible outcomes should occur preferentially to any of the others, then the chance of each outcome is 100%/n. Each of these theories has its limitations, its proponents, and its detractors.</td>
      </tr>
      <tr>
          <td>205</td>
          <td>Proposition, logical proposition.</td>
          <td>A logical proposition is a statement that can be either true or false. For example, “the sun is shining in Berkeley right now” is a proposition. See also &amp;, ↔, →, /, XOR, converse, contrapositive and logical argument.</td>
      </tr>
      <tr>
          <td>206</td>
          <td>Prosecutor’s Fallacy.</td>
          <td>The prosecutor’s fallacy consists of confusing two conditional probabilities: P(A/B) and P(B/A). For instance, P(A/B) could be the chance of observing the evidence if the accused is guilty, while P(B/A) is the chance that the accused is guilty given the evidence. The latter might not make sense at all, but even when it does, the two numbers need not be equal. This fallacy is related to a common misinterpretation of P-values.</td>
      </tr>
      <tr>
          <td>207</td>
          <td>Qualitative Variable.</td>
          <td>A qualitative variable is one whose values are adjectives, such as colors, genders, nationalities, etc. C.f. quantitative variable and categorical variable.</td>
      </tr>
      <tr>
          <td>208</td>
          <td>Quantile.</td>
          <td>The qth quantile of a list (0 &lt; q ≤ 1) is the smallest number such that the fraction q or more of the elements of the list are less than or equal to it. I.e., if the list contains n numbers, the qth quantile, is the smallest number Q such that at least n×q elements of the list are less than or equal to Q.</td>
      </tr>
      <tr>
          <td>209</td>
          <td>Quantitative Variable.</td>
          <td>A variable that takes numerical values for which arithmetic makes sense, for example, counts, temperatures, weights, amounts of money, etc. For some variables that take numerical values, arithmetic with those values does not make sense; such variables are not quantitative. For example, adding and subtracting social security numbers does not make sense. Quantitative variables typically have units of measurement, such as inches, people, or pounds.</td>
      </tr>
      <tr>
          <td>210</td>
          <td>Quartiles.</td>
          <td>There are three quartiles. The first or lower quartile (LQ) of a list is a number (not necessarily a number in the list) such that at least 1/4 of the numbers in the list are no larger than it, and at least 3/4 of the numbers in the list are no smaller than it. The second quartile is the median. The third or upper quartile (UQ) is a number such that at least 3/4 of the entries in the list are no larger than it, and at least 1/4 of the numbers in the list are no smaller than it. To find the quartiles, first sort the list into increasing order. Find the smallest integer that is at least as big as the number of entries in the list divided by four. Call that integer k. The kth element of the sorted list is the lower quartile. Find the smallest integer that is at least as big as the number of entries in the list divided by two. Call that integer l. The lth element of the sorted list is the median. Find the smallest integer that is at least as large as the number of entries in the list times 3/4. Call that integer m. The mth element of the sorted list is the upper quartile.</td>
      </tr>
      <tr>
          <td>211</td>
          <td>Quota Sample.</td>
          <td>A quota sample is a sample picked to match the population with respect to some summary characteristics. It is not a random sample. For example, in an opinion poll, one might select a sample so that the proportions of various ethnicities in the sample match the proportions of ethnicities in the overall population from which the sample is drawn. Matching on summary statistics does not guarantee that the sample comes close to matching the population with respect to the quantity of interest. As a result, quota samples are typically biased, and the size of the bias is generally impossible to determine unless the result can be compared with a known result for the whole population or for a random sample. Moreover, with a quota sample, it is impossible to quantify how representative of the population a quota sample is likely to be—quota sampling does not allow one to quantify the likely size of sampling error. Quota samples are to be avoided, and results based on quota samples are to be viewed with suspicion. See also convenience sample.</td>
      </tr>
      <tr>
          <td>212</td>
          <td>Random Error.</td>
          <td>All measurements are subject to error, which can often be broken down into two components: a bias or systematic error, which affects all measurements the same way; and a random error, which is in general different each time a measurement is made, and behaves like a number drawn with replacement from a box of numbered tickets whose average is zero.</td>
      </tr>
      <tr>
          <td>213</td>
          <td>Random Event.</td>
          <td>See random experiment.</td>
      </tr>
      <tr>
          <td>214</td>
          <td>Random Experiment.</td>
          <td>An experiment or trial whose outcome is not perfectly predictable, but for which the long-run relative frequency of outcomes of different types in repeated trials is predictable. Note that “random” is different from “haphazard,” which does not necessarily imply long-term regularity.</td>
      </tr>
      <tr>
          <td>215</td>
          <td>Random Sample.</td>
          <td>A random sample is a sample whose members are chosen at random from a given population in such a way that the chance of obtaining any particular sample can be computed. The number of units in the sample is called the sample size, often denoted n. The number of units in the population often is denoted N. Random samples can be drawn with or without replacing objects between draws; that is, drawing all n objects in the sample at once (a random sample without replacement), or drawing the objects one at a time, replacing them in the population between draws (a random sample with replacement). In a random sample with replacement, any given member of the population can occur in the sample more than once. In a random sample without replacement, any given member of the population can be in the sample at most once. A random sample without replacement in which every subset of n of the N units in the population is equally likely is also called a simple random sample. The term random sample with replacement denotes a random sample drawn in such a way that every multiset of n units in the population is equally likely. See also probability sample.</td>
      </tr>
      <tr>
          <td>216</td>
          <td>Random Variable.</td>
          <td>A random variable is an assignment of numbers to possible outcomes of a random experiment. For example, consider tossing three coins. The number of heads showing when the coins land is a random variable: it assigns the number 0 to the outcome {T, T, T}, the number 1 to the outcome {T, T, H}, the number 2 to the outcome {T, H, H}, and the number 3 to the outcome {H, H, H}.</td>
      </tr>
      <tr>
          <td>217</td>
          <td>Randomized Controlled Experiment.</td>
          <td>An experiment in which chance is deliberately introduced in assigning subjects to the treatment and control groups. For example, we could write an identifying number for each subject on a slip of paper, stir up the slips of paper, and draw slips without replacement until we have drawn half of them. The subjects identified on the slips drawn could then be assigned to treatment, and the rest to control. Randomizing the assignment tends to decrease confounding of the treatment effect with other factors, by making the treatment and control groups roughly comparable in all respects but the treatment.</td>
      </tr>
      <tr>
          <td>218</td>
          <td>Range.</td>
          <td>The range of a set of numbers is the largest value in the set minus the smallest value in the set. Note that as a statistical term, the range is a single number, not a range of numbers.</td>
      </tr>
      <tr>
          <td>219</td>
          <td>Real number.</td>
          <td>Loosely speaking, the real numbers are all numbers that can be represented as fractions (rational numbers), whether proper or improper—and all numbers in between the rational numbers. That is, the real numbers comprise the rational numbers and all limits of Cauchy sequences of rational numbers, where the Cauchy sequence is with respect to the absolute value metric. (More formally, the real numbers are the completion of the set of rational numbers in the topology induced by the absolute value function.) The real numbers contain all integers, all fractions, and all irrational (and transcendental) numbers, such as π, e, and 2½. There are uncountably many real numbers between 0 and 1; in contrast, there are only countably many rational numbers between 0 and 1.</td>
      </tr>
      <tr>
          <td>220</td>
          <td>Regression, Linear Regression.</td>
          <td>Linear regression fits a line to a scatterplot in such a way as to minimize the sum of the squares of the residuals. The resulting regression line, together with the standard deviations of the two variables or their correlation coefficient, can be a reasonable summary of a scatterplot if the scatterplot is roughly football-shaped. In other cases, it is a poor summary. If we are regressing the variable Y on the variable X, and if Y is plotted on the vertical axis and X is plotted on the horizontal axis, the regression line passes through the point of averages, and has slope equal to the correlation coefficient times the SD of Y divided by the SD of X. This page shows a scatterplot, with a button to plot the regression line.</td>
      </tr>
      <tr>
          <td>221</td>
          <td>Regression Fallacy.</td>
          <td>The regression fallacy is to attribute the regression effect to an external cause.</td>
      </tr>
      <tr>
          <td>222</td>
          <td>Regression Toward the Mean, Regression Effect.</td>
          <td>Suppose one measures two variables for each member of a group of individuals, and that the correlation coefficient of the variables is positive (negative). If the value of the first variable for that individual is above average, the value of the second variable for that individual is likely to be above (below) average, but by fewer standard deviations than the first variable is. That is, the second observation is likely to be closer to the mean in standard units. For example, suppose one measures the heights of fathers and sons. Each individual is a (father, son) pair; the two variables measured are the height of the father and the height of the son. These two variables will tend to have a positive correlation coefficient: fathers who are taller than average tend to have sons who are taller than average. Consider a (father, son) pair chosen at random from this group. Suppose the father’s height is 3SD above the average of all the fathers’ heights. (The SD is the standard deviation of the fathers’ heights.) Then the son’s height is also likely to be above the average of the sons’ heights, but by fewer than 3SD (here the SD is the standard deviation of the sons’ heights).</td>
      </tr>
      <tr>
          <td>223</td>
          <td>Rejection region.</td>
          <td>In an hypothesis test using a test statistic, the rejection region is the set of values of the test statistic for which we reject the null hypothesis.</td>
      </tr>
      <tr>
          <td>224</td>
          <td>Residual.</td>
          <td>The difference between a datum and the value predicted for it by a model. In linear regression of a variable plotted on the vertical axis onto a variable plotted on the horizontal axis, a residual is the “vertical” distance from a datum to the line. Residuals can be positive (if the datum is above the line) or negative (if the datum is below the line). Plots of residuals can reveal computational errors in linear regression, as well as conditions under which linear regression is inappropriate, such as nonlinearity and heteroscedasticity. If linear regression is performed properly, the sum of the residuals from the regression line must be zero; otherwise, there is a computational error somewhere.</td>
      </tr>
      <tr>
          <td>225</td>
          <td>Residual Plot.</td>
          <td>A residual plot for a regression is a plot of the residuals from the regression against the explanatory variable.</td>
      </tr>
      <tr>
          <td>226</td>
          <td>Resistant.</td>
          <td>A statistic is said to be resistant if corrupting a datum cannot change the statistic much. The mean is not resistant; the median is. See also breakdown point.</td>
      </tr>
      <tr>
          <td>227</td>
          <td>Root-mean-square (RMS).</td>
          <td>The RMS of a list is the square-root of the mean of the squares of the elements in the list. It is a measure of the average “size” of the elements of the list. To compute the RMS of a list, you square all the entries, average the numbers you get, and take the square-root of that average.</td>
      </tr>
      <tr>
          <td>228</td>
          <td>Root-mean-square error (RMSE).</td>
          <td>The RMSE of an an estimator of a parameter is the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then RMSE(X) = ( E( (X−t)2 ) )½. The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator. See also mean squared error.</td>
      </tr>
      <tr>
          <td>229</td>
          <td>rms Error of Regression</td>
          <td>The rms error of regression is the rms of the vertical residuals from the regression line. For regressing Y on X, the rms error of regression is equal to (1 − r2)½×SDY, where r is the correlation coefficient between X and Y and SDY is the standard deviation of the values of Y.</td>
      </tr>
      <tr>
          <td>230</td>
          <td>Sample.</td>
          <td>A sample is a collection of units from a population. See also random sample.</td>
      </tr>
      <tr>
          <td>231</td>
          <td>Sample Mean.</td>
          <td>The arithmetic mean of a random sample from a population. It is a statistic commonly used to estimate the population mean. Suppose there are n data, {x1, x2, … , xn}. The sample mean is (x1 + x2 + … + xn)/n. The expected value of the sample mean is the population mean. For sampling with replacement, the SE of the sample mean is the population standard deviation, divided by the square-root of the sample size. For sampling without replacement, the SE of the sample mean is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample mean for sampling with replacement, with N the size of the population and n the size of the sample.</td>
      </tr>
      <tr>
          <td>232</td>
          <td>Sample Percentage.</td>
          <td>The percentage of a random sample with a certain property, such as the percentage of voters registered as Democrats in a simple random sample of voters. The sample mean is a statistic commonly used to estimate the population percentage. The expected value of the sample percentage from a simple random sample or a random sample with replacement is the population percentage. The SE of the sample percentage for sampling with replacement is (p(1−p)/n )½, where p is the population percentage and n is the sample size. The SE of the sample percentage for sampling without replacement is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample percentage for sampling with replacement, with N the size of the population and n the size of the sample. The SE of the sample percentage is often estimated by the bootstrap.</td>
      </tr>
      <tr>
          <td>233</td>
          <td>Sample Size.</td>
          <td>The number of elements in a sample from a population.</td>
      </tr>
      <tr>
          <td>234</td>
          <td>Sound argument.</td>
          <td>A logical argument is sound if it is logically valid and its premises are in fact true. An argument can be logically valid and yet not sound—if its premises are false.</td>
      </tr>
      <tr>
          <td>235</td>
          <td>Sample Standard Deviation, S.</td>
          <td>The sample standard deviation S is an estimator of the standard deviation of a population based on a random sample from the population. The sample standard deviation is a statistic that measures how “spread out” the sample is around the sample mean. It is quite similar to the standard deviation of the sample, but instead of averaging the squared deviations (to get the rms of the deviations of the data from the sample mean) it divides the sum of the squared deviations by (number of data − 1) before taking the square-root. Suppose there are n data, {x1, x2, … , xn}, with mean M = (x1 + x2 + … + xn)/n. Then s = ( ((x1 − M)2 + (x2 − M)2 + … + (xn − M)2)/(n−1) )½ The square of the sample standard deviation, S2 (the sample variance) is an unbiased estimator of the square of the SD of the population (the variance of the population).</td>
      </tr>
      <tr>
          <td>236</td>
          <td>Sample Sum.</td>
          <td>The sum of a random sample from a population. The expected value of the sample sum is the sample size times the population mean. For sampling with replacement, the SE of the sample sum is the population standard deviation, times the square-root of the sample size. For sampling without replacement, the SE of the sample sum is the finite-population correction ((N−n)/(N−1))½ times the SE of the sample sum for sampling with replacement, with N the size of the population and n the size of the sample.</td>
      </tr>
      <tr>
          <td>237</td>
          <td>Sample Survey.</td>
          <td>A survey based on the responses of a sample of individuals, rather than the entire population.</td>
      </tr>
      <tr>
          <td>238</td>
          <td>Sample Variance</td>
          <td>The sample variance is the square of the sample standard deviation S. It is an unbiased estimator of the square of the population standard deviation, which is also called the variance of the population.</td>
      </tr>
      <tr>
          <td>239</td>
          <td>Sampling distribution.</td>
          <td>The sampling distribution of an estimator is the probability distribution of the estimator when it is applied to random samples. The tool on this page allows you to explore empirically the sampling distribution of the sample mean and the sample percentage of random draws with or without replacement draws from a box of numbered tickets.</td>
      </tr>
      <tr>
          <td>240</td>
          <td>Sampling error.</td>
          <td>In estimating from a random sample, the difference between the estimator and the parameter can be written as the sum of two components: bias and sampling error. The bias is the average error of the estimator over all possible samples. The bias is not random. Sampling error is the component of error that varies from sample to sample. The sampling error is random: it comes from “the luck of the draw” in which units happen to be in the sample. It is the chance variation of the estimator. The average of the sampling error over all possible samples (the expected value of the sampling error) is zero. The standard error of the estimator is a measure of the typical size of the sampling error.</td>
      </tr>
      <tr>
          <td>241</td>
          <td>Sampling unit.</td>
          <td>A sample from a population can be drawn one unit at a time, or more than one unit at a time (one can sample clusters of units). The fundamental unit of the sample is called the sampling unit. It need not be a unit of the population.</td>
      </tr>
      <tr>
          <td>242</td>
          <td>Scatterplot.</td>
          <td>A scatterplot is a way to visualize bivariate data. A scatterplot is a plot of pairs of measurements on a collection of “individuals” (which need not be people). For example, suppose we record the heights and weights of a group of 100 people. The scatterplot of those data would be 100 points. Each point represents one person’s height and weight. In a scatterplot of weight against height, the x-coordinate of each point would be height of one person, the y-coordinate of that point would be the weight of the same person. In a scatterplot of height against weight, the x-coordinates would be the weights and the y-coordinates would be the heights.</td>
      </tr>
      <tr>
          <td>243</td>
          <td>Scientific Method.</td>
          <td>The scientific method….</td>
      </tr>
      <tr>
          <td>244</td>
          <td>SD line.</td>
          <td>For a scatterplot, a line that goes through the point of averages, with slope equal to the ratio of the standard deviations of the two plotted variables. If the variable plotted on the horizontal axis is called X and the variable plotted on the vertical axis is called Y, the slope of the SD line is the SD of Y, divided by the SD of X.</td>
      </tr>
      <tr>
          <td>245</td>
          <td>Secular Trend.</td>
          <td>A linear association (trend) with time.</td>
      </tr>
      <tr>
          <td>246</td>
          <td>Selection Bias.</td>
          <td>A systematic tendency for a sampling procedure to include and/or exclude units of a certain type. For example, in a quota sample, unconscious prejudices or predilections on the part of the interviewer can result in selection bias. Selection bias is a potential problem whenever a human has latitude in selecting individual units for the sample; it tends to be eliminated by probability sampling schemes in which the interviewer is told exactly whom to contact (with no room for individual choice).</td>
      </tr>
      <tr>
          <td>247</td>
          <td>Self-Selection.</td>
          <td>Self-selection occurs when individuals decide for themselves whether they are in the control group or the treatment group. Self-selection is quite common in studies of human behavior. For example, studies of the effect of smoking on human health involve self-selection: individuals choose for themselves whether or not to smoke. Self-selection precludes an experiment; it results in an observational study. When there is self-selection, one must be wary of possible confounding from factors that influence individuals’ decisions to belong to the treatment group.</td>
      </tr>
      <tr>
          <td>248</td>
          <td>Set.</td>
          <td>A set is a collection of things (called elements), without regard to their order. An item is either in a set (it is an element of the set), or it is not. It cannot be in the set more than once. Two sets are equal if they contain electly the same elements. For instance, the set {1, 2, 3, 4} is equal to the set {1, 4, 3, 2}, but not to the set {0, 1, 2, 3}. As another example, the set {1, 2, 2} is equal to the set {1, 2}: they have the same two (distinct) elements, 1 and 2.</td>
      </tr>
      <tr>
          <td>249</td>
          <td>Significance, Significance level, Statistical significance.</td>
          <td>The significance level of an hypothesis test is the chance that the test erroneously rejects the null hypothesis when the null hypothesis is true.</td>
      </tr>
      <tr>
          <td>250</td>
          <td>Simple Random Sample.</td>
          <td>A simple random sample of n units from a population is a random sample drawn by a procedure that is equally likely to give every collection of n units from the population; that is, the probability that the sample will consist of any given subset of n of the N units in the population is 1/NCn. Simple random sampling is sampling at random without replacement (without replacing the units between draws). A simple random sample of size n from a population of N ≥ n units can be constructed by assigning a random number between zero and one to each unit in the population, then taking those units that were assigned the n largest random numbers to be the sample.</td>
      </tr>
      <tr>
          <td>251</td>
          <td>Simpson’s Paradox.</td>
          <td>What is true for the parts is not necessarily true for the whole. See also confounding.</td>
      </tr>
      <tr>
          <td>252</td>
          <td>Skewed Distribution.</td>
          <td>A distribution that is not symmetrical.</td>
      </tr>
      <tr>
          <td>253</td>
          <td>Spread, Measure of.</td>
          <td>See also inter-quartile range, range, and standard deviation.</td>
      </tr>
      <tr>
          <td>254</td>
          <td>Square-Root Law.</td>
          <td>The Square-Root Law says that the standard error (SE) of the sample sum of n random draws with replacement from a box of tickets with numbers on them is SE(sample sum) = n½×SD(box), and the standard error of the sample mean of n random draws with replacement from a box of tickets is SE(sample mean) = n−½×SD(box), where SD(box) is the standard deviation of the list of the numbers on all the tickets in the box (including repeated values).</td>
      </tr>
      <tr>
          <td>255</td>
          <td>Standard Deviation (SD).</td>
          <td>The standard deviation of a set of numbers is the rms of the set of deviations between each element of the set and the mean of the set. See also sample standard deviation.</td>
      </tr>
      <tr>
          <td>256</td>
          <td>Standard Error (SE).</td>
          <td>The Standard Error of a random variable is a measure of how far it is likely to be from its expected value; that is, its scatter in repeated experiments. The SE of a random variable X is defined to be SE(X) = [E( (X − E(X))2 )] ½. That is, the standard error is the square-root of the expected squared difference between the random variable and its expected value. The SE of a random variable is analogous to the SD of a list.</td>
      </tr>
      <tr>
          <td>257</td>
          <td>Standard Normal Curve.</td>
          <td>See normal curve.</td>
      </tr>
      <tr>
          <td>258</td>
          <td>Standard Units.</td>
          <td>A variable (a set of data) is said to be in standard units if its mean is zero and its standard deviation is one. You transform a set of data into standard units by subtracting the mean from each element of the list, and dividing the results by the standard deviation. A random variable is said to be in standard units if its expected value is zero and its standard error is one. You transform a random variable to standard units by subtracting its expected value then dividing by its standard error.</td>
      </tr>
      <tr>
          <td>259</td>
          <td>Standardize.</td>
          <td>To transform into standard units.</td>
      </tr>
      <tr>
          <td>260</td>
          <td>Statistic.</td>
          <td>A number that can be computed from data, involving no unknown parameters. As a function of a random sample, a statistic is a random variable. Statistics are used to estimate parameters, and to test hypotheses.</td>
      </tr>
      <tr>
          <td>261</td>
          <td>Stratified Sample.</td>
          <td>In a stratified sample, subsets of sampling units are selected separately from different strata, rather than from the frame as a whole.</td>
      </tr>
      <tr>
          <td>262</td>
          <td>Stratified sampling</td>
          <td>The act of drawing a stratified sample.</td>
      </tr>
      <tr>
          <td>263</td>
          <td>Stratum</td>
          <td>In random sampling, sometimes the sample is drawn separately from different disjoint subsets of the population. Each such subset is called a stratum. (The plural of stratum is strata.) Samples drawn in such a way are called stratified samples. Estimators based on stratified random samples can have smaller sampling errors than estimators computed from simple random samples of the same size, if the average variability of the variable of interest within strata is smaller than it is across the entire population; that is, if stratum membership is associated with the variable. For example, to determine average home prices in the U.S., it would be advantageous to stratify on geography, because average home prices vary enormously with location. We might divide the country into states, then divide each state into urban, suburban, and rural areas; then draw random samples separately from each such division.</td>
      </tr>
      <tr>
          <td>264</td>
          <td>Studentized score</td>
          <td>The observed value of a statistic, minus the expected value of the statistic, divided by the estimated standard error of the statistic.</td>
      </tr>
      <tr>
          <td>265</td>
          <td>Student’s t curve.</td>
          <td>Student’s t curve is a family of curves indexed by a parameter called the degrees of freedom, which can take the values 1, 2, … Student’s t curve is used to approximate some probability histograms. Consider a population of numbers that are nearly normally distributed and have population mean is μ. Consider drawing a random sample of size n with replacement from the population, and computing the sample mean M and the sample standard deviation S. Define the random variable T = (M − μ)/(S/n½). If the sample size n is large, the probability histogram of T can be approximated accurately by the normal curve. However, for small and intermediate values of n, Student’s t curve with n − 1 degrees of freedom gives a better approximation. That is, P(a &lt; T &lt; b) is approximately the area under Student’s T curve with n − 1 degrees of freedom, from a to b. Student’s t curve can be used to test hypotheses about the population mean and construct confidence intervals for the population mean, when the population distribution is known to be nearly normally distributed. This page contains a tool that shows Student’s t curve and lets you find the area under parts of the curve.</td>
      </tr>
      <tr>
          <td>266</td>
          <td>Subject, Experimental Subject.</td>
          <td>A member of the control group or the treatment group.</td>
      </tr>
      <tr>
          <td>267</td>
          <td>Subset.</td>
          <td>A subset of a given set is a collection of things that belong to the original set. Every element of the subset must belong to the original set, but not every element of the original set need be in a subset (otherwise, a subset would always be identical to the set it came from).</td>
      </tr>
      <tr>
          <td>268</td>
          <td>Survey.</td>
          <td>See sample survey.</td>
      </tr>
      <tr>
          <td>269</td>
          <td>Symmetric Distribution.</td>
          <td>The probability distribution of a random variable X is symmetric if there is a number a such that the chance that X≥a+b is the same as the chance that X≤a−b for every value of b. A list of numbers has a symmetric distribution if there is a number a such that the fraction of numbers in the list that are greater than or equal to a+b is the same as the fraction of numbers in the list that are less than or equal to a−b, for every value of b. In either case, the histogram or the probability histogram will be symmetrical about a vertical line drawn at x=a.</td>
      </tr>
      <tr>
          <td>270</td>
          <td>Systematic error.</td>
          <td>An error that affects all the measurements similarly. For example, if a ruler is too short, everything measured with it will appear to be longer than it really is (ignoring random error). If your watch runs fast, every time interval you measure with it will appear to be longer than it really is (again, ignoring random error). Systematic errors do not tend to average out.</td>
      </tr>
      <tr>
          <td>271</td>
          <td>Systematic sample.</td>
          <td>A systematic sample from a frame of units is one drawn by listing the units and selecting every kth element of the list. For example, if there are N units in the frame, and we want a sample of size N/10, we would take every tenth unit: the first unit, the eleventh unit, the 21st unit, etc. Systematic samples are not random samples, but they often behave essentially as if they were random, if the order in which the units appears in the list is haphazard. Systematic samples are a special case of cluster samples.</td>
      </tr>
      <tr>
          <td>272</td>
          <td>Systematic random sample.</td>
          <td>A systematic sample starting at a random point in the listing of units in the of frame, instead of starting at the first unit. Systematic random sampling is better than systematic sampling, but typically not as good as simple random sampling.</td>
      </tr>
      <tr>
          <td>273</td>
          <td>t test.</td>
          <td>An hypothesis test based on approximating the probability histogram of the test statistic by Student’s t curve. t tests usually are used to test hypotheses about the mean of a population when the sample size is intermediate and the distribution of the population is known to be nearly normal.</td>
      </tr>
      <tr>
          <td>274</td>
          <td>Test Statistic.</td>
          <td>A statistic used to test hypotheses. An hypothesis test can be constructed by deciding to reject the null hypothesis when the value of the test statistic is in some range or collection of ranges. To get a test with a specified significance level, the chance when the null hypothesis is true that the test statistic falls in the range where the hypothesis would be rejected must be at most the specified significance level. The Z statistic is a common test statistic.</td>
      </tr>
      <tr>
          <td>275</td>
          <td>Transformation.</td>
          <td>Transformations turn lists into other lists, or variables into other variables. For example, to transform a list of temperatures in degrees Celsius into the corresponding list of temperatures in degrees Fahrenheit, you multiply each element by 9/5, and add 32 to each product. This is an example of an affine transformation: multiply by something and add something (y = ax + b is the general affine transformation of x; it’s the familiar equation of a straight line). In a linear transformation, you only multiply by something (y = ax). Affine transformations are used to put variables in standard units. In that case, you subtract the mean and divide the results by the SD. This is equivalent to multiplying by the reciprocal of the SD and adding the negative of the mean, divided by the SD, so it is an affine transformation. Affine transformations with positive multiplicative constants have a simple effect on the mean, median, mode, quartiles, and other percentiles: the new value of any of these is the old one, transformed using exactly the same formula. When the multiplicative constant is negative, the mean, median, mode, are still transformed by the same rule, but quartiles and percentiles are reversed: the qth quantile of the transformed distribution is the transformed value of the 1−qth quantile of the original distribution (ignoring the effect of data spacing). The effect of an affine transformation on the SD, range, and IQR, is to make the new value the old value times the absolute value of the number you multiplied the first list by: what you added does not affect them.</td>
      </tr>
      <tr>
          <td>276</td>
          <td>Treatment.</td>
          <td>The substance or procedure studied in an experiment or observational study. At issue is whether the treatment has an effect on the outcome or variable of interest.</td>
      </tr>
      <tr>
          <td>277</td>
          <td>Treatment Effect.</td>
          <td>The effect of the treatment on the variable of interest. Establishing whether the treatment has an effect is the point of an experiment.</td>
      </tr>
      <tr>
          <td>278</td>
          <td>Treatment group.</td>
          <td>The individuals who receive the treatment, as opposed to those in the control group, who do not.</td>
      </tr>
      <tr>
          <td>279</td>
          <td>Tuple, n-tuple.</td>
          <td>A tuple is an ordered collection of things. Two tuples are equal if they contain the same things, in the same order. For instance, the tuple (1, 2, 3) is equal to the tuple (1, 2, 3) but not equal to the tuple (1, 3, 2). Tuples can contain repeated elements. For instance, the tuple (1, 2, 2) is not equal to the tuple (1, 2), nor to the tuple (2, 2, 1). An n-tuple, where n is an integer, is a tuple with n positions. For example, (1, 2) is a 2-tuple (aka ordered pair) and (7, 3, 2, 2, 2, 1) is a 6-tuple.</td>
      </tr>
      <tr>
          <td>280</td>
          <td>Two-sided Hypothesis test.</td>
          <td>C.f. one-sided test. An hypothesis test of the null hypothesis that the value of a parameter, μ, is equal to a null value, μ0, designed to have power against the alternative hypothesis that either μ &lt; μ0 or μ &gt; μ0 (the alternative hypothesis contains values on both sides of the null value). For example, a significance level 5%, two-sided z test of the null hypothesis that the mean of a population equals zero against the alternative that it is greater than zero would reject the null hypothesis for values of $$ /z/ = \left / \frac{\mbox{sample mean}}{\mbox{SE}} \right / &gt; 1.96.$$</td>
      </tr>
      <tr>
          <td>281</td>
          <td>Type I and Type II errors.</td>
          <td>These refer to hypothesis testing. A Type I error occurs when the null hypothesis is rejected erroneously when it is in fact true. A Type II error occurs if the null hypothesis is not rejected when it is in fact false. See also significance level and power.</td>
      </tr>
      <tr>
          <td>282</td>
          <td>Unbiased.</td>
          <td>Not biased; having zero bias.</td>
      </tr>
      <tr>
          <td>283</td>
          <td>Uncontrolled Experiment.</td>
          <td>An experiment in which there is no control group; i.e., in which the method of comparison is not used: the experimenter decides who gets the treatment, but the outcome of the treated group is not compared with the outcome of a control group that does not receive treatment.</td>
      </tr>
      <tr>
          <td>284</td>
          <td>Uncorrelated.</td>
          <td>A set of bivariate data is uncorrelated if its correlation coefficient is zero. Two random variables are uncorrelated if the expected value of their product equals the product of their expected values. If two random variables are independent, they are uncorrelated. (The converse is not true in general.)</td>
      </tr>
      <tr>
          <td>285</td>
          <td>Uncountable.</td>
          <td>A set is uncountable if it is not countable, that is, if its elements cannot be put in one-to-one correspondence with the positive integers.</td>
      </tr>
      <tr>
          <td>286</td>
          <td>Unimodal.</td>
          <td>Having exactly one mode.</td>
      </tr>
      <tr>
          <td>287</td>
          <td>Union.</td>
          <td>The union of two or more sets is the set of objects contained by at least one of the sets. The union of the events A and B is denoted “A+B”, “A or B”, and “A∪B”. C.f. intersection.</td>
      </tr>
      <tr>
          <td>288</td>
          <td>Unit.</td>
          <td>A member of a population.</td>
      </tr>
      <tr>
          <td>289</td>
          <td>Univariate.</td>
          <td>Having or having to do with a single variable. Some univariate techniques and statistics include the histogram, IQR, mean, median, percentiles, quantiles, and SD. C.f. bivariate.</td>
      </tr>
      <tr>
          <td>290</td>
          <td>Upper Quartile (UQ).</td>
          <td>See quartiles.</td>
      </tr>
      <tr>
          <td>291</td>
          <td>Valid (logical) argument.</td>
          <td>A valid logical argument is one in which the truth of the premises indeed guarantees the truth of the conclusion. For example, the following logical argument is valdraft: false</td>
      </tr>
      <tr>
          <td>id: If the forecast calls for rain, I will not wear sandals. The forecast calls for rain. Therefore, I will not wear sandals. This argument has two premises which, together, guarantee the truth of the conclusion. An argument can be logically valid even if its premises are false. See also invalid argument and sound argument.</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>292</td>
          <td>Variable.</td>
          <td>A numerical value or a characteristic that can differ from individual to individual. See also categorical variable, qualitative variable, quantitative variable, discrete variable, continuous variable, and random variable.</td>
      </tr>
      <tr>
          <td>293</td>
          <td>Variance, population variance</td>
          <td>The variance of a list is the square of the standard deviation of the list, that is, the average of the squares of the deviations of the numbers in the list from their mean. The variance of a random variable X, Var(X), is the expected value of the squared difference between the variable and its expected value: Var(X) = E((X − E(X))2). The variance of a random variable is the square of the standard error (SE) of the variable.</td>
      </tr>
      <tr>
          <td>294</td>
          <td>Venn Diagram.</td>
          <td>A pictorial way of showing the relations among sets or events. The universal set or outcome space is usually drawn as a rectangle; sets are regions within the rectangle. The overlap of the regions corresponds to the intersection of the sets. If the regions do not overlap, the sets are disjoint. The part of the rectangle included in one or more of the regions corresponds to the union of the sets. This page contains a tool that illustrates Venn diagrams; the tool represents the probability of an event by the area of the event.</td>
      </tr>
      <tr>
          <td>295</td>
          <td>XOR, exclusive disjunction.</td>
          <td>XOR is an operation on two logical propositions. If p and q are two propositions, (p XOR q) is a proposition that is true if either p is true or if q is true, but not both. (p XOR q) is logically equivalent to ((p</td>
      </tr>
      <tr>
          <td>296</td>
          <td>z-score</td>
          <td>The observed value of the Z statistic.</td>
      </tr>
      <tr>
          <td>297</td>
          <td>Z statistic</td>
          <td>A Z statistic is a test statistic whose distribution under the null hypothesis has expected value zero and can be approximated well by the normal curve. Usually, Z statistics are constructed by standardizing some other statistic. The Z statistic is related to the original statistic by Z = (original − expected value of original)/SE(original).</td>
      </tr>
      <tr>
          <td>298</td>
          <td>z-test</td>
          <td>An hypothesis test based on approximating thehttps://dasarpai.com/300-important-statistical-terms/ probability histogram of the Z statistic under the null hypothesis by the normal curve.</td>
      </tr>
  </tbody>
</table>
<p>These definitions are taken from - <a href="https://www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm">https://www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm</a></p>
<p>I am maintaining these terms on this page, so that it here for my reference. I am rearranging these terms here. Removing certain unrelated terms (as per me). I may change this table based on my further exploration and understanding. For the original definition please read from the above link. If any term is misunderstood or misexplained then please comment below. I will correct it here, after due consideration. If any important term is missing from this table then please mention the commentbox below.</p>

	<style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<div class="d-print-none">
<h2 class="feedback--title">Feedback</h2>
<p class="feedback--question">Was this page helpful?</p>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button>
<p class="feedback--response feedback--response-yes">
  Glad to hear it! Please <a href="https://github.com/googleforgames/agones/issues/new">tell us how we can improve</a>.
</p>
<p class="feedback--response feedback--response-no">
  Sorry to hear that. Please <a href="https://github.com/googleforgames/agones/issues/new">tell us how we can improve</a>.
</p>
</div>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof gtag !== 'function') return;
    gtag('event', 'page_helpful', {
      'event_category': 'Helpful',
      'event_label': window.location.pathname,
      'value': value
    });
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback( 100 );
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script>
<br />
Last modified September 29, 2021

</div>


          </main>
        </div>
      </div>
      <footer class="td-footer row d-print-none">
  <div class="container-fluid">
    <div class="row mx-md-2">
      <div class="td-footer__left col-6 col-sm-4 order-sm-1">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="User mailing list" aria-label="User mailing list">
    <a target="_blank" rel="noopener" href="https://groups.google.com/forum/#!forum/agones-discuss" aria-label="User mailing list">
      <i class="fa fa-envelope"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Twitter" aria-label="Twitter">
    <a target="_blank" rel="noopener" href="https://twitter.com/agonesdev" aria-label="Twitter">
      <i class="fab fa-twitter"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__right col-6 col-sm-4 order-sm-3">
        <ul class="td-footer__links-list">
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="GitHub" aria-label="GitHub">
    <a target="_blank" rel="noopener" href="https://github.com/googleforgames/agones" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Slack" aria-label="Slack">
    <a target="_blank" rel="noopener" href="https://join.slack.com/t/agones/shared_invite/zt-2mg1j7ddw-0QYA9IAvFFRKw51ZBK6mkQ" aria-label="Slack">
      <i class="fab fa-slack"></i>
    </a>
  </li>
  
  <li class="td-footer__links-item" data-bs-toggle="tooltip" title="Community Meetings" aria-label="Community Meetings">
    <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLhkWKwFGACw2dFpdmwxOyUCzlGP2-n7uF" aria-label="Community Meetings">
      <i class="fab fa-youtube"></i>
    </a>
  </li>
  
</ul>

      </div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2">
        <span class="td-footer__copyright">&copy;
    2025
    <span class="td-footer__authors">Copyright Google LLC All Rights Reserved.</span></span><span class="td-footer__all_rights_reserved">All Rights Reserved</span><span class="ms-2"><a href="https://policies.google.com/privacy" target="_blank" rel="noopener">Privacy Policy</a></span>
      </div>
    </div>
  </div>
</footer>

    </div>
    <script src="/site/js/main.js"></script>
<script src='/site/js/prism.js'></script>
<script src='/site/js/tabpane-persist.js'></script>
<script src=http://localhost:1313/site/js/asciinema-player.js></script>


<script > 
    (function() {
      var a = document.querySelector("#td-section-nav");
      addEventListener("beforeunload", function(b) {
          localStorage.setItem("menu.scrollTop", a.scrollTop)
      }), a.scrollTop = localStorage.getItem("menu.scrollTop")
    })()
  </script>
  

  </body>
</html>